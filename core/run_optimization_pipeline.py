#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Complete Optimization Pipeline for Spoofing Detection
-----------------------------------------------------
è‡ªåŠ¨åŒ–è¿è¡Œå®Œæ•´çš„ä¼˜åŒ–æµç¨‹ï¼š
1. å¢å¼ºç‰¹å¾å·¥ç¨‹å’Œå¤šç§æ¨¡å‹è®­ç»ƒå¯¹æ¯”ï¼ˆå‡è®¾å·²è¿è¡ŒETLç”Ÿæˆå¢å¼ºæ ‡ç­¾ï¼‰
2. æ€§èƒ½åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ

å‰ç½®æ¡ä»¶ï¼š
éœ€è¦å…ˆè¿è¡Œ ETL æµç¨‹ç”Ÿæˆæ•°æ®å’Œæ ‡ç­¾ï¼š
python scripts/data_process/run_etl_from_event.py \
    --root "/path/to/event_stream" \
    --enhanced_labels  # ç”Ÿæˆå¢å¼ºæ ‡ç­¾

ä½¿ç”¨æ–¹æ³•ï¼š
python run_optimization_pipeline.py --data_root "/path/to/data"
"""

import subprocess
import sys
import time
import argparse
from pathlib import Path
import json

def run_command(cmd, description):
    """è¿è¡Œå‘½ä»¤å¹¶å¤„ç†ç»“æœ"""
    print(f"\nğŸš€ {description}")
    print(f"Command: {' '.join(cmd)}")
    
    start_time = time.time()
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        elapsed = time.time() - start_time
        print(f"âœ… {description} completed successfully in {elapsed:.1f}s")
        
        if result.stdout:
            print(f"Output preview:\n{result.stdout[:500]}...")
        
        return True, result.stdout
        
    except subprocess.CalledProcessError as e:
        elapsed = time.time() - start_time
        print(f"âŒ {description} failed after {elapsed:.1f}s")
        print(f"Error: {e.stderr}")
        return False, e.stderr

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    print("ğŸ” Checking dependencies...")
    
    required_packages = [
        'pandas', 'numpy', 'sklearn', 'lightgbm', 
        'xgboost', 'optuna', 'imblearn', 
        'matplotlib', 'seaborn'
    ]
    
    # åŒ…åæ˜ å°„ï¼ˆimportå -> pipå®‰è£…åï¼‰
    package_mapping = {
        'sklearn': 'scikit-learn',
        'imblearn': 'imbalanced-learn'
    }
    
    missing = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            # ä½¿ç”¨pipå®‰è£…å
            pip_name = package_mapping.get(package, package)
            missing.append(pip_name)
    
    if missing:
        print(f"âŒ Missing packages: {missing}")
        print("Install with: pip install " + " ".join(missing))
        return False
    
    print("âœ… All dependencies available")
    return True

def check_data_availability(data_root, use_enhanced_labels):
    """æ£€æŸ¥æ•°æ®å¯ç”¨æ€§"""
    print("ğŸ” Checking data availability...")
    
    data_path = Path(data_root)
    features_dir = data_path / "features_select"
    labels_dir = data_path / "labels_select"
    
    if not features_dir.exists():
        print(f"âŒ Features directory not found: {features_dir}")
        print("Please run ETL pipeline first:")
        print("python scripts/data_process/run_etl_from_event.py --root /path/to/event_stream --enhanced_labels")
        return False
    
    if not labels_dir.exists():
        print(f"âŒ Labels directory not found: {labels_dir}")
        print("Please run ETL pipeline first:")
        print("python scripts/data_process/run_etl_from_event.py --root /path/to/event_stream --enhanced_labels")
        return False
    
    # æ£€æŸ¥æ–‡ä»¶æ•°é‡
    feature_files = list(features_dir.glob("X_*.parquet"))
    label_files = list(labels_dir.glob("labels_*.parquet"))
    
    print(f"Found {len(feature_files)} feature files and {len(label_files)} label files")
    
    if len(feature_files) == 0 or len(label_files) == 0:
        print("âŒ No data files found. Please run ETL pipeline first.")
        return False
    
    # å¦‚æœä½¿ç”¨å¢å¼ºæ ‡ç­¾ï¼Œæ£€æŸ¥æ ‡ç­¾æ–‡ä»¶ä¸­æ˜¯å¦åŒ…å«å¢å¼ºæ ‡ç­¾åˆ—
    if use_enhanced_labels and len(label_files) > 0:
        try:
            import pandas as pd
            sample_labels = pd.read_parquet(label_files[0])
            enhanced_cols = ['composite_spoofing', 'conservative_spoofing', 'quick_cancel_impact', 
                           'price_manipulation', 'fake_liquidity', 'layering_cancel']
            
            available_enhanced = [col for col in enhanced_cols if col in sample_labels.columns]
            if not available_enhanced:
                print("âš ï¸ Enhanced labels requested but not found in data.")
                print("Please re-run ETL with --enhanced_labels flag:")
                print("python scripts/data_process/run_etl_from_event.py --root /path/to/event_stream --enhanced_labels")
                return False
            else:
                print(f"âœ… Found enhanced label columns: {available_enhanced}")
        except Exception as e:
            print(f"âš ï¸ Could not verify enhanced labels: {e}")
    
    print("âœ… Data availability check passed")
    return True

def main():
    parser = argparse.ArgumentParser(description="Run complete optimization pipeline")
    parser.add_argument("--data_root", required=True, 
                       help="Root directory containing features_select and labels_select")
    parser.add_argument("--results_dir", 
                       help="Directory to save all results and analysis output (default: {data_root}/results)")
    parser.add_argument("--skip_baseline", action="store_true", 
                       help="Skip baseline training")
    parser.add_argument("--use_enhanced_labels", action="store_true",
                       help="Use enhanced spoofing labels instead of original y_label")
    parser.add_argument("--label_type", choices=["composite_spoofing", "conservative_spoofing"], 
                       default="composite_spoofing",
                       help="Which enhanced label type to use (only with --use_enhanced_labels)")
    parser.add_argument("--experiments", nargs="+", 
                       default=["enhanced", "ensemble", "optimized"],
                       help="Experiments to run")
    
    args = parser.parse_args()
    
    data_root = Path(args.data_root)
    if not data_root.exists():
        print(f"âŒ Data root does not exist: {data_root}")
        return
    
    # è®¾ç½®ç»“æœä¿å­˜ç›®å½•
    if args.results_dir:
        results_base_dir = Path(args.results_dir)
    else:
        results_base_dir = data_root / "results"
    
    # åˆ›å»ºç»“æœç›®å½•ç»“æ„
    results_base_dir.mkdir(exist_ok=True)
    experiment_results_dir = results_base_dir / "experiment_results"
    analysis_output_dir = results_base_dir / "analysis_output"
    experiment_results_dir.mkdir(exist_ok=True)
    analysis_output_dir.mkdir(exist_ok=True)
    
    print(f"ğŸ“ Results will be saved to: {results_base_dir}")
    if args.use_enhanced_labels:
        print(f"ğŸ·ï¸ Using enhanced labels: {args.label_type}")
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    # æ£€æŸ¥æ•°æ®å¯ç”¨æ€§
    if not check_data_availability(data_root, args.use_enhanced_labels):
        return
    
    results = {}
    total_start = time.time()
    
    print(f"\nğŸ¯ Starting optimization pipeline for: {data_root}")
    print(f"Experiments to run: {args.experiments}")
    
    # Step 1: åŸºçº¿æ¨¡å‹è®­ç»ƒï¼ˆå¯é€‰ï¼‰
    if not args.skip_baseline:
        success, output = run_command([
            sys.executable, "scripts/train/train_baseline_fixed.py",
            "--data_root", str(data_root),
            "--train_regex", "202503|202504",
            "--valid_regex", "202505",
            "--device", "cpu",
            "--balance_method", "undersample"
        ], "Step 1: Training baseline model")
        
        if success:
            # è§£æåŸºçº¿ç»“æœ
            try:
                # ä»è¾“å‡ºä¸­æå–å…³é”®æŒ‡æ ‡
                lines = output.split('\n')
                pr_auc = None
                prec_k = None
                
                for line in lines:
                    if "PR-AUCï¼š" in line:
                        pr_auc = float(line.split('ï¼š')[1])
                    elif "Precision@Top0.1%ï¼š" in line:
                        prec_k = float(line.split('ï¼š')[1])
                
                results['baseline'] = {
                    'PR-AUC': pr_auc,
                    'Precision@0.1%': prec_k,
                    'method': 'LightGBM + Undersample'
                }
            except:
                print("âš ï¸ Could not parse baseline results")
    
    # Step 2: è¿è¡Œå¢å¼ºå®éªŒ
    experiment_configs = {
        'enhanced': {
            'args': ['--sampling_method', 'undersample'],
            'description': 'Enhanced features + Undersample'
        },
        'ensemble': {
            'args': ['--use_ensemble', '--sampling_method', 'undersample'],
            'description': 'Model ensemble + Undersample'
        },
        'optimized': {
            'args': ['--optimize_params', '--sampling_method', 'undersample', '--n_trials', '30'],
            'description': 'Hyperparameter optimization + Undersample'
        }
    }
    
    for exp_name in args.experiments:
        if exp_name not in experiment_configs:
            print(f"âš ï¸ Unknown experiment: {exp_name}")
            continue
        
        config = experiment_configs[exp_name]
        
        cmd = [
            sys.executable, "scripts/train/train_baseline_enhanced_fixed.py",
            "--data_root", str(data_root),
            "--train_regex", "202503|202504",
            "--valid_regex", "202505"
        ] + config['args']
        
        # æ·»åŠ å¢å¼ºæ ‡ç­¾å‚æ•°
        if args.use_enhanced_labels:
            cmd.extend(["--use_enhanced_labels", "--label_type", args.label_type])
        
        success, output = run_command(cmd, f"Step 2.{exp_name}: {config['description']}")
        
        if success:
            # è§£æç»“æœï¼ˆæ”¹è¿›ç‰ˆï¼‰
            try:
                # ä»è¾“å‡ºä¸­æå–å…³é”®æŒ‡æ ‡
                lines = output.split('\n')
                pr_auc = None
                prec_k = None
                roc_auc = None
                
                for line in lines:
                    line = line.strip()
                    if line.startswith("PR-AUC:"):
                        pr_auc = float(line.split(':')[1].strip())
                    elif line.startswith("Precision@0.1%:"):
                        prec_k = float(line.split(':')[1].strip())
                    elif line.startswith("ROC-AUC:"):
                        roc_auc = float(line.split(':')[1].strip())
                
                label_suffix = f"_{args.label_type}" if args.use_enhanced_labels else "_original"
                results[exp_name] = {
                    'method': config['description'] + label_suffix,
                    'PR-AUC': pr_auc,
                    'Precision@0.1%': prec_k,
                    'ROC-AUC': roc_auc,
                    'success': True
                }
                
                # æ˜¾ç¤ºè§£æç»“æœ
                if pr_auc is not None:
                    print(f"  ğŸ“Š Parsed results: PR-AUC={pr_auc:.6f}")
                else:
                    print(f"  âš ï¸ Could not parse PR-AUC from output")
                    
            except Exception as e:
                print(f"âš ï¸ Could not parse {exp_name} results: {e}")
                results[exp_name] = {
                    'method': config['description'],
                    'success': False,
                    'error': str(e)
                }
    
    # Step 3: æ€§èƒ½å¯¹æ¯”å’Œåˆ†æ
    if len(results) > 1:
        # ä¿å­˜ç»“æœä¸ºJSON
        for exp_name, result in results.items():
            result_file = experiment_results_dir / f"{exp_name}_results.json"
            with open(result_file, 'w') as f:
                json.dump(result, f, indent=2)
        
        # è¿è¡Œæ€§èƒ½å¯¹æ¯”
        success, output = run_command([
            sys.executable, "scripts/analysis/performance_comparison.py",
            "--results_dir", str(experiment_results_dir),
            "--output_dir", str(analysis_output_dir)
        ], "Step 3: Performance analysis and comparison")
    
    # æ€»ç»“
    total_elapsed = time.time() - total_start
    print(f"\nğŸ‰ Pipeline completed in {total_elapsed/60:.1f} minutes")
    print(f"ğŸ“Š Results summary:")
    
    for exp_name, result in results.items():
        status = "âœ…" if result.get('success', False) else "âŒ"
        method = result.get('method', 'Unknown')
        pr_auc = result.get('PR-AUC', 'N/A')
        print(f"  {status} {exp_name}: {method} (PR-AUC: {pr_auc})")
    
    # ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ Next steps:")
    print(f"  1. Review analysis results in: {analysis_output_dir}")
    print(f"  2. Check detailed logs for each experiment")
    print(f"  3. Consider running additional experiments based on recommendations")
    
    if len(results) < 2:
        print(f"  4. Run more experiments for better comparison")

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# å‰ç½®æ­¥éª¤ï¼šè¿è¡ŒETLç”Ÿæˆå¢å¼ºæ ‡ç­¾
python scripts/data_process/run_etl_from_event.py \
  --root "/obs/users/fenglang/general/Spoofing Detect/data/event_stream" \
  --enhanced_labels \
  --backend polars

# è¿è¡Œå®Œæ•´ä¼˜åŒ–æµç¨‹ï¼ŒæŒ‡å®šç»“æœä¿å­˜ç›®å½•
python run_optimization_pipeline.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --results_dir "/home/ma-user/code/fenglang/Spoofing Detect/results"

# ä½¿ç”¨å¢å¼ºæ ‡ç­¾ï¼ˆETLæµç¨‹å·²ç”Ÿæˆï¼‰
python run_optimization_pipeline.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --results_dir "/home/ma-user/code/fenglang/Spoofing Detect/results" \
  --use_enhanced_labels \
  --label_type "composite_spoofing"

# ä½¿ç”¨ä¿å®ˆæ ‡ç­¾ï¼ˆæ›´ä¸¥æ ¼çš„æ¬ºè¯ˆæ£€æµ‹ï¼‰
python run_optimization_pipeline.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --results_dir "/home/ma-user/code/fenglang/Spoofing Detect/results" \
  --use_enhanced_labels \
  --label_type "conservative_spoofing"

# è·³è¿‡åŸºçº¿ï¼Œåªè¿è¡Œæ¨¡å‹ä¼˜åŒ–ï¼ŒæŒ‡å®šç»“æœç›®å½•
python run_optimization_pipeline.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --results_dir "/home/ma-user/code/fenglang/Spoofing Detect/results" \
  --skip_baseline \
  --experiments enhanced ensemble

# å¿«é€Ÿæµ‹è¯•
python run_optimization_pipeline.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --results_dir "/home/ma-user/code/fenglang/Spoofing Detect/results" \
  --skip_baseline \
  --experiments enhanced
""" 