#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Spoofing Detection Pipeline with Extended Labels
-----------------------------------------------
åŸºäºå¤šç§è™šå‡æŠ¥å•æ¨¡å¼çš„å®Œæ•´æ£€æµ‹æµç¨‹ï¼š
â€¢ å®šä¹‰å¤šç§è™šå‡æŠ¥å•æ¨¡å¼
â€¢ ç”Ÿæˆæ‰©å±•æ ‡ç­¾ (Extended Labels) 
â€¢ è®­ç»ƒè¯„ä¼°å„ç§æ¨¡å¼çš„é¢„æµ‹æ•ˆæœ
â€¢ æä¾›å®Œæ•´çš„ä»æ•°æ®åˆ°è¯„ä¼°çš„pipeline
"""

import argparse
import pandas as pd
import numpy as np
import lightgbm as lgb
from pathlib import Path
from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve
import json
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

class SpoofingPattern:
    """è™šå‡æŠ¥å•æ¨¡å¼å®šä¹‰"""
    
    def __init__(self, name: str, description: str, rules: dict):
        self.name = name
        self.description = description
        self.rules = rules
    
    def detect(self, df: pd.DataFrame) -> pd.Series:
        """æ£€æµ‹è¯¥æ¨¡å¼çš„è™šå‡æŠ¥å•"""
        raise NotImplementedError

class QuickCancelPattern(SpoofingPattern):
    """å¿«é€Ÿæ’¤å•æ¨¡å¼ - åœ¨æœ€ä½³ä»·ä½æäº¤å¤§å•åå¿«é€Ÿæ’¤å•"""
    
    def __init__(self):
        super().__init__(
            name="quick_cancel",
            description="åœ¨æœ€ä½³ä»·ä½æäº¤å¤§å•åå¿«é€Ÿæ’¤å•ï¼Œå½±å“å¸‚åœºæµåŠ¨æ€§",
            rules={
                "survival_time_ms": 100,
                "at_best_price": True,
                "large_order_quantile": 0.8
            }
        )
    
    def detect(self, df: pd.DataFrame) -> pd.Series:
        """æ£€æµ‹å¿«é€Ÿæ’¤å•æ¨¡å¼"""
        conditions = [
            df['å­˜æ´»æ—¶é—´_ms'] < self.rules['survival_time_ms'],
            df['äº‹ä»¶ç±»å‹'] == 'æ’¤å•',
            ((df['å§”æ‰˜ä»·æ ¼'] == df['bid1']) & (df['æ–¹å‘_å§”æ‰˜'] == 'ä¹°')) |
            ((df['å§”æ‰˜ä»·æ ¼'] == df['ask1']) & (df['æ–¹å‘_å§”æ‰˜'] == 'å–')),
            df['å§”æ‰˜æ•°é‡'] > df.groupby('ticker')['å§”æ‰˜æ•°é‡'].transform('quantile', self.rules['large_order_quantile'])
        ]
        return np.all(conditions, axis=0).astype(int)

class PriceManipulationPattern(SpoofingPattern):
    """ä»·æ ¼æ“çºµæ¨¡å¼ - æ¿€è¿›å®šä»·åå¿«é€Ÿæ’¤å•"""
    
    def __init__(self):
        super().__init__(
            name="price_manipulation", 
            description="é€šè¿‡æ¿€è¿›å®šä»·åå¿«é€Ÿæ’¤å•æ¥æ“çºµä»·æ ¼",
            rules={
                "survival_time_ms": 500,
                "price_aggressiveness_threshold": 2.0,
                "large_order_quantile": 0.75
            }
        )
    
    def detect(self, df: pd.DataFrame) -> pd.Series:
        conditions = [
            df['å­˜æ´»æ—¶é—´_ms'] < self.rules['survival_time_ms'],
            df['äº‹ä»¶ç±»å‹'] == 'æ’¤å•',
            df['price_aggressiveness'].abs() > self.rules['price_aggressiveness_threshold'],
            df['å§”æ‰˜æ•°é‡'] > df.groupby('ticker')['å§”æ‰˜æ•°é‡'].transform('quantile', self.rules['large_order_quantile'])
        ]
        return np.all(conditions, axis=0).astype(int)

class FakeLiquidityPattern(SpoofingPattern):
    """è™šå‡æµåŠ¨æ€§æ¨¡å¼ - åœ¨æœ€ä½³ä»·ä½æä¾›è™šå‡æµåŠ¨æ€§"""
    
    def __init__(self):
        super().__init__(
            name="fake_liquidity",
            description="åœ¨æœ€ä½³ä»·ä½æä¾›è™šå‡æµåŠ¨æ€§åå¿«é€Ÿæ’¤å•",
            rules={
                "survival_time_ms": 200,
                "at_touch": True,
                "large_order_quantile": 0.9
            }
        )
    
    def detect(self, df: pd.DataFrame) -> pd.Series:
        conditions = [
            df['å­˜æ´»æ—¶é—´_ms'] < self.rules['survival_time_ms'],
            df['äº‹ä»¶ç±»å‹'] == 'æ’¤å•',
            ((df['å§”æ‰˜ä»·æ ¼'] == df['bid1']) | (df['å§”æ‰˜ä»·æ ¼'] == df['ask1'])),
            df['å§”æ‰˜æ•°é‡'] > df.groupby('ticker')['å§”æ‰˜æ•°é‡'].transform('quantile', self.rules['large_order_quantile'])
        ]
        return np.all(conditions, axis=0).astype(int)

class LayeringPattern(SpoofingPattern):
    """åˆ†å±‚ä¸‹å•æ¨¡å¼ - é€šè¿‡åˆ†å±‚è®¢å•è¯¯å¯¼å¸‚åœº"""
    
    def __init__(self):
        super().__init__(
            name="layering",
            description="é€šè¿‡åˆ†å±‚ä¸‹å•æ¨¡å¼è¯¯å¯¼å¸‚åœºæ·±åº¦æ„ŸçŸ¥",
            rules={
                "survival_time_ms": 1000,
                "requires_layering_score": True
            }
        )
    
    def detect(self, df: pd.DataFrame) -> pd.Series:
        if 'layering_score' not in df.columns:
            return pd.Series(0, index=df.index)
        
        conditions = [
            df['layering_score'] > 0,
            df['å­˜æ´»æ—¶é—´_ms'] < self.rules['survival_time_ms'],
            df['äº‹ä»¶ç±»å‹'] == 'æ’¤å•'
        ]
        return np.all(conditions, axis=0).astype(int)

class VolatilePeriodPattern(SpoofingPattern):
    """æ³¢åŠ¨æ—¶æ®µæ“çºµæ¨¡å¼ - åœ¨å¸‚åœºæ´»è·ƒæ—¶æ®µçš„å¼‚å¸¸è¡Œä¸º"""
    
    def __init__(self):
        super().__init__(
            name="volatile_period",
            description="åœ¨å¼€ç›˜æ”¶ç›˜ç­‰æ³¢åŠ¨æ—¶æ®µè¿›è¡Œæ“çºµ",
            rules={
                "survival_time_ms": 50,
                "size_multiplier": 1.0
            }
        )
    
    def detect(self, df: pd.DataFrame) -> pd.Series:
        # å¸‚åœºæ´»è·ƒæ—¶æ®µ
        market_open = ((df['å§”æ‰˜_datetime'].dt.time >= pd.to_datetime('09:30').time()) & 
                      (df['å§”æ‰˜_datetime'].dt.time <= pd.to_datetime('10:30').time()))
        market_close = ((df['å§”æ‰˜_datetime'].dt.time >= pd.to_datetime('14:00').time()) & 
                       (df['å§”æ‰˜_datetime'].dt.time <= pd.to_datetime('15:00').time()))
        
        conditions = [
            df['å­˜æ´»æ—¶é—´_ms'] < self.rules['survival_time_ms'],
            df['äº‹ä»¶ç±»å‹'] == 'æ’¤å•',
            market_open | market_close,
            df['å§”æ‰˜æ•°é‡'] > df.groupby('ticker')['å§”æ‰˜æ•°é‡'].transform('median') * self.rules['size_multiplier']
        ]
        return np.all(conditions, axis=0).astype(int)

class SpoofingDetectionPipeline:
    """è™šå‡æŠ¥å•æ£€æµ‹Pipeline"""
    
    def __init__(self):
        self.patterns = [
            QuickCancelPattern(),
            PriceManipulationPattern(), 
            FakeLiquidityPattern(),
            LayeringPattern(),
            VolatilePeriodPattern()
        ]
        
        self.safe_features = [
            # è¡Œæƒ…ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
            "bid1", "ask1", "prev_close", "mid_price", "spread",
            # ä»·æ ¼ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
            "delta_mid", "pct_spread", "price_dev_prevclose",
            # è®¢å•ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
            "is_buy", "log_qty",
            # å†å²ç»Ÿè®¡ç‰¹å¾ï¼ˆåªä½¿ç”¨è¿‡å»çš„ä¿¡æ¯ï¼‰
            "orders_100ms", "cancels_5s",
            # æ—¶é—´ç‰¹å¾
            "time_sin", "time_cos", "in_auction",
            # å¢å¼ºç‰¹å¾ï¼ˆåŸºäºå§”æ‰˜æ—¶åˆ»çš„ä¿¡æ¯ï¼‰
            "book_imbalance", "price_aggressiveness"
        ]
        
        self.leakage_features = [
            "final_survival_time_ms", "total_events", "total_traded_qty",
            "num_trades", "num_cancels", "is_fully_filled", "layering_score"
        ]
    
    def load_data(self, data_root: Path) -> pd.DataFrame:
        """åŠ è½½å’Œåˆå¹¶æ•°æ®"""
        print("ğŸ“¥ Loading data...")
        
        # åŠ è½½ç‰¹å¾
        feat_files = list((data_root / "features_select").glob("X_*.parquet"))
        df_features = pd.concat([pd.read_parquet(f) for f in feat_files], ignore_index=True)
        
        # åŠ è½½æ ‡ç­¾
        label_files = list((data_root / "labels_select").glob("labels_*.parquet"))
        df_labels = pd.concat([pd.read_parquet(f) for f in label_files], ignore_index=True)
        
        # åˆå¹¶æ•°æ®
        df = df_features.merge(df_labels, on=['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·'], how='inner')
        
        print(f"  Features: {df_features.shape}")
        print(f"  Labels: {df_labels.shape}")
        print(f"  Merged: {df.shape}")
        
        return df
    
    def generate_extended_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç”Ÿæˆæ‰©å±•æ ‡ç­¾"""
        print("\nğŸ·ï¸ Generating Extended Labels...")
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        if 'å§”æ‰˜_datetime' not in df.columns:
            df['å§”æ‰˜_datetime'] = pd.to_datetime(df['è‡ªç„¶æ—¥'].astype(str), format='%Y%m%d')
        else:
            df['å§”æ‰˜_datetime'] = pd.to_datetime(df['å§”æ‰˜_datetime'])
        
        # ä¸ºæ¯ç§æ¨¡å¼ç”Ÿæˆæ ‡ç­¾
        pattern_results = {}
        for pattern in self.patterns:
            try:
                df[pattern.name] = pattern.detect(df)
                pos_count = df[pattern.name].sum()
                pos_rate = pos_count / len(df) * 100
                pattern_results[pattern.name] = {
                    'count': pos_count,
                    'rate': pos_rate,
                    'description': pattern.description
                }
                print(f"  {pattern.name:<20}: {pos_count:>6,} ({pos_rate:>6.3f}%) - {pattern.description}")
            except Exception as e:
                print(f"  âš ï¸ {pattern.name} failed: {e}")
                df[pattern.name] = 0
                pattern_results[pattern.name] = {'count': 0, 'rate': 0.0, 'description': pattern.description}
        
        # ç»„åˆæ ‡ç­¾
        pattern_cols = [p.name for p in self.patterns]
        df['extended_liberal'] = (df[pattern_cols].sum(axis=1) >= 1).astype(int)  # ä»»æ„æ¨¡å¼
        df['extended_moderate'] = (df[pattern_cols].sum(axis=1) >= 2).astype(int)  # ä¸¤ç§æ¨¡å¼
        df['extended_strict'] = (df[pattern_cols].sum(axis=1) >= 3).astype(int)    # ä¸‰ç§æ¨¡å¼
        
        # ç»„åˆæ ‡ç­¾ç»Ÿè®¡
        for label_type in ['extended_liberal', 'extended_moderate', 'extended_strict']:
            pos_count = df[label_type].sum()
            pos_rate = pos_count / len(df) * 100
            print(f"  {label_type:<20}: {pos_count:>6,} ({pos_rate:>6.3f}%)")
        
        return df, pattern_results
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """å‡†å¤‡è®­ç»ƒç‰¹å¾ï¼Œç§»é™¤æ•°æ®æ³„éœ²"""
        print("\nğŸ”§ Preparing features...")
        
        # ç§»é™¤æ³„éœ²ç‰¹å¾
        df_clean = df.drop(columns=self.leakage_features, errors='ignore')
        
        # è·å–å¯ç”¨çš„å®‰å…¨ç‰¹å¾
        available_features = [f for f in self.safe_features if f in df_clean.columns]
        
        print(f"  Safe features: {len(available_features)}")
        print(f"  Removed leakage features: {len([f for f in self.leakage_features if f in df.columns])}")
        
        return df_clean, available_features
    
    def train_evaluate_model(self, X_train, y_train, X_valid, y_valid, label_name: str) -> Dict:
        """è®­ç»ƒå’Œè¯„ä¼°å•ä¸ªæ¨¡å‹"""
        pos_count = y_train.sum()
        if pos_count == 0:
            return None
        
        # æ•°æ®å¹³è¡¡
        pos_indices = y_train[y_train == 1].index
        neg_indices = y_train[y_train == 0].index
        
        balance_ratio = 20
        target_neg_size = min(len(pos_indices) * balance_ratio, len(neg_indices))
        selected_neg_indices = np.random.choice(neg_indices, target_neg_size, replace=False)
        
        selected_indices = np.concatenate([pos_indices, selected_neg_indices])
        X_train_balanced = X_train.loc[selected_indices]
        y_train_balanced = y_train.loc[selected_indices]
        
        # è®­ç»ƒæ¨¡å‹
        model = lgb.LGBMClassifier(
            objective='binary',
            metric='average_precision',
            learning_rate=0.05,
            num_leaves=31,
            max_depth=6,
            n_estimators=1000,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=10,
            reg_lambda=10,
            random_state=42,
            verbose=-1
        )
        
        model.fit(
            X_train_balanced, y_train_balanced,
            eval_set=[(X_valid, y_valid)],
            callbacks=[lgb.early_stopping(100, verbose=False)]
        )
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred_proba = model.predict_proba(X_valid)[:, 1]
        
        metrics = {
            'PR-AUC': average_precision_score(y_valid, y_pred_proba),
            'ROC-AUC': roc_auc_score(y_valid, y_pred_proba),
            'positive_samples': pos_count,
            'positive_rate': pos_count / len(y_train) * 100
        }
        
        # Precision at K
        for k in [0.001, 0.005, 0.01, 0.05]:
            k_int = max(1, int(len(y_valid) * k))
            top_k_idx = y_pred_proba.argsort()[::-1][:k_int]
            prec_k = y_valid.iloc[top_k_idx].mean()
            metrics[f'Precision@{k*100:.1f}%'] = prec_k
        
        return {
            'model': model,
            'metrics': metrics,
            'predictions': y_pred_proba
        }
    
    def run_full_pipeline(self, data_root: str, train_regex: str, valid_regex: str, results_dir: str = None):
        """è¿è¡Œå®Œæ•´pipeline"""
        print("ğŸš€ Spoofing Detection Pipeline with Extended Labels")
        print("=" * 80)
        
        data_root = Path(data_root)
        if results_dir:
            results_dir = Path(results_dir)
        else:
            results_dir = data_root / "extended_labels_results"
        results_dir.mkdir(exist_ok=True)
        
        # 1. åŠ è½½æ•°æ®
        df = self.load_data(data_root)
        
        # 2. ç”Ÿæˆæ‰©å±•æ ‡ç­¾
        df, pattern_results = self.generate_extended_labels(df)
        
        # 3. å‡†å¤‡ç‰¹å¾
        df_clean, feature_cols = self.prepare_features(df)
        
        # 4. æ•°æ®åˆ‡åˆ†
        train_mask = df_clean["è‡ªç„¶æ—¥"].astype(str).str.contains(train_regex)
        valid_mask = df_clean["è‡ªç„¶æ—¥"].astype(str).str.contains(valid_regex)
        
        df_train = df_clean[train_mask].copy()
        df_valid = df_clean[valid_mask].copy()
        
        print(f"\nğŸ“… Data split:")
        print(f"  Training: {len(df_train):,} samples")
        print(f"  Validation: {len(df_valid):,} samples")
        print(f"  Features: {len(feature_cols)}")
        
        X_train = df_train[feature_cols].fillna(0)
        X_valid = df_valid[feature_cols].fillna(0)
        
        # 5. è®­ç»ƒè¯„ä¼°æ‰€æœ‰æ ‡ç­¾ç­–ç•¥
        print(f"\nğŸ¯ Training and Evaluation:")
        
        all_results = {}
        
        # åŸå§‹æ ‡ç­¾
        if 'y_label' in df_train.columns:
            result = self.train_evaluate_model(
                X_train, df_train['y_label'], X_valid, df_valid['y_label'], 'Original'
            )
            if result:
                all_results['original'] = result['metrics']
                print(f"  Original Labels    : PR-AUC={result['metrics']['PR-AUC']:.6f}")
        
        # å„ç§è™šå‡æŠ¥å•æ¨¡å¼
        for pattern in self.patterns:
            if pattern.name in df_train.columns:
                result = self.train_evaluate_model(
                    X_train, df_train[pattern.name], X_valid, df_valid[pattern.name], pattern.name
                )
                if result:
                    all_results[pattern.name] = result['metrics']
                    print(f"  {pattern.name:<15}: PR-AUC={result['metrics']['PR-AUC']:.6f}")
        
        # ç»„åˆæ‰©å±•æ ‡ç­¾
        for ext_label in ['extended_liberal', 'extended_moderate', 'extended_strict']:
            if ext_label in df_train.columns:
                result = self.train_evaluate_model(
                    X_train, df_train[ext_label], X_valid, df_valid[ext_label], ext_label
                )
                if result:
                    all_results[ext_label] = result['metrics']
                    print(f"  {ext_label:<15}: PR-AUC={result['metrics']['PR-AUC']:.6f}")
        
        # 6. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self.generate_report(all_results, pattern_results, results_dir)
        
        print(f"\nğŸ’¾ Results saved to: {results_dir}")
        return all_results
    
    def generate_report(self, all_results: Dict, pattern_results: Dict, results_dir: Path):
        """ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š"""
        print(f"\nğŸ“Š Extended Labels Performance Report")
        print("=" * 80)
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_df = pd.DataFrame(all_results).T
        
        # ä¸»è¦æŒ‡æ ‡å¯¹æ¯”
        key_metrics = ['PR-AUC', 'ROC-AUC', 'Precision@0.1%', 'Precision@1.0%']
        available_metrics = [m for m in key_metrics if m in comparison_df.columns]
        
        print(f"\nğŸ¯ Performance Comparison:")
        print(comparison_df[available_metrics].round(6))
        
        # æœ€ä½³ç­–ç•¥
        if 'PR-AUC' in comparison_df.columns:
            best_strategy = comparison_df['PR-AUC'].idxmax()
            best_score = comparison_df.loc[best_strategy, 'PR-AUC']
            print(f"\nğŸ† Best Strategy: {best_strategy} (PR-AUC: {best_score:.6f})")
        
        # æ¨¡å¼åˆ†æ
        print(f"\nğŸ” Spoofing Pattern Analysis:")
        for pattern_name, stats in pattern_results.items():
            print(f"  {pattern_name:<20}: {stats['count']:>6,} samples ({stats['rate']:>6.3f}%)")
            print(f"  {'':>22}  {stats['description']}")
        
        # ä¿å­˜ç»“æœ
        comparison_df.to_csv(results_dir / "extended_labels_comparison.csv", float_format='%.8f')
        
        with open(results_dir / "pattern_analysis.json", 'w') as f:
            json.dump(pattern_results, f, indent=2)
        
        with open(results_dir / "performance_results.json", 'w') as f:
            json.dump(all_results, f, indent=2, default=str)

def main():
    parser = argparse.ArgumentParser(description="Spoofing Detection Pipeline with Extended Labels")
    parser.add_argument("--data_root", required=True, help="æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--train_regex", default="202503|202504", help="è®­ç»ƒæ•°æ®æ—¥æœŸæ­£åˆ™")
    parser.add_argument("--valid_regex", default="202505", help="éªŒè¯æ•°æ®æ—¥æœŸæ­£åˆ™")
    parser.add_argument("--results_dir", help="ç»“æœä¿å­˜ç›®å½•")
    
    args = parser.parse_args()
    
    # è¿è¡Œpipeline
    pipeline = SpoofingDetectionPipeline()
    results = pipeline.run_full_pipeline(
        data_root=args.data_root,
        train_regex=args.train_regex,
        valid_regex=args.valid_regex,
        results_dir=args.results_dir
    )

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# è¿è¡Œå®Œæ•´çš„è™šå‡æŠ¥å•æ£€æµ‹pipeline
python scripts/spoofing_detection_pipeline.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --train_regex "202503|202504" \
  --valid_regex "202505"

# æŒ‡å®šç»“æœä¿å­˜ç›®å½•
python scripts/spoofing_detection_pipeline.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --results_dir "./extended_labels_analysis"
""" 