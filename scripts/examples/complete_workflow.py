#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
complete_workflow.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹ - ä»ETLåˆ°ç‰¹å¾åˆ†æçš„ç«¯åˆ°ç«¯å¤„ç†
â€¢ åˆ†æ—¥æœŸç‰¹å¾è®¡ç®—ï¼ˆä¸è¿‡æ»¤ï¼‰
â€¢ æ•´åˆæ‰€æœ‰æ—¥æœŸæ•°æ®
â€¢ å…¨å±€ç‰¹å¾è´¨é‡åˆ†æ
â€¢ æ™ºèƒ½ç‰¹å¾è¿‡æ»¤
â€¢ æ¨¡å‹å°±ç»ªæ•°æ®ç”Ÿæˆ
"""

import sys
import subprocess
from pathlib import Path
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
import argparse

console = Console()

def run_etl_pipeline(data_root: str, tickers: list = None, backend: str = "polars"):
    """è¿è¡ŒETLæµæ°´çº¿"""
    console.print(f"\n[bold green]ğŸš€ Step 1: è¿è¡ŒETLæµæ°´çº¿[/bold green]")
    
    cmd = [
        "python", "scripts/data_process/run_etl_from_event_refactored.py",
        "--root", f"{data_root}/event_stream",
        "--backend", backend,
        "--extended_labels"
    ]
    
    if tickers:
        cmd.extend(["--tickers"] + tickers)
    
    console.print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        console.print("[green]âœ… ETLæµæ°´çº¿æ‰§è¡ŒæˆåŠŸ[/green]")
        return True
    except subprocess.CalledProcessError as e:
        console.print(f"[red]âŒ ETLæµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}[/red]")
        console.print(f"é”™è¯¯è¾“å‡º: {e.stderr}")
        return False

def run_feature_analysis(data_root: str, save_filtered: bool = True):
    """è¿è¡Œç‰¹å¾è´¨é‡åˆ†æ"""
    console.print(f"\n[bold green]ğŸ” Step 2: ç‰¹å¾è´¨é‡åˆ†æ[/bold green]")
    
    analysis_dir = Path(data_root) / "analysis_results"
    analysis_dir.mkdir(exist_ok=True)
    
    cmd = [
        "python", "scripts/data_process/feature_analysis.py",
        "--data_root", data_root,
        "--target_col", "y_label",
        "--output_dir", str(analysis_dir)
    ]
    
    if save_filtered:
        cmd.append("--save_filtered")
    
    console.print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        console.print("[green]âœ… ç‰¹å¾åˆ†æå®Œæˆ[/green]")
        return True
    except subprocess.CalledProcessError as e:
        console.print(f"[red]âŒ ç‰¹å¾åˆ†æå¤±è´¥: {e}[/red]")
        console.print(f"é”™è¯¯è¾“å‡º: {e.stderr}")
        return False

def generate_training_data(data_root: str):
    """ç”Ÿæˆè®­ç»ƒå°±ç»ªçš„æ•°æ®"""
    console.print(f"\n[bold green]ğŸ“Š Step 3: ç”Ÿæˆè®­ç»ƒæ•°æ®[/bold green]")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ é¢å¤–çš„æ•°æ®å¤„ç†æ­¥éª¤
    # æ¯”å¦‚ï¼š
    # - ç‰¹å¾æ ‡å‡†åŒ–
    # - æ•°æ®åˆ†å‰²ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼‰
    # - æ ·æœ¬å¹³è¡¡å¤„ç†ç­‰
    
    filtered_data_path = Path(data_root) / "filtered_data.parquet"
    if filtered_data_path.exists():
        console.print(f"[green]âœ… è®­ç»ƒæ•°æ®å·²å‡†å¤‡: {filtered_data_path}[/green]")
        
        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        import pandas as pd
        df = pd.read_parquet(filtered_data_path)
        console.print(f"  æ•°æ®å½¢çŠ¶: {df.shape}")
        console.print(f"  ç‰¹å¾æ•°é‡: {len([c for c in df.columns if c not in ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·', 'y_label']])}")
        console.print(f"  æ ·æœ¬æ•°é‡: {len(df):,}")
        
        if 'y_label' in df.columns:
            positive_rate = df['y_label'].mean()
            console.print(f"  æ­£æ ·æœ¬æ¯”ä¾‹: {positive_rate:.4f}")
        
        return True
    else:
        console.print(f"[red]âŒ æœªæ‰¾åˆ°è¿‡æ»¤åçš„æ•°æ®æ–‡ä»¶[/red]")
        return False

def create_data_summary(data_root: str):
    """åˆ›å»ºæ•°æ®æ‘˜è¦æŠ¥å‘Š"""
    console.print(f"\n[bold green]ğŸ“‹ Step 4: ç”Ÿæˆæ•°æ®æ‘˜è¦[/bold green]")
    
    analysis_dir = Path(data_root) / "analysis_results"
    summary_file = analysis_dir / "data_summary.md"
    
    try:
        import pandas as pd
        import json
        
        # è¯»å–åˆ†æç»“æœ
        with open(analysis_dir / "feature_analysis.json", "r", encoding="utf-8") as f:
            analysis_results = json.load(f)
        
        # è¯»å–è¿‡æ»¤åçš„æ•°æ®
        filtered_data = pd.read_parquet(Path(data_root) / "filtered_data.parquet")
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary_content = f"""# æ•°æ®å¤„ç†æ‘˜è¦æŠ¥å‘Š

## ğŸ“Š åŸºç¡€ç»Ÿè®¡

- **æ•°æ®æ ¹ç›®å½•**: {data_root}
- **æœ€ç»ˆæ•°æ®å½¢çŠ¶**: {filtered_data.shape}
- **ç‰¹å¾æ•°é‡**: {len([c for c in filtered_data.columns if c not in ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·', 'y_label']])}
- **æ ·æœ¬æ•°é‡**: {len(filtered_data):,}
- **å†…å­˜ä½¿ç”¨**: {filtered_data.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB

## ğŸš« ç§»é™¤çš„ç‰¹å¾

### é—®é¢˜ç‰¹å¾ç»Ÿè®¡
- **å¸¸æ•°åˆ—**: {len(analysis_results.get('problematic_features', {}).get('constant_features', []))} ä¸ª
- **ä½æ–¹å·®ç‰¹å¾**: {len(analysis_results.get('problematic_features', {}).get('low_variance_features', []))} ä¸ª  
- **é«˜ç¼ºå¤±ç‡ç‰¹å¾**: {len(analysis_results.get('problematic_features', {}).get('high_missing_features', []))} ä¸ª
- **é‡å¤ç‰¹å¾**: {len(analysis_results.get('problematic_features', {}).get('duplicate_features', []))} ä¸ª

### ä¿¡æ¯æ³„éœ²ç‰¹å¾
- **ç–‘ä¼¼æ³„éœ²ç‰¹å¾**: {len(analysis_results.get('leakage_features', {}).get('suspicious_features', []))} ä¸ª
- **æ¨¡å¼åŒ¹é…ç‰¹å¾**: {sum(len(matches) for matches in analysis_results.get('leakage_features', {}).get('pattern_matches', {}).values())} ä¸ª

## âœ… å»ºè®®ç­–ç•¥

- **å»ºè®®ç§»é™¤ç‰¹å¾æ•°**: {len(analysis_results.get('recommendations', {}).get('features_to_remove', []))} ä¸ª
- **éœ€è°ƒæŸ¥ç‰¹å¾æ•°**: {len(analysis_results.get('recommendations', {}).get('features_to_investigate', []))} ä¸ª
- **ä¼˜å…ˆç‰¹å¾æ•°**: {len(analysis_results.get('recommendations', {}).get('priority_features', []))} ä¸ª
- **å®‰å…¨æ ¸å¿ƒç‰¹å¾æ•°**: {len(analysis_results.get('recommendations', {}).get('safe_features', []))} ä¸ª

## ğŸ¯ æ ‡ç­¾åˆ†å¸ƒ

"""
        
        if 'y_label' in filtered_data.columns:
            positive_count = filtered_data['y_label'].sum()
            total_count = len(filtered_data)
            positive_rate = positive_count / total_count
            
            summary_content += f"""- **æ­£æ ·æœ¬æ•°é‡**: {positive_count:,}
- **è´Ÿæ ·æœ¬æ•°é‡**: {total_count - positive_count:,}
- **æ­£æ ·æœ¬æ¯”ä¾‹**: {positive_rate:.4f}
"""
        
        summary_content += f"""
## ğŸ“ˆ ä¸‹ä¸€æ­¥å»ºè®®

1. **ç‰¹å¾å·¥ç¨‹**: è€ƒè™‘åŸºäºæ ¸å¿ƒç‰¹å¾æ„é€ æ–°çš„è¡ç”Ÿç‰¹å¾
2. **æ¨¡å‹é€‰æ‹©**: æ ¹æ®ç‰¹å¾ç±»å‹é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç®—æ³•  
3. **æ ·æœ¬å¹³è¡¡**: æ ¹æ®æ ‡ç­¾åˆ†å¸ƒè€ƒè™‘é‡‡æ ·ç­–ç•¥
4. **äº¤å‰éªŒè¯**: æŒ‰æ—¶é—´/è‚¡ç¥¨è¿›è¡Œåˆ†å±‚äº¤å‰éªŒè¯
5. **ç‰¹å¾é€‰æ‹©**: å¯è¿›ä¸€æ­¥ä½¿ç”¨æ¨¡å‹å†…ç½®çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•

## ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶

- `filtered_data.parquet`: è¿‡æ»¤åçš„è®­ç»ƒæ•°æ®
- `analysis_results/feature_analysis.json`: è¯¦ç»†åˆ†æç»“æœ
- `analysis_results/data_summary.md`: æœ¬æ‘˜è¦æŠ¥å‘Š

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        with open(summary_file, "w", encoding="utf-8") as f:
            f.write(summary_content)
        
        console.print(f"[green]ğŸ“„ æ‘˜è¦æŠ¥å‘Šå·²ç”Ÿæˆ: {summary_file}[/green]")
        return True
        
    except Exception as e:
        console.print(f"[red]âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}[/red]")
        return False

def main():
    """å®Œæ•´å·¥ä½œæµç¨‹ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å®Œæ•´æ•°æ®å¤„ç†å·¥ä½œæµç¨‹")
    parser.add_argument("--data_root", required=True, help="æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--tickers", nargs="*", help="è‚¡ç¥¨ä»£ç ç­›é€‰")
    parser.add_argument("--backend", choices=["pandas", "polars"], default="polars", help="ETLåç«¯")
    parser.add_argument("--skip_etl", action="store_true", help="è·³è¿‡ETLæ­¥éª¤")
    parser.add_argument("--skip_analysis", action="store_true", help="è·³è¿‡ç‰¹å¾åˆ†ææ­¥éª¤")
    
    args = parser.parse_args()
    
    console.print(f"[bold cyan]ğŸ¯ å¼€å§‹å®Œæ•´æ•°æ®å¤„ç†å·¥ä½œæµç¨‹[/bold cyan]")
    console.print(f"æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    console.print(f"è‚¡ç¥¨ç­›é€‰: {args.tickers if args.tickers else 'å…¨éƒ¨'}")
    console.print(f"å¤„ç†åç«¯: {args.backend}")
    
    success_steps = 0
    total_steps = 4
    
    # Step 1: ETL Pipeline
    if not args.skip_etl:
        if run_etl_pipeline(args.data_root, args.tickers, args.backend):
            success_steps += 1
    else:
        console.print(f"[yellow]â­ï¸  è·³è¿‡ETLæ­¥éª¤[/yellow]")
        success_steps += 1
    
    # Step 2: Feature Analysis
    if not args.skip_analysis:
        if run_feature_analysis(args.data_root, save_filtered=True):
            success_steps += 1
    else:
        console.print(f"[yellow]â­ï¸  è·³è¿‡ç‰¹å¾åˆ†ææ­¥éª¤[/yellow]")
        success_steps += 1
    
    # Step 3: Generate Training Data
    if generate_training_data(args.data_root):
        success_steps += 1
    
    # Step 4: Create Summary
    if create_data_summary(args.data_root):
        success_steps += 1
    
    # æ€»ç»“
    console.print(f"\n[bold green]ğŸ‰ å·¥ä½œæµç¨‹å®Œæˆ[/bold green]")
    console.print(f"æˆåŠŸæ­¥éª¤: {success_steps}/{total_steps}")
    
    if success_steps == total_steps:
        console.print(f"[bold green]âœ… æ‰€æœ‰æ­¥éª¤æ‰§è¡ŒæˆåŠŸï¼æ•°æ®å·²å‡†å¤‡å¥½ç”¨äºæ¨¡å‹è®­ç»ƒã€‚[/bold green]")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„å…³é”®æ–‡ä»¶
        data_root_path = Path(args.data_root)
        key_files = [
            data_root_path / "filtered_data.parquet",
            data_root_path / "analysis_results" / "feature_analysis.json", 
            data_root_path / "analysis_results" / "data_summary.md"
        ]
        
        console.print(f"\n[bold cyan]ğŸ“ å…³é”®è¾“å‡ºæ–‡ä»¶:[/bold cyan]")
        for file_path in key_files:
            if file_path.exists():
                console.print(f"  âœ… {file_path}")
            else:
                console.print(f"  âŒ {file_path}")
    else:
        console.print(f"[bold red]âš ï¸  éƒ¨åˆ†æ­¥éª¤æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚[/bold red]")

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹:

# å®Œæ•´å·¥ä½œæµç¨‹
python scripts/examples/complete_workflow.py \
    --data_root "/path/to/data" \
    --tickers 000001.SZ 000002.SZ \
    --backend polars

# åªè¿è¡Œåˆ†æéƒ¨åˆ†ï¼ˆè·³è¿‡ETLï¼‰
python scripts/examples/complete_workflow.py \
    --data_root "/path/to/data" \
    --skip_etl

# åªè¿è¡ŒETLéƒ¨åˆ†
python scripts/examples/complete_workflow.py \
    --data_root "/path/to/data" \
    --tickers 000001.SZ \
    --skip_analysis
""" 