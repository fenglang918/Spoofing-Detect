#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åˆ†æå’Œå±•ç¤ºåˆ†è‚¡ç¥¨çš„è¯„ä¼°ç»“æœ
"""

import argparse
import json
import os
import glob
import pandas as pd

def load_latest_evaluation_results(eval_dir):
    """åŠ è½½æœ€æ–°çš„è¯„ä¼°ç»“æœ"""
    pattern = os.path.join(eval_dir, "evaluation_results_*.json")
    files = glob.glob(pattern)
    
    if not files:
        return None
    
    # è·å–æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(files, key=os.path.getctime)
    
    with open(latest_file, 'r') as f:
        return json.load(f), latest_file

def load_confusion_matrix_results(eval_dir):
    """åŠ è½½æœ€æ–°çš„æ··æ·†çŸ©é˜µç»“æœ"""
    pattern = os.path.join(eval_dir, "confusion_matrix_*.json")
    files = glob.glob(pattern)
    
    if not files:
        return None
    
    # è·å–æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(files, key=os.path.getctime)
    
    with open(latest_file, 'r') as f:
        return json.load(f), latest_file

def analyze_per_ticker_performance(results_data, conf_matrix_data=None):
    """åˆ†æåˆ†è‚¡ç¥¨æ€§èƒ½"""
    print("ğŸ“Š Per-Ticker Performance Analysis")
    print("=" * 60)
    
    if 'per_ticker_metrics' not in results_data:
        print("âŒ æ²¡æœ‰å‘ç°åˆ†è‚¡ç¥¨è¯„ä¼°ç»“æœ")
        print("è¯·ç¡®ä¿ä½¿ç”¨äº†æœ€æ–°ç‰ˆæœ¬çš„è®­ç»ƒè„šæœ¬")
        return
    
    ticker_metrics = results_data['per_ticker_metrics']
    
    if not ticker_metrics:
        print("âŒ åˆ†è‚¡ç¥¨è¯„ä¼°ç»“æœä¸ºç©º")
        return
    
    print(f"ğŸ“ˆ å…±åˆ†æ {len(ticker_metrics)} ä¸ªè‚¡ç¥¨çš„æ€§èƒ½\n")
    
    # åˆ›å»ºDataFrameä¾¿äºåˆ†æ
    df_results = []
    for ticker, metrics in ticker_metrics.items():
        row = {'ticker': ticker}
        row.update(metrics)
        df_results.append(row)
    
    df = pd.DataFrame(df_results)
    
    # æ˜¾ç¤ºè¯¦ç»†è¡¨æ ¼
    print("ğŸ“‹ è¯¦ç»†è¯„ä¼°ç»“æœ:")
    print(f"{'è‚¡ç¥¨ä»£ç ':<12} {'PR-AUC':<8} {'ROC-AUC':<8} {'P@0.1%':<8} {'P@0.5%':<8} {'P@1.0%':<8}")
    print("-" * 70)
    
    for _, row in df.iterrows():
        print(f"{row['ticker']:<12} "
              f"{row.get('PR-AUC', 0):<8.4f} "
              f"{row.get('ROC-AUC', 0):<8.4f} "
              f"{row.get('Precision@0.1%', 0):<8.4f} "
              f"{row.get('Precision@0.5%', 0):<8.4f} "
              f"{row.get('Precision@1.0%', 0):<8.4f}")
    
    # ç»Ÿè®¡åˆ†æ
    print(f"\nğŸ“Š ç»Ÿè®¡åˆ†æ:")
    numeric_cols = ['PR-AUC', 'ROC-AUC', 'Precision@0.1%', 'Precision@0.5%', 'Precision@1.0%']
    
    for col in numeric_cols:
        if col in df.columns:
            values = df[col].dropna()
            if len(values) > 0:
                print(f"  {col}:")
                print(f"    å¹³å‡å€¼: {values.mean():.4f}")
                print(f"    æœ€å¤§å€¼: {values.max():.4f} ({df.loc[values.idxmax(), 'ticker']})")
                print(f"    æœ€å°å€¼: {values.min():.4f} ({df.loc[values.idxmin(), 'ticker']})")
                print(f"    æ ‡å‡†å·®: {values.std():.4f}")
    
    # æ’ååˆ†æ
    print(f"\nğŸ† å„æŒ‡æ ‡æ’å:")
    for col in ['PR-AUC', 'ROC-AUC', 'Precision@0.1%']:
        if col in df.columns:
            top_ticker = df.loc[df[col].idxmax()]
            print(f"  {col} æœ€ä½³: {top_ticker['ticker']} ({top_ticker[col]:.4f})")
    
    # æ··æ·†çŸ©é˜µåˆ†æ
    if conf_matrix_data and 'per_ticker_confusion_matrices' in conf_matrix_data:
        print(f"\nğŸ¯ åˆ†è‚¡ç¥¨æ··æ·†çŸ©é˜µåˆ†æ:")
        ticker_conf_matrices = conf_matrix_data['per_ticker_confusion_matrices']
        
        print(f"{'è‚¡ç¥¨ä»£ç ':<12} {'TP':<6} {'FP':<6} {'TN':<8} {'FN':<6} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8}")
        print("-" * 70)
        
        for ticker in sorted(ticker_conf_matrices.keys()):
            conf = ticker_conf_matrices[ticker]
            tp = conf['true_positives']
            fp = conf['false_positives']
            tn = conf['true_negatives']
            fn = conf['false_negatives']
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            
            print(f"{ticker:<12} "
                  f"{tp:<6} "
                  f"{fp:<6} "
                  f"{tn:<8,} "
                  f"{fn:<6} "
                  f"{precision:<8.4f} "
                  f"{recall:<8.4f}")
    
    return df

def main():
    parser = argparse.ArgumentParser(description="åˆ†æåˆ†è‚¡ç¥¨è¯„ä¼°ç»“æœ")
    parser.add_argument("--eval_dir", default="results/evaluation_results", 
                       help="è¯„ä¼°ç»“æœç›®å½•")
    parser.add_argument("--save_summary", action="store_true", 
                       help="ä¿å­˜åˆ†ææ‘˜è¦åˆ°æ–‡ä»¶")
    parser.add_argument("--export_csv", action="store_true", 
                       help="å¯¼å‡ºç»“æœåˆ°CSVæ–‡ä»¶")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.eval_dir):
        print(f"âŒ è¯„ä¼°ç»“æœç›®å½•ä¸å­˜åœ¨: {args.eval_dir}")
        return
    
    print("ğŸ” åŠ è½½è¯„ä¼°ç»“æœ...")
    
    # åŠ è½½è¯„ä¼°ç»“æœ
    results_data, results_file = load_latest_evaluation_results(args.eval_dir)
    if not results_data:
        print(f"âŒ åœ¨ {args.eval_dir} ä¸­æœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶")
        return
    
    print(f"âœ… å·²åŠ è½½: {results_file}")
    
    # åŠ è½½æ··æ·†çŸ©é˜µç»“æœ
    conf_matrix_data = None
    conf_result = load_confusion_matrix_results(args.eval_dir)
    if conf_result:
        conf_matrix_data, conf_file = conf_result
        print(f"âœ… å·²åŠ è½½æ··æ·†çŸ©é˜µ: {conf_file}")
    
    # åˆ†æç»“æœ
    df = analyze_per_ticker_performance(results_data, conf_matrix_data)
    
    if df is not None and args.export_csv:
        csv_file = os.path.join(args.eval_dir, "per_ticker_analysis.csv")
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ç»“æœå·²å¯¼å‡ºåˆ°: {csv_file}")
    
    if args.save_summary:
        summary_file = os.path.join(args.eval_dir, "per_ticker_summary.txt")
        # è¿™é‡Œå¯ä»¥æ·»åŠ ä¿å­˜æ‘˜è¦çš„é€»è¾‘
        print(f"\nâœ… æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")

if __name__ == "__main__":
    main() 