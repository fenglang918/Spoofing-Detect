#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Model Prediction and Visualization Analysis Script
==============================
Features:
1. Load trained model
2. Load validation set features and labels
3. Make predictions on validation set
4. Plot market data with real and predicted anomaly periods
5. Link to order data for accurate timestamps
"""

import argparse
import glob
import os
import pickle
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime, timedelta
import seaborn as sns
from pathlib import Path
import warnings
import sys
from pathlib import Path

warnings.filterwarnings('ignore')

# æ·»åŠ è®­ç»ƒè„šæœ¬è·¯å¾„ä»¥ä¾¿åŠ è½½æ¨¡å‹ç±»å’Œç‰¹å¾å‡½æ•°
sys.path.append(str(Path(__file__).parent.parent / "train"))

# Set matplotlib font for English display
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


# æ¨¡å‹ç±»å®šä¹‰ï¼ˆä»è®­ç»ƒè„šæœ¬å¤åˆ¶ï¼‰
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import average_precision_score
try:
    from xgboost import XGBClassifier
except ImportError:
    XGBClassifier = None


class EnsembleClassifier:
    """é›†æˆåˆ†ç±»å™¨ç±»ï¼ˆä»è®­ç»ƒè„šæœ¬å¤åˆ¶ï¼‰"""
    def __init__(self, models=None):
        if models is None:
            self.models = {
                'lgb': lgb.LGBMClassifier(
                    objective='binary',
                    metric='average_precision',
                    boosting_type='gbdt',
                    n_estimators=1000,
                    learning_rate=0.05,
                    num_leaves=31,
                    max_depth=6,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_alpha=10,
                    reg_lambda=10,
                    random_state=42,
                    verbose=-1
                )
            }
            
            if XGBClassifier is not None:
                self.models['xgb'] = XGBClassifier(
                    objective='binary:logistic',
                    eval_metric='aucpr',
                    n_estimators=500,
                    learning_rate=0.05,
                    max_depth=6,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_alpha=10,
                    reg_lambda=10,
                    random_state=42,
                    verbosity=0
                )
            
            self.models['rf'] = RandomForestClassifier(
                n_estimators=300,
                max_depth=8,
                min_samples_split=20,
                min_samples_leaf=10,
                random_state=42,
                n_jobs=-1
            )
        else:
            self.models = models
        
        self.weights = None
        self.fitted_models = {}
    
    def fit(self, X, y, X_val=None, y_val=None):
        """è®­ç»ƒé›†æˆæ¨¡å‹"""
        print("ğŸš€ Training ensemble models...")
        
        for name, model in self.models.items():
            print(f"  Training {name}...")
            
            if name == 'lgb' and X_val is not None:
                try:
                    model.fit(
                        X, y,
                        eval_set=[(X_val, y_val)],
                        early_stopping_rounds=100,
                        verbose=False
                    )
                except TypeError:
                    from lightgbm import early_stopping
                    model.fit(
                        X, y,
                        eval_set=[(X_val, y_val)],
                        callbacks=[early_stopping(100)]
                    )
            else:
                model.fit(X, y)
            
            self.fitted_models[name] = model
        
        if X_val is not None and y_val is not None:
            self._compute_weights(X_val, y_val)
        else:
            self.weights = {name: 1.0/len(self.models) for name in self.models.keys()}
    
    def _compute_weights(self, X_val, y_val):
        """åŸºäºéªŒè¯é›†æ€§èƒ½è®¡ç®—æƒé‡"""
        scores = {}
        for name, model in self.fitted_models.items():
            pred_proba = model.predict_proba(X_val)[:, 1]
            score = average_precision_score(y_val, pred_proba)
            scores[name] = score
            print(f"  {name} PR-AUC: {score:.4f}")
        
        total_score = sum(scores.values())
        if total_score > 0:
            self.weights = {name: score/total_score for name, score in scores.items()}
        else:
            self.weights = {name: 1.0/len(self.models) for name in self.models.keys()}
        print(f"  Weights: {self.weights}")
    
    def predict_proba(self, X):
        """é›†æˆé¢„æµ‹"""
        predictions = []
        for name, model in self.fitted_models.items():
            pred = model.predict_proba(X)[:, 1]
            predictions.append(pred * self.weights[name])
        
        ensemble_pred = np.sum(predictions, axis=0)
        return np.column_stack([1 - ensemble_pred, ensemble_pred])


def load_model(model_path):
    """åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹"""
    print(f"ğŸ“¥ Loading model from: {model_path}")
    
    with open(model_path, 'rb') as f:
        model_data = pickle.load(f)
    
    model = model_data['model']
    features = model_data['features']
    model_type = model_data.get('model_type', 'unknown')
    results = model_data.get('results', {})
    
    print(f"âœ… Model loaded successfully")
    print(f"   Type: {model_type}")
    print(f"   Features: {len(features)}")
    print(f"   Performance: PR-AUC={results.get('PR-AUC', 'N/A'):.4f}")
    
    # å¦‚æœæ˜¯é›†æˆæ¨¡å‹ï¼Œæå–ä¸»è¦æ¨¡å‹
    if hasattr(model, 'fitted_models') and 'lgb' in model.fitted_models:
        print("   Using LightGBM from ensemble")
        main_model = model.fitted_models['lgb']
        return main_model, features, model_data
    
    return model, features, model_data


def load_validation_data(data_root, valid_regex):
    """Load validation set features and labels"""
    print(f"ğŸ“Š Loading validation data with regex: {valid_regex}")
    
    # Load feature data
    feat_pats = [
        os.path.join(data_root, "features", "X_*.parquet"),
        os.path.join(data_root, "features_select", "X_*.parquet")
    ]
    
    feat_files = []
    for pat in feat_pats:
        feat_files.extend(sorted(glob.glob(pat)))
        if feat_files:
            break
    
    if not feat_files:
        raise FileNotFoundError("No feature files found")
    
    print(f"Found {len(feat_files)} feature files")
    df_feat = pd.concat([pd.read_parquet(f) for f in feat_files], ignore_index=True)
    
    # Load label data
    lab_pats = [
        os.path.join(data_root, "labels", "labels_*.parquet"),
        os.path.join(data_root, "labels_select", "labels_*.parquet"),
        os.path.join(data_root, "labels_enhanced", "labels_*.parquet")
    ]
    
    lab_files = []
    for pat in lab_pats:
        lab_files.extend(sorted(glob.glob(pat)))
        if lab_files:
            break
    
    if not lab_files:
        raise FileNotFoundError("No label files found")
    
    print(f"Found {len(lab_files)} label files")
    df_lab = pd.concat([pd.read_parquet(f) for f in lab_files], ignore_index=True)
    
    # Merge data
    df = df_feat.merge(df_lab, on=["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"], how="inner")
    print(f"Merged data shape: {df.shape}")
    
    # Filter validation set
    valid_mask = df["è‡ªç„¶æ—¥"].astype(str).str.contains(valid_regex)
    df_valid = df[valid_mask].copy()
    
    print(f"Validation data: {len(df_valid):,} records")
    print(f"Date range: {df_valid['è‡ªç„¶æ—¥'].min()} to {df_valid['è‡ªç„¶æ—¥'].max()}")
    print(f"Tickers: {df_valid['ticker'].unique()}")
    
    # Apply enhanced feature engineering (keep time info for visualization)
    print("ğŸ”§ Applying leakage-free feature cleaning...")
    
    # Keep visualization needed time and ID columns
    visualization_cols = ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·', 'y_label', 
                         'å§”æ‰˜_datetime', 'äº‹ä»¶_datetime', 'æ—¶é—´_å§”æ‰˜', 'æ—¶é—´_äº‹ä»¶']
    viz_backup = df_valid[[col for col in visualization_cols if col in df_valid.columns]].copy()
    
    try:
        # å°è¯•å¯¼å…¥é˜²æ³„éœ²æ¸…ç†å‡½æ•°
        import sys
        from pathlib import Path
        leakage_path = Path(__file__).parent.parent / "data_process" / "features"
        sys.path.append(str(leakage_path))
        
        from leakage_free_features import clean_features_for_training
        df_valid_clean = clean_features_for_training(df_valid, "y_label")
        
        # å°†é‡è¦çš„IDå’Œæ—¶é—´åˆ—é‡æ–°æ·»åŠ åˆ°æ¸…ç†åçš„æ•°æ®ä¸­
        df_valid = pd.concat([viz_backup, df_valid_clean], axis=1)
        print("âœ… Leakage-free features applied from data process script")
        print(f"   Restored visualization columns: {list(viz_backup.columns)}")
        
    except ImportError as e:
        print(f"âš ï¸ Could not import leakage cleaning functions: {e}")
        print("   Using basic feature cleaning...")
        
        # åŸºç¡€æ¸…ç†ï¼ˆä¿ç•™æ—¶é—´ä¿¡æ¯ï¼‰
        leakage_cols = [
            "å­˜æ´»æ—¶é—´_ms", "æˆäº¤ä»·æ ¼", "æˆäº¤æ•°é‡", "äº‹ä»¶ç±»å‹",
            "is_cancel_event", "is_trade_event",
            "flag_R1", "flag_R2", "enhanced_spoofing_liberal", 
            "enhanced_spoofing_moderate", "enhanced_spoofing_strict"
        ]
        
        # ç§»é™¤æ³„éœ²åˆ—ï¼ˆä¿ç•™é‡è¦çš„å¯è§†åŒ–åˆ—ï¼‰
        cols_to_remove = [col for col in leakage_cols if col in df_valid.columns and col not in visualization_cols]
        df_valid = df_valid.drop(columns=cols_to_remove)
        print(f"   Removed {len(cols_to_remove)} leakage columns")
        print(f"   Kept visualization columns: {[col for col in visualization_cols if col in df_valid.columns]}")
    
    return df_valid


def load_market_data(data_root, date, ticker):
    """åŠ è½½æŒ‡å®šæ—¥æœŸå’Œè‚¡ç¥¨çš„è¡Œæƒ…æ•°æ®"""
    market_file = os.path.join(data_root, "base_data", str(date), str(date), ticker, "è¡Œæƒ….csv")
    
    if not os.path.exists(market_file):
        raise FileNotFoundError(f"Market data not found: {market_file}")
    
    # å°è¯•ä¸åŒçš„ç¼–ç 
    encodings = ['gbk', 'utf-8', 'gb2312', 'cp936']
    df_market = None
    
    for encoding in encodings:
        try:
            df_market = pd.read_csv(market_file, encoding=encoding)
            print(f"âœ… Successfully loaded market data with encoding: {encoding}")
            break
        except UnicodeDecodeError:
            continue
    
    if df_market is None:
        raise ValueError(f"Could not decode market data file with any encoding")
    
    return df_market


def load_order_data(data_root, date, ticker):
    """åŠ è½½æŒ‡å®šæ—¥æœŸå’Œè‚¡ç¥¨çš„é€ç¬”å§”æ‰˜æ•°æ®"""
    order_file = os.path.join(data_root, "base_data", str(date), str(date), ticker, "é€ç¬”å§”æ‰˜.csv")
    
    if not os.path.exists(order_file):
        print(f"âš ï¸ Order data not found: {order_file}")
        return None
    
    # å°è¯•ä¸åŒçš„ç¼–ç 
    encodings = ['gbk', 'utf-8', 'gb2312', 'cp936']
    df_order = None
    
    for encoding in encodings:
        try:
            df_order = pd.read_csv(order_file, encoding=encoding)
            print(f"âœ… Successfully loaded order data with encoding: {encoding}")
            break
        except UnicodeDecodeError:
            continue
    
    if df_order is None:
        print(f"âš ï¸ Could not decode order data file")
        return None
    
    return df_order


def prepare_time_series(df_market):
    """å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®"""
    # å‡è®¾æ—¶é—´åˆ—æ˜¯ç¬¬4åˆ—
    time_col = df_market.columns[3]
    
    # å¦‚æœæ—¶é—´æ˜¯æ•°å­—æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ—¶é—´
    if df_market[time_col].dtype in ['int64', 'float64']:
        time_strs = df_market[time_col].astype(str).str.zfill(8)
        time_strs = time_strs.str[:2] + ':' + time_strs.str[2:4] + ':' + time_strs.str[4:6] + '.' + time_strs.str[6:]
        
        date_col = df_market.columns[2]
        date_str = str(df_market[date_col].iloc[0])
        
        datetime_strs = date_str + ' ' + time_strs
        df_market['datetime'] = pd.to_datetime(datetime_strs, format='%Y%m%d %H:%M:%S.%f', errors='coerce')
    else:
        df_market['datetime'] = pd.to_datetime(df_market[time_col], errors='coerce')
    
    # ç§»é™¤æ— æ•ˆæ—¶é—´
    df_market = df_market.dropna(subset=['datetime'])
    
    # å¤„ç†é‡å¤æ—¶é—´æ ‡ç­¾é—®é¢˜
    if df_market['datetime'].duplicated().any():
        print(f"   Warning: Found {df_market['datetime'].duplicated().sum()} duplicate timestamps, removing duplicates")
        df_market = df_market.drop_duplicates(subset=['datetime'], keep='first')
    
    # é‡ç½®ç´¢å¼•ä»¥é¿å…é‡å¤ç´¢å¼•é—®é¢˜
    df_market = df_market.reset_index(drop=True)
    
    # æŒ‰æ—¶é—´æ’åº
    df_market = df_market.sort_values('datetime')
    
    return df_market


def get_price_column(df_market):
    """è·å–ä»·æ ¼åˆ—"""
    price_candidates = []
    
    for col in df_market.columns:
        col_lower = str(col).lower()
        if any(keyword in col_lower for keyword in ['ä»·æ ¼', 'price', 'æˆäº¤ä»·', 'ä¸­é—´ä»·', 'æœ€æ–°ä»·']):
            if df_market[col].dtype in ['int64', 'float64'] and df_market[col].sum() > 0:
                price_candidates.append(col)
    
    if price_candidates:
        for col in price_candidates:
            if 'æˆäº¤' in str(col) or 'trade' in str(col).lower():
                return col
        return price_candidates[0]
    
    # ä½¿ç”¨ç¬¬5åˆ—
    if len(df_market.columns) > 4:
        return df_market.columns[4]
    
    raise ValueError("Could not identify price column in market data")


def map_orders_to_time(df_validation, df_order=None, order_id_col='äº¤æ˜“æ‰€å§”æ‰˜å·'):
    """Use existing time information in validation set directly, no need to remap"""
    df_result = df_validation.copy()
    
    # Check if time columns exist
    time_columns = ['å§”æ‰˜_datetime', 'äº‹ä»¶_datetime', 'æ—¶é—´_å§”æ‰˜', 'æ—¶é—´_äº‹ä»¶']
    available_time_cols = [col for col in time_columns if col in df_result.columns]
    
    print(f"ğŸ•’ Detected time columns: {available_time_cols}")
    
    if 'å§”æ‰˜_datetime' in df_result.columns:
        # Use order time as primary timestamp
        df_result['order_time'] = pd.to_datetime(df_result['å§”æ‰˜_datetime'])
        mapped_count = df_result['order_time'].notna().sum()
        print(f"âœ… Using å§”æ‰˜_datetime: {mapped_count:,} / {len(df_result):,} orders have timestamps")
        
    elif 'äº‹ä»¶_datetime' in df_result.columns:
        # Alternative: use event time
        df_result['order_time'] = pd.to_datetime(df_result['äº‹ä»¶_datetime'])
        mapped_count = df_result['order_time'].notna().sum()
        print(f"âœ… Using äº‹ä»¶_datetime: {mapped_count:,} / {len(df_result):,} orders have timestamps")
        
    elif 'æ—¶é—´_å§”æ‰˜' in df_result.columns and 'è‡ªç„¶æ—¥' in df_result.columns:
        # Construct timestamp from numeric time
        date_str = df_result['è‡ªç„¶æ—¥'].astype(str)
        time_str = df_result['æ—¶é—´_å§”æ‰˜'].astype(str).str.zfill(9)
        datetime_str = date_str + time_str
        df_result['order_time'] = pd.to_datetime(datetime_str, format='%Y%m%d%H%M%S%f', errors='coerce')
        mapped_count = df_result['order_time'].notna().sum()
        print(f"âœ… Constructed from æ—¶é—´_å§”æ‰˜: {mapped_count:,} / {len(df_result):,} orders have timestamps")
        
    else:
        print("âš ï¸ No available time information found, using virtual timestamps")
        # Create virtual time series
        df_result['order_time'] = pd.date_range(
            start='2025-01-01 09:30:00', 
            periods=len(df_result), 
            freq='1S'
        )
        mapped_count = len(df_result)
    
    return df_result


def make_predictions(model, df_validation, features):
    """ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    print(f"ğŸ”® Making predictions...")
    
    # æ£€æŸ¥å“ªäº›ç‰¹å¾åœ¨æ•°æ®ä¸­å­˜åœ¨
    available_features = [f for f in features if f in df_validation.columns]
    missing_features = [f for f in features if f not in df_validation.columns]
    
    print(f"   Available features: {len(available_features)}/{len(features)}")
    if missing_features:
        print(f"   Missing features: {missing_features[:10]}{'...' if len(missing_features) > 10 else ''}")
    
    # åˆ›å»ºç‰¹å¾æ•°æ®ï¼Œç¼ºå¤±çš„ç‰¹å¾ç”¨0å¡«å……
    feature_data = pd.DataFrame()
    for feature in features:
        if feature in df_validation.columns:
            feature_data[feature] = df_validation[feature]
        else:
            feature_data[feature] = 0
    
    feature_data = feature_data.fillna(0)
    
    if hasattr(model, 'predict_proba'):
        y_pred_proba = model.predict_proba(feature_data)[:, 1]
    else:
        y_pred_scores = model.predict(feature_data)
        y_pred_proba = 1 / (1 + np.exp(-y_pred_scores))
    
    df_result = df_validation.copy()
    df_result['predicted_proba'] = y_pred_proba
    df_result['predicted_binary'] = (y_pred_proba > 0.5).astype(int)
    
    # è®¡ç®—ç»Ÿè®¡å€¼ï¼ˆé¿å…Seriesè½¬æ¢é—®é¢˜ï¼‰
    try:
        true_positive = int(df_result['y_label'].sum())
    except (ValueError, TypeError):
        true_positive = int(df_result['y_label'].values.sum())
    
    try:
        pred_positive = int(df_result['predicted_binary'].sum())
    except (ValueError, TypeError):
        pred_positive = int(df_result['predicted_binary'].values.sum())
    
    print(f"âœ… Predictions completed")
    print(f"   True positives: {true_positive:,}")
    print(f"   Predicted positives: {pred_positive:,}")
    print(f"   Max prediction probability: {float(y_pred_proba.max()):.4f}")
    print(f"   Mean prediction probability: {float(y_pred_proba.mean()):.4f}")
    
    return df_result


def enhance_features_simple(df):
    """ç®€åŒ–çš„ç‰¹å¾å¢å¼ºï¼ˆä»…ç”ŸæˆåŸºç¡€å¢å¼ºç‰¹å¾ï¼‰"""
    print("ğŸ”§ Generating basic enhanced features...")
    
    # è®¡ç®—åŸºç¡€è¡ç”Ÿç‰¹å¾
    if 'relative_spread' not in df.columns and 'spread' in df.columns and 'mid_price' in df.columns:
        df['relative_spread'] = df['spread'] / df['mid_price']
    
    # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
    if 'mid_price' in df.columns:
        df['price_volatility'] = df.groupby('ticker')['mid_price'].transform(
            lambda x: x.rolling(100, min_periods=1).std()
        )
        df['price_momentum'] = df.groupby('ticker')['mid_price'].transform(
            lambda x: x.pct_change(5)
        )
    
    if 'spread' in df.columns:
        df['spread_volatility'] = df.groupby('ticker')['spread'].transform(
            lambda x: x.rolling(50, min_periods=1).std()
        )
    
    # è®¢å•æµæŒ‡æ ‡
    if 'bid1' in df.columns and 'ask1' in df.columns:
        df['order_imbalance'] = (df['bid1'] - df['ask1']) / (df['bid1'] + df['ask1'])
    else:
        df['order_imbalance'] = 0
    
    # å†å²è¡Œä¸ºç‰¹å¾
    if 'log_qty' in df.columns:
        df['hist_order_size_mean'] = df.groupby('ticker')['log_qty'].transform(
            lambda x: x.expanding().mean().shift(1)
        )
    
    if 'spread' in df.columns:
        df['hist_spread_mean'] = df.groupby('ticker')['spread'].transform(
            lambda x: x.expanding().mean().shift(1)
        )
    
    # ä»·æ ¼çº§åˆ«ç‰¹å¾
    if 'delta_mid' in df.columns and 'spread' in df.columns:
        df['at_bid'] = (df['delta_mid'] <= -df['spread']/2).astype(int)
        df['at_ask'] = (df['delta_mid'] >= df['spread']/2).astype(int)
        df['between_quotes'] = ((df['delta_mid'] > -df['spread']/2) & 
                               (df['delta_mid'] < df['spread']/2)).astype(int)
    else:
        df['at_bid'] = 0
        df['at_ask'] = 0
        df['between_quotes'] = 0
    
    # åˆ†ä½æ•°ç‰¹å¾
    for col in ['log_qty', 'spread', 'delta_mid']:
        if col in df.columns:
            df[f'{col}_rank'] = df.groupby('ticker')[col].transform(
                lambda x: x.rank(pct=True)
            )
            df[f'{col}_zscore'] = df.groupby('ticker')[col].transform(
                lambda x: (x - x.mean()) / (x.std() + 1e-8)
            )
    
    # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.fillna(0)
    
    print(f"âœ… Enhanced features created, final shape: {df.shape}")
    return df


def plot_market_with_anomalies(df_market, df_predictions, ticker, date, output_dir, 
                              prob_threshold=0.1, top_k_percent=0.05):
    """Plot market data with anomaly indicators"""
    print(f"ğŸ“ˆ Creating market visualization...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        df_market = prepare_time_series(df_market)
        price_col = get_price_column(df_market)
        
        # ç¡®ä¿æ•°æ®ä¸ä¸ºç©ºä¸”æœ‰æœ‰æ•ˆæ—¶é—´åºåˆ—
        if len(df_market) == 0 or df_market['datetime'].isna().all():
            raise ValueError("No valid time series data available")
        
        # ç¡®ä¿ç´¢å¼•å’Œåˆ—åå”¯ä¸€
        if df_market.index.duplicated().any():
            df_market = df_market.reset_index(drop=True)
        
        if df_market.columns.duplicated().any():
            df_market = df_market.loc[:, ~df_market.columns.duplicated()]
            
        # åŒæ ·å¤„ç†é¢„æµ‹æ•°æ®
        df_predictions = df_predictions.reset_index(drop=True)
        if df_predictions.columns.duplicated().any():
            df_predictions = df_predictions.loc[:, ~df_predictions.columns.duplicated()]
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12), height_ratios=[3, 1])
        
        # Main plot: price trend
        ax1.plot(df_market['datetime'], df_market[price_col], 
                 color='black', linewidth=1, alpha=0.8, label='Price Trend')
        
        # Mark real anomalies
        true_anomalies = df_predictions[df_predictions['y_label'] == 1]
        if len(true_anomalies) > 0:
            if 'order_time' in true_anomalies.columns and true_anomalies['order_time'].notna().any():
                true_times = pd.to_datetime(true_anomalies['order_time'].dropna())
            else:
                market_start = df_market['datetime'].min()
                market_end = df_market['datetime'].max()
                time_range = (market_end - market_start).total_seconds()
                n_orders = len(true_anomalies)
                estimated_times = [market_start + timedelta(seconds=i*time_range/n_orders) 
                                  for i in range(n_orders)]
                true_times = pd.Series(estimated_times)
            
            # Use flag to ensure label appears only once
            real_anomaly_labeled = False
            for time_point in true_times:
                ax1.axvline(x=time_point, color='red', alpha=0.7, linewidth=2, 
                           label='Real Anomaly' if not real_anomaly_labeled else "")
                real_anomaly_labeled = True
        
        # Mark predicted anomalies
        top_k_threshold = df_predictions['predicted_proba'].quantile(1 - top_k_percent)
        pred_anomalies = df_predictions[df_predictions['predicted_proba'] >= top_k_threshold]
        
        if len(pred_anomalies) > 0:
            if 'order_time' in pred_anomalies.columns and pred_anomalies['order_time'].notna().any():
                pred_times = pd.to_datetime(pred_anomalies['order_time'].dropna())
            else:
                market_start = df_market['datetime'].min()
                market_end = df_market['datetime'].max()
                time_range = (market_end - market_start).total_seconds()
                n_orders = len(pred_anomalies)
                estimated_times = [market_start + timedelta(seconds=i*time_range/n_orders) 
                                  for i in range(n_orders)]
                pred_times = pd.Series(estimated_times)
            
            # Use flag to ensure label appears only once
            pred_anomaly_labeled = False
            for time_point in pred_times:
                ax1.axvline(x=time_point, color='blue', alpha=0.5, linewidth=1.5, linestyle='--',
                           label='Predicted Anomaly' if not pred_anomaly_labeled else "")
                pred_anomaly_labeled = True
        
        ax1.set_title(f'{ticker} Market Trend and Anomaly Detection - {date}', fontsize=14, fontweight='bold')
        ax1.set_ylabel(f'Price ({price_col})', fontsize=12)
        ax1.grid(True, alpha=0.3)
        ax1.legend(loc='best')
        
        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
        ax1.xaxis.set_major_locator(mdates.HourLocator(interval=1))
        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
        
        # å­å›¾ï¼šé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        if len(df_predictions) > 0:
            if 'order_time' in df_predictions.columns and df_predictions['order_time'].notna().any():
                df_pred_sorted = df_predictions.dropna(subset=['order_time']).sort_values('order_time')
                pred_times_all = pd.to_datetime(df_pred_sorted['order_time'])
                pred_probas = df_pred_sorted['predicted_proba']
            else:
                market_start = df_market['datetime'].min()
                market_end = df_market['datetime'].max()
                time_range = (market_end - market_start).total_seconds()
                n_orders = len(df_predictions)
                estimated_times = [market_start + timedelta(seconds=i*time_range/n_orders) 
                                  for i in range(n_orders)]
                pred_times_all = pd.Series(estimated_times)
                pred_probas = df_predictions['predicted_proba']
            
            ax2.plot(pred_times_all, pred_probas, color='green', alpha=0.6, linewidth=1)
            ax2.fill_between(pred_times_all, pred_probas, alpha=0.3, color='green')
            
            ax2.axhline(y=prob_threshold, color='orange', linestyle=':', alpha=0.7, 
                       label=f'Probability Threshold {prob_threshold}')
            ax2.axhline(y=top_k_threshold, color='purple', linestyle=':', alpha=0.7,
                       label=f'Top {top_k_percent*100:.1f}% Threshold {top_k_threshold:.3f}')
        
        ax2.set_xlabel('Time', fontsize=12)
        ax2.set_ylabel('Prediction Probability', fontsize=12)
        ax2.set_title('Anomaly Probability Prediction', fontsize=12)
        ax2.grid(True, alpha=0.3)
        ax2.legend(loc='best')
        ax2.set_ylim(0, max(1, df_predictions['predicted_proba'].max() * 1.1))
        
        ax2.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
        ax2.xaxis.set_major_locator(mdates.HourLocator(interval=1))
        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)
        
        plt.tight_layout()
        
        output_file = os.path.join(output_dir, f'{ticker}_{date}_market_anomaly_detection.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"âœ… Market visualization saved: {output_file}")
        
        plt.close()  # å…³é—­å›¾å½¢ä»¥é‡Šæ”¾å†…å­˜
        
        return fig
        
    except Exception as e:
        plt.close('all')  # æ¸…ç†ä»»ä½•æ‰“å¼€çš„å›¾å½¢
        raise RuntimeError(f"Failed to create market visualization: {str(e)}")


def generate_summary_report(df_predictions, model_data, output_dir):
    """Generate prediction results summary report"""
    print("ğŸ“‹ Generating summary report...")
    
    from sklearn.metrics import average_precision_score, roc_auc_score
    
    y_true = df_predictions['y_label']
    y_pred_proba = df_predictions['predicted_proba']
    y_pred_binary = df_predictions['predicted_binary']
    
    # ç¡®ä¿è¾“å…¥æ˜¯æ­£ç¡®çš„æ•°ç»„æ ¼å¼
    y_true_array = np.asarray(y_true).ravel()  # å±•å¹³ä¸º1Dæ•°ç»„
    y_pred_proba_array = np.asarray(y_pred_proba).ravel()  # å±•å¹³ä¸º1Dæ•°ç»„
    
    # ç¡®ä¿æ•°ç»„é•¿åº¦ä¸€è‡´
    min_len = min(len(y_true_array), len(y_pred_proba_array))
    y_true_array = y_true_array[:min_len]
    y_pred_proba_array = y_pred_proba_array[:min_len]
    
    print(f"   Array lengths after alignment: y_true={len(y_true_array)}, y_pred={len(y_pred_proba_array)}")
    
    # ç¡®ä¿æ•°ç»„å½¢çŠ¶æ­£ç¡®
    if y_true_array.ndim != 1 or y_pred_proba_array.ndim != 1:
        raise ValueError("Input arrays must be 1-dimensional")
    
    pr_auc = average_precision_score(y_true_array, y_pred_proba_array)
    roc_auc = roc_auc_score(y_true_array, y_pred_proba_array)
    
    total_samples = len(df_predictions)
    
    # å®‰å…¨åœ°è½¬æ¢Seriesä¸ºintï¼ˆé¿å…è½¬æ¢é”™è¯¯ï¼‰
    try:
        true_positives = int(y_true_array.sum())
    except (ValueError, TypeError):
        true_positives = int(np.asarray(y_true).sum())
        
    try:
        pred_positives = int(y_pred_binary.sum())
    except (ValueError, TypeError):
        pred_positives = int(np.asarray(y_pred_binary).sum())
    
    precision_at_k = {}
    for k in [0.001, 0.005, 0.01, 0.05, 0.1]:
        k_samples = max(1, int(total_samples * k))
        top_k_idx = y_pred_proba.nlargest(k_samples).index
        prec_k = np.asarray(y_true.loc[top_k_idx]).mean()
        precision_at_k[f"P@{k*100:.1f}%"] = prec_k
    
    report = f"""# Model Prediction Summary Report

## Model Information
- **Model Type**: {model_data.get('model_type', 'Unknown')}
- **Feature Count**: {len(model_data['features'])}
- **Training Performance**: {model_data.get('results', {}).get('PR-AUC', 'N/A')}

## Validation Set Prediction Results

### Basic Statistics
- **Total Samples**: {total_samples:,}
- **Real Anomalies**: {true_positives:,} ({true_positives/total_samples*100:.3f}%)
- **Predicted Anomalies**: {pred_positives:,} ({pred_positives/total_samples*100:.3f}%)

### Performance Metrics
- **PR-AUC**: {pr_auc:.6f}
- **ROC-AUC**: {roc_auc:.6f}

### Precision@K Analysis
"""
    
    for metric, value in precision_at_k.items():
        report += f"- **{metric}**: {value:.6f}\n"
    
    report += f"""
### Prediction Probability Distribution
- **Minimum**: {y_pred_proba.min():.6f}
- **Maximum**: {y_pred_proba.max():.6f}
- **Mean**: {y_pred_proba.mean():.6f}
- **Median**: {y_pred_proba.median():.6f}
- **Standard Deviation**: {y_pred_proba.std():.6f}
"""
    
    if 'ticker' in df_predictions.columns:
        try:
            ticker_stats = df_predictions.groupby('ticker').agg({
                'y_label': ['count', 'sum'],
                'predicted_proba': ['mean', 'max'],
                'predicted_binary': 'sum'
            }).round(4)
            
            report += "\n### Results by Stock\n\n" + ticker_stats.to_string() + "\n"
        except Exception as e:
            print(f"âš ï¸ Stock grouping statistics failed: {e}")
            # Simplified stock statistics
            ticker_simple_stats = df_predictions.groupby('ticker').size()
            report += f"\n### Sample Count by Stock\n\n{ticker_simple_stats.to_string()}\n"
    
    report_file = os.path.join(output_dir, "prediction_summary_report.md")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… Summary report saved: {report_file}")
    
    return report


def create_simple_summary_plot(df_predictions, ticker, date, output_dir):
    """Create simple statistical summary plots"""
    print(f"ğŸ“Š Creating simple summary visualization...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # ç¡®ä¿æ•°æ®æ²¡æœ‰é‡å¤ç´¢å¼•å’Œé‡å¤åˆ—
        df_predictions = df_predictions.reset_index(drop=True)
        
        # æ£€æŸ¥å¹¶å¤„ç†é‡å¤åˆ—å
        if df_predictions.columns.duplicated().any():
            df_predictions = df_predictions.loc[:, ~df_predictions.columns.duplicated()]
            print(f"   Removed duplicate columns")
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. Prediction probability distribution histogram
        ax1.hist(df_predictions['predicted_proba'], bins=50, alpha=0.7, color='blue', edgecolor='black')
        ax1.set_title('Prediction Probability Distribution', fontsize=12)
        ax1.set_xlabel('Prediction Probability')
        ax1.set_ylabel('Frequency')
        ax1.grid(True, alpha=0.3)
        
        # 2. Box plot of real labels vs prediction probabilities
        true_anomalies = df_predictions[df_predictions['y_label'] == 1]['predicted_proba']
        normal_samples = df_predictions[df_predictions['y_label'] == 0]['predicted_proba']
        
        ax2.boxplot([normal_samples, true_anomalies], labels=['Normal', 'Anomaly'])
        ax2.set_title('Prediction Probability Distribution by Real Label', fontsize=12)
        ax2.set_ylabel('Prediction Probability')
        ax2.grid(True, alpha=0.3)
        
        # 3. Top K ç²¾åº¦åˆ†æ
        k_values = [0.001, 0.005, 0.01, 0.05, 0.1]
        precisions = []
        
        for k in k_values:
            k_samples = max(1, int(len(df_predictions) * k))
            top_k_idx = df_predictions['predicted_proba'].nlargest(k_samples).index
            prec_k = df_predictions.loc[top_k_idx, 'y_label'].mean()
            precisions.append(prec_k)
        
        ax3.plot([k*100 for k in k_values], precisions, 'bo-', linewidth=2, markersize=8)
        ax3.set_title('Precision@K Analysis', fontsize=12)
        ax3.set_xlabel('Top K%')
        ax3.set_ylabel('Precision')
        ax3.grid(True, alpha=0.3)
        
        # 4. Statistical summary
        stats_text = f"""Sample Statistics:
Total Samples: {len(df_predictions):,}
Real Anomalies: {int(df_predictions['y_label'].sum()):,}
Anomaly Rate: {df_predictions['y_label'].mean()*100:.3f}%

Prediction Statistics:
Max Probability: {df_predictions['predicted_proba'].max():.4f}
Mean Probability: {df_predictions['predicted_proba'].mean():.4f}
Median Probability: {df_predictions['predicted_proba'].median():.4f}"""
        
        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=11,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgray'))
        ax4.set_title('Statistical Summary', fontsize=12)
        ax4.axis('off')
        
        plt.suptitle(f'{ticker} - {date} Prediction Analysis', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        output_file = os.path.join(output_dir, f'{ticker}_{date}_simple_summary.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"âœ… Simple summary saved: {output_file}")
        
        plt.close()
        return fig
        
    except Exception as e:
        plt.close('all')
        print(f"âš ï¸ Simple visualization failed: {e}")
        return None


def plot_monthly_summary(df_predictions, month_str, output_dir, prob_threshold=0.1, top_k_percent=0.05):
    """ç”Ÿæˆæ•´ä¸ªæœˆçš„æ±‡æ€»å›¾è¡¨"""
    print(f"ğŸ“Š Creating monthly summary visualization for {month_str}...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # æŒ‰æ—¥æœŸèšåˆæ•°æ®
        daily_stats = df_predictions.groupby('è‡ªç„¶æ—¥').agg({
            'y_label': ['count', 'sum'],
            'predicted_proba': ['mean', 'max', 'std'],
            'predicted_binary': 'sum'
        }).round(4)
        
        # å±•å¹³åˆ—å
        daily_stats.columns = ['_'.join(col).strip() for col in daily_stats.columns.values]
        daily_stats = daily_stats.reset_index()
        
        # è½¬æ¢æ—¥æœŸ
        daily_stats['date'] = pd.to_datetime(daily_stats['è‡ªç„¶æ—¥'].astype(str), format='%Y%m%d')
        daily_stats = daily_stats.sort_values('date')
        
        # è®¡ç®—æ¯æ—¥ç»Ÿè®¡
        daily_stats['anomaly_rate'] = daily_stats['y_label_sum'] / daily_stats['y_label_count']
        daily_stats['pred_anomaly_rate'] = daily_stats['predicted_binary_sum'] / daily_stats['y_label_count']
        
        # åˆ›å»ºå›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
        
        # 1. æ¯æ—¥çœŸå®å¼‚å¸¸æ•°é‡å’Œé¢„æµ‹å¼‚å¸¸æ•°é‡
        ax1.plot(daily_stats['date'], daily_stats['y_label_sum'], 'ro-', 
                label='Real Anomalies', linewidth=2, markersize=6)
        ax1.plot(daily_stats['date'], daily_stats['predicted_binary_sum'], 'bo-', 
                label='Predicted Anomalies', linewidth=2, markersize=6)
        ax1.set_title(f'Daily Anomaly Counts - {month_str}', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Anomaly Count')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
        
        # 2. æ¯æ—¥å¼‚å¸¸ç‡
        ax2.plot(daily_stats['date'], daily_stats['anomaly_rate'] * 100, 'r-', 
                label='Real Anomaly Rate', linewidth=2)
        ax2.plot(daily_stats['date'], daily_stats['pred_anomaly_rate'] * 100, 'b--', 
                label='Predicted Anomaly Rate', linewidth=2)
        ax2.set_title(f'Daily Anomaly Rates - {month_str}', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Anomaly Rate (%)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)
        
        # 3. æ¯æ—¥å¹³å‡é¢„æµ‹æ¦‚ç‡
        ax3.plot(daily_stats['date'], daily_stats['predicted_proba_mean'], 'g-', 
                linewidth=2, label='Mean Prediction Probability')
        ax3.fill_between(daily_stats['date'], 
                        daily_stats['predicted_proba_mean'] - daily_stats['predicted_proba_std'],
                        daily_stats['predicted_proba_mean'] + daily_stats['predicted_proba_std'],
                        alpha=0.3, color='green')
        ax3.axhline(y=prob_threshold, color='orange', linestyle=':', alpha=0.7, 
                   label=f'Threshold {prob_threshold}')
        ax3.set_title(f'Daily Average Prediction Probability - {month_str}', fontsize=14, fontweight='bold')
        ax3.set_ylabel('Prediction Probability')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
        
        # 4. æœˆåº¦ç»Ÿè®¡æ€»ç»“
        total_samples = len(df_predictions)
        total_real_anomalies = int(df_predictions['y_label'].sum())
        total_pred_anomalies = int(df_predictions['predicted_binary'].sum())
        overall_anomaly_rate = total_real_anomalies / total_samples * 100
        
        # æŒ‰è‚¡ç¥¨ç»Ÿè®¡
        if 'ticker' in df_predictions.columns:
            ticker_stats = df_predictions.groupby('ticker').agg({
                'y_label': ['count', 'sum'],
                'predicted_proba': 'mean'
            }).round(4)
            ticker_stats.columns = ['_'.join(col).strip() for col in ticker_stats.columns.values]
            
            # æŸ±çŠ¶å›¾æ˜¾ç¤ºå„è‚¡ç¥¨çš„å¼‚å¸¸æƒ…å†µ
            tickers = ticker_stats.index.tolist()
            real_counts = ticker_stats['y_label_sum'].tolist()
            
            x_pos = np.arange(len(tickers))
            ax4.bar(x_pos, real_counts, alpha=0.7, color='red', label='Real Anomalies')
            ax4.set_xlabel('Stock Ticker')
            ax4.set_ylabel('Anomaly Count')
            ax4.set_title(f'Anomaly Count by Stock - {month_str}', fontsize=14, fontweight='bold')
            ax4.set_xticks(x_pos)
            ax4.set_xticklabels(tickers, rotation=45)
            ax4.grid(True, alpha=0.3)
            ax4.legend()
        else:
            # å¦‚æœæ²¡æœ‰è‚¡ç¥¨ä¿¡æ¯ï¼Œæ˜¾ç¤ºæ–‡æœ¬ç»Ÿè®¡
            stats_text = f"""Monthly Summary Statistics:

Total Samples: {total_samples:,}
Real Anomalies: {total_real_anomalies:,}
Predicted Anomalies: {total_pred_anomalies:,}
Overall Anomaly Rate: {overall_anomaly_rate:.3f}%

Trading Days: {len(daily_stats)}
Avg Daily Samples: {total_samples/len(daily_stats):.0f}
Avg Daily Anomalies: {total_real_anomalies/len(daily_stats):.1f}"""
            
            ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=12,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue'))
            ax4.set_title(f'Monthly Statistics Summary - {month_str}', fontsize=14, fontweight='bold')
            ax4.axis('off')
        
        plt.suptitle(f'Monthly Anomaly Detection Summary - {month_str}', fontsize=18, fontweight='bold')
        plt.tight_layout()
        
        output_file = os.path.join(output_dir, f'monthly_summary_{month_str}.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"âœ… Monthly summary saved: {output_file}")
        
        plt.close()
        
        # ç”Ÿæˆæœˆåº¦æ±‡æ€»æŠ¥å‘Š
        monthly_report = f"""# Monthly Summary Report - {month_str}

## Overall Statistics
- **Total Samples**: {total_samples:,}
- **Real Anomalies**: {total_real_anomalies:,} ({overall_anomaly_rate:.3f}%)
- **Predicted Anomalies**: {total_pred_anomalies:,}
- **Trading Days**: {len(daily_stats)}

## Daily Statistics
- **Average Daily Samples**: {total_samples/len(daily_stats):.0f}
- **Average Daily Anomalies**: {total_real_anomalies/len(daily_stats):.1f}
- **Max Daily Anomalies**: {daily_stats['y_label_sum'].max()}
- **Min Daily Anomalies**: {daily_stats['y_label_sum'].min()}

## Performance Summary
- **Average Prediction Probability**: {df_predictions['predicted_proba'].mean():.6f}
- **Max Prediction Probability**: {df_predictions['predicted_proba'].max():.6f}
- **Prediction Standard Deviation**: {df_predictions['predicted_proba'].std():.6f}
"""
        
        report_file = os.path.join(output_dir, f'monthly_summary_report_{month_str}.md')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(monthly_report)
        
        print(f"âœ… Monthly summary report saved: {report_file}")
        
        return fig
        
    except Exception as e:
        plt.close('all')
        raise RuntimeError(f"Failed to create monthly summary: {str(e)}")


def main():
    parser = argparse.ArgumentParser(description="Model Prediction and Visualization Analysis")
    parser.add_argument("--data_root", required=True, help="Data root directory")
    parser.add_argument("--model_path", required=True, help="Path to trained model file")
    parser.add_argument("--valid_regex", default="202505", help="Validation set date regex")
    parser.add_argument("--output_dir", default="results/prediction_analysis", help="Output directory")
    parser.add_argument("--prob_threshold", type=float, default=0.1, help="Anomaly probability threshold")
    parser.add_argument("--top_k_percent", type=float, default=0.05, help="Top K% prediction display ratio")
    parser.add_argument("--max_plots", type=int, default=5, help="Maximum number of plots")
    
    args = parser.parse_args()
    
    print("ğŸš€ Starting model prediction and visualization analysis")
    print(f"   Data root: {args.data_root}")
    print(f"   Model: {args.model_path}")
    print(f"   Validation regex: {args.valid_regex}")
    print(f"   Output: {args.output_dir}")
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    try:
        # 1. Load model
        model, features, model_data = load_model(args.model_path)
        
        # 2. Load validation data
        df_validation = load_validation_data(args.data_root, args.valid_regex)
        
        # 3. Make predictions
        df_predictions = make_predictions(model, df_validation, features)
        
        # 4. Group by date and stock for visualization
        ticker_date_groups = df_predictions.groupby(['ticker', 'è‡ªç„¶æ—¥'])
        plot_count = 0
        
        for (ticker, date), group_data in ticker_date_groups:
            if plot_count >= args.max_plots:
                print(f"ğŸ“Š Reached maximum plot limit ({args.max_plots})")
                break
            
            print(f"\nğŸ“ˆ Processing: {ticker} - {date}")
            print(f"   Sample count: {len(group_data):,}")
            
            # Calculate statistics (avoid Series conversion issues)
            try:
                true_positive = int(group_data['y_label'].sum())
            except (ValueError, TypeError):
                true_positive = int(group_data['y_label'].values.sum())
            
            try:
                high_prob_pred = int((group_data['predicted_proba'] > args.prob_threshold).sum())
            except (ValueError, TypeError):
                high_prob_pred = int((group_data['predicted_proba'] > args.prob_threshold).values.sum())
            
            print(f"   Real anomalies: {true_positive:,}")
            print(f"   High probability predictions: {high_prob_pred:,}")
            
            try:
                df_market = load_market_data(args.data_root, date, ticker)
                df_order = load_order_data(args.data_root, date, ticker)
                
                group_data_with_time = map_orders_to_time(group_data, df_order)
                
                try:
                    plot_market_with_anomalies(
                        df_market, group_data_with_time, ticker, date, args.output_dir,
                        prob_threshold=args.prob_threshold, top_k_percent=args.top_k_percent
                    )
                except Exception as market_plot_error:
                    print(f"âš ï¸ Market plot failed, creating simple summary instead: {market_plot_error}")
                    create_simple_summary_plot(group_data, ticker, date, args.output_dir)
                
                plot_count += 1
                
            except Exception as e:
                print(f"âš ï¸ Skipping {ticker}-{date}: {e}")
                continue
        
        # 5. Generate summary report
        generate_summary_report(df_predictions, model_data, args.output_dir)
        
        # 6. ç”Ÿæˆæœˆåº¦æ±‡æ€»å›¾è¡¨
        print(f"\nğŸ“Š Generating monthly summary...")
        month_str = args.valid_regex
        try:
            plot_monthly_summary(df_predictions, month_str, args.output_dir,
                                prob_threshold=args.prob_threshold, top_k_percent=args.top_k_percent)
        except Exception as e:
            print(f"âš ï¸ Monthly summary failed: {e}")
        
        print(f"\nâœ… Analysis complete!")
        print(f"   Generated {plot_count} daily visualization plots")
        print(f"   Generated monthly summary")
        print(f"   Output directory: {args.output_dir}")
        
    except Exception as e:
        print(f"âŒ Analysis failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

"""
Usage example:
--prob_threshold (é»˜è®¤: 0.1)
ä½œç”¨: å¼‚å¸¸æ¦‚ç‡é˜ˆå€¼
è¯´æ˜: è¶…è¿‡æ­¤æ¦‚ç‡çš„é¢„æµ‹è¢«æ ‡è®°ä¸ºå¼‚å¸¸ï¼Œç”¨äºå¯è§†åŒ–ä¸­çš„é˜ˆå€¼çº¿
å–å€¼èŒƒå›´: 0.0 - 1.0
ç¤ºä¾‹: 0.1 (10%çš„æ¦‚ç‡é˜ˆå€¼)
--top_k_percent (é»˜è®¤: 0.05)
ä½œç”¨: Top K%é¢„æµ‹æ˜¾ç¤ºæ¯”ä¾‹
è¯´æ˜: æ˜¾ç¤ºæ¦‚ç‡æœ€é«˜çš„å‰K%é¢„æµ‹ä½œä¸ºå¼‚å¸¸æ ‡è®°
å–å€¼èŒƒå›´: 0.0 - 1.0
ç¤ºä¾‹: 0.05 (æ˜¾ç¤ºå‰5%çš„é«˜æ¦‚ç‡é¢„æµ‹)
--max_plots (é»˜è®¤: 5)
ä½œç”¨: æœ€å¤§ç”Ÿæˆå›¾è¡¨æ•°é‡
è¯´æ˜: é™åˆ¶ç”Ÿæˆçš„è‚¡ç¥¨-æ—¥æœŸç»„åˆå›¾è¡¨æ•°é‡ï¼Œé¿å…ç”Ÿæˆè¿‡å¤šæ–‡ä»¶
ç¤ºä¾‹: 5 (æœ€å¤šç”Ÿæˆ5ä¸ªå›¾è¡¨)

# ç”Ÿæˆæ•´ä¸ªæœˆçš„åˆ†æï¼ˆæ¯æ—¥å•ç‹¬å›¾è¡¨ + æœˆåº¦æ±‡æ€»å›¾è¡¨ï¼‰
python scripts/analysis/model_prediction_visualization.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --model_path "results/trained_models/spoofing_model_Enhanced_undersample_Ensemble.pkl" \
  --valid_regex "202505" \
  --output_dir "results/prediction_visualization/202505_full_month" \
  --prob_threshold 0.01 \
  --top_k_percent 0.005 \
  --max_plots 50

è¾“å‡ºæ–‡ä»¶è¯´æ˜:
1. æ¯ä¸ªè‚¡ç¥¨-æ—¥æœŸç»„åˆçš„å•ç‹¬å›¾è¡¨: ticker_date_market_anomaly_detection.png
2. æ¯ä¸ªè‚¡ç¥¨-æ—¥æœŸçš„ç®€å•æ±‡æ€»: ticker_date_simple_summary.png  
3. æœˆåº¦æ±‡æ€»å›¾è¡¨: monthly_summary_YYYYMM.png (åŒ…å«4ä¸ªå­å›¾)
   - æ¯æ—¥å¼‚å¸¸æ•°é‡è¶‹åŠ¿
   - æ¯æ—¥å¼‚å¸¸ç‡å˜åŒ–
   - æ¯æ—¥å¹³å‡é¢„æµ‹æ¦‚ç‡
   - æŒ‰è‚¡ç¥¨çš„å¼‚å¸¸ç»Ÿè®¡
4. æœˆåº¦æ±‡æ€»æŠ¥å‘Š: monthly_summary_report_YYYYMM.md
5. æ•´ä½“é¢„æµ‹æŠ¥å‘Š: prediction_summary_report.md
""" 