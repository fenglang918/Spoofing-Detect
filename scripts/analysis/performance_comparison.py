#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Performance Comparison and Analysis
-----------------------------------
å¯¹æ¯”ä¸åŒæ¨¡å‹é…ç½®çš„æ€§èƒ½ï¼š
â€¢ åŸºçº¿æ¨¡å‹ vs å¢å¼ºæ¨¡å‹
â€¢ ä¸åŒé‡‡æ ·ç­–ç•¥çš„æ•ˆæœ
â€¢ ç‰¹å¾é‡è¦æ€§åˆ†æ
â€¢ æ¨¡å‹è¯Šæ–­å’Œå»ºè®®
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import argparse

def load_results(results_dir):
    """åŠ è½½ä¸åŒå®éªŒçš„ç»“æœ"""
    results = {}
    
    results_path = Path(results_dir)
    
    # å¯»æ‰¾ç»“æœæ–‡ä»¶
    for result_file in results_path.glob("*.json"):
        experiment_name = result_file.stem
        with open(result_file, 'r') as f:
            results[experiment_name] = json.load(f)
    
    return results

def create_performance_comparison(results):
    """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # å‡†å¤‡æ•°æ®
    experiments = list(results.keys())
    metrics = ['PR-AUC', 'ROC-AUC', 'Precision@0.1%', 'Precision@0.5%', 'Precision@1.0%']
    
    # 1. ä¸»è¦æŒ‡æ ‡å¯¹æ¯”
    metric_data = []
    for exp in experiments:
        for metric in metrics:
            if metric in results[exp]:
                metric_data.append({
                    'Experiment': exp,
                    'Metric': metric,
                    'Value': results[exp][metric]
                })
    
    metric_df = pd.DataFrame(metric_data)
    
    # PR-AUCå¯¹æ¯”
    pr_auc_data = metric_df[metric_df['Metric'] == 'PR-AUC']
    axes[0, 0].bar(pr_auc_data['Experiment'], pr_auc_data['Value'])
    axes[0, 0].set_title('PR-AUC Comparison')
    axes[0, 0].set_ylabel('PR-AUC')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # ROC-AUCå¯¹æ¯”
    roc_auc_data = metric_df[metric_df['Metric'] == 'ROC-AUC']
    axes[0, 1].bar(roc_auc_data['Experiment'], roc_auc_data['Value'])
    axes[0, 1].set_title('ROC-AUC Comparison')
    axes[0, 1].set_ylabel('ROC-AUC')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # Precision@Kå¯¹æ¯”
    precision_data = metric_df[metric_df['Metric'].str.contains('Precision@')]
    if not precision_data.empty:
        pivot_data = precision_data.pivot(index='Experiment', columns='Metric', values='Value')
        pivot_data.plot(kind='bar', ax=axes[0, 2])
        axes[0, 2].set_title('Precision@K Comparison')
        axes[0, 2].set_ylabel('Precision')
        axes[0, 2].tick_params(axis='x', rotation=45)
        axes[0, 2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # 2. è®­ç»ƒæ—¶é—´å¯¹æ¯”
    if any('training_time' in results[exp] for exp in experiments):
        training_times = [results[exp].get('training_time', 0) for exp in experiments]
        axes[1, 0].bar(experiments, training_times)
        axes[1, 0].set_title('Training Time Comparison')
        axes[1, 0].set_ylabel('Time (seconds)')
        axes[1, 0].tick_params(axis='x', rotation=45)
    
    # 3. æ¨¡å‹å¤æ‚åº¦å¯¹æ¯”
    if any('n_features' in results[exp] for exp in experiments):
        n_features = [results[exp].get('n_features', 0) for exp in experiments]
        axes[1, 1].bar(experiments, n_features)
        axes[1, 1].set_title('Feature Count Comparison')
        axes[1, 1].set_ylabel('Number of Features')
        axes[1, 1].tick_params(axis='x', rotation=45)
    
    # 4. æ•°æ®åˆ†å¸ƒå¯¹æ¯”
    if any('positive_ratio' in results[exp] for exp in experiments):
        pos_ratios = [results[exp].get('positive_ratio', 0) * 100 for exp in experiments]
        axes[1, 2].bar(experiments, pos_ratios)
        axes[1, 2].set_title('Positive Sample Ratio')
        axes[1, 2].set_ylabel('Positive Ratio (%)')
        axes[1, 2].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    return fig

def analyze_feature_importance(results):
    """åˆ†æç‰¹å¾é‡è¦æ€§å˜åŒ–"""
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # æ”¶é›†æ‰€æœ‰å®éªŒçš„ç‰¹å¾é‡è¦æ€§
    all_features = set()
    for exp in results:
        if 'feature_importance' in results[exp]:
            all_features.update(results[exp]['feature_importance'].keys())
    
    if not all_features:
        print("No feature importance data found")
        return None
    
    # åˆ›å»ºç‰¹å¾é‡è¦æ€§çŸ©é˜µ
    importance_matrix = []
    experiment_names = []
    
    for exp in results:
        if 'feature_importance' in results[exp]:
            importance_row = []
            for feature in sorted(all_features):
                importance_row.append(results[exp]['feature_importance'].get(feature, 0))
            importance_matrix.append(importance_row)
            experiment_names.append(exp)
    
    if importance_matrix:
        # çƒ­åŠ›å›¾
        importance_df = pd.DataFrame(
            importance_matrix, 
            columns=sorted(all_features),
            index=experiment_names
        )
        
        # é€‰æ‹©topç‰¹å¾
        top_features = importance_df.mean().nlargest(20).index
        
        sns.heatmap(
            importance_df[top_features], 
            annot=True, 
            cmap='viridis', 
            ax=axes[0]
        )
        axes[0].set_title('Feature Importance Heatmap (Top 20)')
        axes[0].set_xlabel('Features')
        axes[0].set_ylabel('Experiments')
        
        # ç‰¹å¾é‡è¦æ€§å˜åŒ–
        if len(experiment_names) > 1:
            for feature in top_features[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªç‰¹å¾
                values = importance_df[feature].values
                axes[1].plot(range(len(experiment_names)), values, marker='o', label=feature)
            
            axes[1].set_title('Top 5 Feature Importance Trends')
            axes[1].set_xlabel('Experiments')
            axes[1].set_ylabel('Importance')
            axes[1].set_xticks(range(len(experiment_names)))
            axes[1].set_xticklabels(experiment_names, rotation=45)
            axes[1].legend()
    
    plt.tight_layout()
    return fig

def generate_recommendations(results):
    """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
    recommendations = []
    
    # æ‰¾åˆ°æœ€ä½³æ€§èƒ½çš„å®éªŒ
    best_pr_auc = 0
    best_experiment = None
    
    for exp, result in results.items():
        pr_auc = result.get('PR-AUC')
        if pr_auc is not None and pr_auc > best_pr_auc:
            best_pr_auc = pr_auc
            best_experiment = exp
    
    if best_experiment:
        recommendations.append(f"ğŸ† Best performing model: {best_experiment} (PR-AUC: {best_pr_auc:.4f})")
    
    # åˆ†ææ€§èƒ½å·®å¼‚ - ä¿®å¤é™¤é›¶é”™è¯¯
    pr_aucs = []
    for result in results.values():
        pr_auc = result.get('PR-AUC')
        if pr_auc is not None and pr_auc > 0:  # ç¡®ä¿éNoneä¸”éé›¶
            pr_aucs.append(pr_auc)
    
    if len(pr_aucs) > 1:
        min_pr_auc = min(pr_aucs)
        max_pr_auc = max(pr_aucs)
        
        if min_pr_auc > 0:  # ç¡®ä¿åˆ†æ¯ä¸ä¸ºé›¶
            improvement = (max_pr_auc - min_pr_auc) / min_pr_auc * 100
            if improvement > 50:
                recommendations.append(f"ğŸ’¡ Significant improvement achieved: {improvement:.1f}% gain from optimization")
            elif improvement < 10:
                recommendations.append("âš ï¸ Limited improvement - consider more advanced techniques")
        else:
            recommendations.append("âš ï¸ Some experiments had zero PR-AUC - check model training")
    elif len(pr_aucs) == 1:
        recommendations.append("â„¹ï¸ Only one valid experiment result - need more experiments for comparison")
    else:
        recommendations.append("âŒ No valid PR-AUC results found - check experiment outputs")
    
    # æ£€æŸ¥æ•°æ®ä¸å¹³è¡¡é—®é¢˜
    for exp, result in results.items():
        if 'positive_ratio' in result and result['positive_ratio'] is not None:
            pos_ratio = result['positive_ratio'] * 100
            if pos_ratio < 0.1:
                recommendations.append(f"âš–ï¸ Severe class imbalance in {exp} ({pos_ratio:.3f}%) - consider better sampling")
            elif pos_ratio > 10:
                recommendations.append(f"ğŸ“ˆ Good class balance in {exp} ({pos_ratio:.1f}%)")
    
    # ç‰¹å¾æ•°é‡åˆ†æ
    feature_counts = []
    for result in results.values():
        n_features = result.get('n_features')
        if n_features is not None and n_features > 0:
            feature_counts.append(n_features)
    
    if len(feature_counts) > 1 and min(feature_counts) > 0:
        if max(feature_counts) > min(feature_counts) * 2:
            recommendations.append("ğŸ”§ Feature engineering made significant impact - continue exploring features")
    
    # è®­ç»ƒæ—¶é—´åˆ†æ
    training_times = []
    for result in results.values():
        training_time = result.get('training_time')
        if training_time is not None and training_time > 0:
            training_times.append(training_time)
    
    if training_times and max(training_times) > 300:  # 5 minutes
        recommendations.append("â±ï¸ Long training time detected - consider model simplification for production")
    
    return recommendations

def create_summary_report(results, output_dir):
    """åˆ›å»ºæ€»ç»“æŠ¥å‘Š"""
    report = []
    report.append("# Spoofing Detection Model Performance Report\n")
    report.append(f"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # å®éªŒæ¦‚è§ˆ
    report.append("## Experiments Overview\n")
    for exp, result in results.items():
        report.append(f"### {exp}")
        
        # å®‰å…¨æ ¼å¼åŒ–PR-AUC
        pr_auc = result.get('PR-AUC', 'N/A')
        if isinstance(pr_auc, (int, float)) and pr_auc is not None:
            report.append(f"- PR-AUC: {pr_auc:.6f}")
        else:
            report.append(f"- PR-AUC: {pr_auc}")
        
        # å®‰å…¨æ ¼å¼åŒ–ROC-AUC
        roc_auc = result.get('ROC-AUC', 'N/A')
        if isinstance(roc_auc, (int, float)) and roc_auc is not None:
            report.append(f"- ROC-AUC: {roc_auc:.6f}")
        else:
            report.append(f"- ROC-AUC: {roc_auc}")
        
        # Featuresæ•°é‡
        n_features = result.get('n_features', 'N/A')
        report.append(f"- Features: {n_features}")
        
        # å®‰å…¨æ ¼å¼åŒ–è®­ç»ƒæ—¶é—´
        training_time = result.get('training_time', 'N/A')
        if isinstance(training_time, (int, float)) and training_time is not None:
            report.append(f"- Training time: {training_time:.1f}s")
        else:
            report.append(f"- Training time: {training_time}")
        
        # å®‰å…¨æ ¼å¼åŒ–æ­£æ ·æœ¬æ¯”ä¾‹
        positive_ratio = result.get('positive_ratio', 'N/A')
        if isinstance(positive_ratio, (int, float)) and positive_ratio is not None:
            report.append(f"- Positive ratio: {positive_ratio:.4%}\n")
        else:
            report.append(f"- Positive ratio: {positive_ratio}\n")
    
    # æ€§èƒ½å¯¹æ¯”
    report.append("## Performance Comparison\n")
    metrics_df = pd.DataFrame(results).T
    if not metrics_df.empty:
        report.append(metrics_df.to_markdown())
        report.append("\n")
    
    # å»ºè®®
    recommendations = generate_recommendations(results)
    if recommendations:
        report.append("## Recommendations\n")
        for rec in recommendations:
            report.append(f"- {rec}")
        report.append("\n")
    
    # ä¿å­˜æŠ¥å‘Š
    output_path = Path(output_dir) / "performance_report.md"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    print(f"ğŸ“Š Report saved to: {output_path}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--results_dir", required=True, help="Results directory")
    parser.add_argument("--output_dir", default="analysis_output", help="Output directory")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # åŠ è½½ç»“æœ
    print("ğŸ“ Loading experiment results...")
    results = load_results(args.results_dir)
    
    if not results:
        print("âŒ No results found in the specified directory")
        return
    
    print(f"ğŸ” Found {len(results)} experiments: {list(results.keys())}")
    
    # åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾
    print("ğŸ“Š Creating performance comparison plots...")
    perf_fig = create_performance_comparison(results)
    perf_fig.savefig(output_dir / "performance_comparison.png", dpi=300, bbox_inches='tight')
    plt.close(perf_fig)
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("ğŸ”§ Analyzing feature importance...")
    feat_fig = analyze_feature_importance(results)
    if feat_fig:
        feat_fig.savefig(output_dir / "feature_importance_analysis.png", dpi=300, bbox_inches='tight')
        plt.close(feat_fig)
    
    # ç”Ÿæˆå»ºè®®
    print("ğŸ’¡ Generating recommendations...")
    recommendations = generate_recommendations(results)
    
    print("\nğŸ¯ Key Recommendations:")
    for rec in recommendations:
        print(f"  {rec}")
    
    # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
    print("ğŸ“ Creating summary report...")
    create_summary_report(results, output_dir)
    
    print(f"\nâœ… Analysis complete! Results saved to: {output_dir}")

if __name__ == "__main__":
    main()

"""
# ä½¿ç”¨ç¤ºä¾‹
python scripts/analysis/performance_comparison.py \
    --results_dir "experiments/results" \
    --output_dir "analysis_output"
""" 