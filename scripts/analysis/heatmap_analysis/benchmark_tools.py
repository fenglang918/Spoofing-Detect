#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Heatmap Analysis Benchmark Tools
===============================
æ•´åˆçš„æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·é›†
åŒ…å«ï¼šå¹¶å‘å¼‚å¸¸æ£€æµ‹æµ‹è¯•ã€é¢„æµ‹çƒ­åŠ›å›¾æµ‹è¯•ã€é€šç”¨æ€§èƒ½æµ‹è¯•
"""

import time
import numpy as np
import pandas as pd
import argparse
import multiprocessing as mp
from pathlib import Path
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import pickle
import joblib

# å¯¼å…¥æ ¸å¿ƒæ£€æµ‹å‡½æ•°
import sys
sys.path.append(str(Path(__file__).parent))
from manipulation_detection_heatmap import (
    ManipulationDetector, 
    parallel_statistical_features,
    parallel_anomaly_detection,
    HeatmapVisualizer,
    PerformanceMonitor
)

class BenchmarkSuite:
    """åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, output_dir="benchmark_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.results = {}
        
    def generate_synthetic_data(self, n_samples=10000, n_stocks=50):
        """ç”Ÿæˆåˆæˆæµ‹è¯•æ•°æ®"""
        print(f"ğŸ”§ ç”Ÿæˆæµ‹è¯•æ•°æ®: {n_samples:,} æ ·æœ¬, {n_stocks} è‚¡ç¥¨")
        
        np.random.seed(42)
        data = {
            'ticker': np.random.choice([f'STOCK_{i:03d}' for i in range(n_stocks)], n_samples),
            'statistical_anomaly_score': np.random.exponential(0.3, n_samples),
            'order_frequency_anomaly': np.random.binomial(1, 0.1, n_samples),
            'cancel_ratio_anomaly': np.random.binomial(1, 0.05, n_samples),
            'price_volatility_anomaly': np.random.binomial(1, 0.08, n_samples),
            'qty_anomaly': np.random.binomial(1, 0.06, n_samples),
            'hour': np.random.randint(9, 16, n_samples),
            'è‡ªç„¶æ—¥': np.random.choice(['20250301', '20250302'], n_samples),
            'composite_anomaly_score': np.random.beta(0.5, 2, n_samples),
            'is_anomalous_period': np.random.binomial(1, 0.1, n_samples),
            'known_spoofing': np.random.binomial(1, 0.05, n_samples),
            'model_spoofing_prob': np.random.beta(0.3, 2, n_samples)
        }
        
        df = pd.DataFrame(data)
        print(f"âœ… æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")
        return df

    def benchmark_parallel_anomaly_detection(self, df, max_workers=None):
        """å¹¶å‘å¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•"""
        print("\nğŸš€ å¹¶å‘å¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•")
        print("=" * 40)
        
        # 1. ä¼ ç»Ÿå•çº¿ç¨‹æ–¹æ³•
        print("ğŸ”„ æµ‹è¯•ä¼ ç»Ÿå•çº¿ç¨‹å¼‚å¸¸æ£€æµ‹...")
        start_time = time.time()
        detector_traditional = ManipulationDetector(enable_parallel=False)
        result_traditional = detector_traditional.detect_anomalous_periods(df.copy())
        traditional_time = time.time() - start_time
        
        # 2. å¹¶å‘ä¼˜åŒ–æ–¹æ³•
        print(f"ğŸš€ æµ‹è¯•å¹¶å‘ä¼˜åŒ–å¼‚å¸¸æ£€æµ‹...")
        start_time = time.time()
        detector_parallel = ManipulationDetector(enable_parallel=True, max_workers=max_workers)
        result_parallel = detector_parallel.detect_anomalous_periods(df.copy())
        parallel_time = time.time() - start_time
        
        # 3. è®¡ç®—æ€§èƒ½æå‡
        speedup = traditional_time / parallel_time if parallel_time > 0 else 1
        
        print(f"âœ… ä¼ ç»Ÿæ–¹æ³•: {traditional_time:.2f}s")
        print(f"âœ… å¹¶å‘æ–¹æ³•: {parallel_time:.2f}s")
        print(f"ğŸ“Š æ€§èƒ½æå‡: {speedup:.2f}x")
        
        return {
            'traditional_time': traditional_time,
            'parallel_time': parallel_time,
            'speedup': speedup
        }

    def benchmark_heatmap_generation(self, df):
        """çƒ­åŠ›å›¾ç”Ÿæˆæ€§èƒ½æµ‹è¯•"""
        print("\nğŸ“Š çƒ­åŠ›å›¾ç”Ÿæˆæ€§èƒ½æµ‹è¯•")
        print("=" * 40)
        
        visualizer = HeatmapVisualizer()
        results = {}
        
        heatmap_tests = [
            ("hourly_heatmap", visualizer.create_hourly_manipulation_heatmap),
            ("daily_heatmap", visualizer.create_daily_manipulation_heatmap),
            ("prediction_comparison", visualizer.create_prediction_comparison_heatmap),
            ("correlation_heatmap", visualizer.create_manipulation_correlation_heatmap)
        ]
        
        for name, func in heatmap_tests:
            print(f"ğŸ–¼ï¸ ç”Ÿæˆ {name}...")
            start_time = time.time()
            
            try:
                output_path = self.output_dir / f"benchmark_{name}.png"
                func(df, output_path)
                generation_time = time.time() - start_time
                results[name] = {
                    'time': generation_time,
                    'success': True,
                    'file_size': output_path.stat().st_size if output_path.exists() else 0
                }
                print(f"âœ… {name}: {generation_time:.2f}s")
            except Exception as e:
                generation_time = time.time() - start_time
                results[name] = {
                    'time': generation_time,
                    'success': False,
                    'error': str(e)
                }
                print(f"âŒ {name}: å¤±è´¥ - {e}")
        
        return results

    def benchmark_scaling_test(self, base_samples=5000, max_samples=30000, step=5000):
        """æ‰©å±•æ€§æµ‹è¯•"""
        print("\nğŸ“ˆ æ‰©å±•æ€§æµ‹è¯•")
        print("=" * 40)
        
        sample_sizes = list(range(base_samples, max_samples + 1, step))
        traditional_times = []
        parallel_times = []
        speedup_ratios = []
        
        for n_samples in sample_sizes:
            print(f"ğŸ§ª æµ‹è¯•æ ·æœ¬æ•°: {n_samples:,}")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            df = self.generate_synthetic_data(n_samples=n_samples, n_stocks=20)
            
            # ç®€åŒ–çš„æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            detector_trad = ManipulationDetector(enable_parallel=False)
            detector_trad.detect_anomalous_periods(df.copy())
            trad_time = time.time() - start_time
            traditional_times.append(trad_time)
            
            start_time = time.time()
            detector_par = ManipulationDetector(enable_parallel=True, max_workers=8)
            detector_par.detect_anomalous_periods(df.copy())
            par_time = time.time() - start_time
            parallel_times.append(par_time)
            
            speedup = trad_time / par_time if par_time > 0 else 1
            speedup_ratios.append(speedup)
            
            print(f"   ä¼ ç»Ÿ: {trad_time:.2f}s, å¹¶å‘: {par_time:.2f}s, åŠ é€Ÿ: {speedup:.2f}x")
        
        return sample_sizes, traditional_times, parallel_times, speedup_ratios

    def benchmark_model_prediction(self, df, batch_sizes=[1000, 5000, 10000, 20000]):
        """æ¨¡å‹é¢„æµ‹æ€§èƒ½æµ‹è¯•"""
        print("\nğŸ¤– æ¨¡å‹é¢„æµ‹æ€§èƒ½æµ‹è¯•")
        print("=" * 40)
        
        results = {}
        
        # åˆ›å»ºè™šæ‹Ÿæ¨¡å‹è¿›è¡Œæµ‹è¯•
        detector = ManipulationDetector()
        
        for batch_size in batch_sizes:
            print(f"ğŸ“¦ æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size:,}")
            start_time = time.time()
            
            # æ¨¡æ‹Ÿæ‰¹é‡é¢„æµ‹
            n_batches = len(df) // batch_size + (1 if len(df) % batch_size else 0)
            predictions = []
            
            for i in range(n_batches):
                start_idx = i * batch_size
                end_idx = min((i + 1) * batch_size, len(df))
                batch_df = df.iloc[start_idx:end_idx]
                
                # æ¨¡æ‹Ÿé¢„æµ‹å¤„ç†æ—¶é—´
                time.sleep(0.001 * len(batch_df) / 1000)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
                batch_pred = np.random.random(len(batch_df))
                predictions.extend(batch_pred)
            
            prediction_time = time.time() - start_time
            throughput = len(df) / prediction_time
            
            results[batch_size] = {
                'time': prediction_time,
                'throughput': throughput,
                'n_batches': n_batches
            }
            
            print(f"âœ… æ‰¹æ¬¡ {batch_size:,}: {prediction_time:.2f}s, ååé‡: {throughput:.0f} æ ·æœ¬/ç§’")
        
        return results

    def create_performance_plots(self, results):
        """åˆ›å»ºç»¼åˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        fig = plt.figure(figsize=(16, 12))
        
        # 1. å¹¶å‘å¼‚å¸¸æ£€æµ‹æ€§èƒ½å¯¹æ¯”
        if 'parallel_anomaly' in results:
            ax1 = plt.subplot(2, 3, 1)
            par_results = results['parallel_anomaly']
            methods = ['Traditional', 'Parallel']
            times = [par_results['traditional_time'], par_results['parallel_time']]
            
            bars = ax1.bar(methods, times, color=['red', 'green'], alpha=0.7)
            ax1.set_ylabel('Execution Time (s)')
            ax1.set_title('Anomaly Detection Performance')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, time_val in zip(bars, times):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        f'{time_val:.2f}s', ha='center', va='bottom')
        
        # 2. æ‰©å±•æ€§æµ‹è¯•ç»“æœ
        if 'scaling' in results:
            ax2 = plt.subplot(2, 3, 2)
            sample_sizes, trad_times, par_times, speedups = results['scaling']
            
            ax2.plot(sample_sizes, speedups, 'o-', color='blue', linewidth=2, markersize=6)
            ax2.axhline(y=1, color='red', linestyle='--', alpha=0.7)
            ax2.set_xlabel('Sample Size')
            ax2.set_ylabel('Speedup Ratio')
            ax2.set_title('Scaling Performance')
            ax2.grid(True, alpha=0.3)
        
        # 3. çƒ­åŠ›å›¾ç”Ÿæˆæ—¶é—´
        if 'heatmap' in results:
            ax3 = plt.subplot(2, 3, 3)
            heatmap_results = results['heatmap']
            heatmap_names = []
            heatmap_times = []
            
            for name, result in heatmap_results.items():
                if result['success']:
                    heatmap_names.append(name.replace('_', '\n'))
                    heatmap_times.append(result['time'])
            
            if heatmap_names:
                bars = ax3.bar(heatmap_names, heatmap_times, color='orange', alpha=0.7)
                ax3.set_ylabel('Generation Time (s)')
                ax3.set_title('Heatmap Generation')
                ax3.tick_params(axis='x', rotation=45)
        
        # 4. æ¨¡å‹é¢„æµ‹ååé‡
        if 'model_prediction' in results:
            ax4 = plt.subplot(2, 3, 4)
            pred_results = results['model_prediction']
            batch_sizes = list(pred_results.keys())
            throughputs = [pred_results[bs]['throughput'] for bs in batch_sizes]
            
            ax4.plot(batch_sizes, throughputs, 's-', color='purple', linewidth=2, markersize=6)
            ax4.set_xlabel('Batch Size')
            ax4.set_ylabel('Throughput (samples/s)')
            ax4.set_title('Model Prediction Throughput')
            ax4.grid(True, alpha=0.3)
        
        # 5. ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
        ax5 = plt.subplot(2, 3, 5)
        cpu_count = mp.cpu_count()
        memory_gb = 8  # å‡è®¾å€¼
        
        resource_labels = ['CPU Cores', 'Memory (GB)', 'Max Workers']
        resource_values = [cpu_count, memory_gb, min(cpu_count, 20)]
        
        bars = ax5.bar(resource_labels, resource_values, color=['cyan', 'magenta', 'yellow'], alpha=0.7)
        ax5.set_ylabel('Resource Count')
        ax5.set_title('System Resources')
        
        # 6. ç»¼åˆæ€§èƒ½æ‘˜è¦
        ax6 = plt.subplot(2, 3, 6)
        ax6.axis('off')
        
        summary_text = f"""
Benchmark Summary
================
CPU Cores: {cpu_count}
Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}

Performance Highlights:
"""
        
        if 'parallel_anomaly' in results:
            speedup = results['parallel_anomaly']['speedup']
            summary_text += f"â€¢ Anomaly Detection Speedup: {speedup:.2f}x\n"
        
        if 'scaling' in results and results['scaling'][3]:  # speedups
            max_speedup = max(results['scaling'][3])
            summary_text += f"â€¢ Max Scaling Speedup: {max_speedup:.2f}x\n"
        
        if 'heatmap' in results:
            success_count = sum(1 for r in results['heatmap'].values() if r['success'])
            total_count = len(results['heatmap'])
            summary_text += f"â€¢ Heatmap Success Rate: {success_count}/{total_count}\n"
        
        ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes, 
                fontsize=10, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        plot_path = self.output_dir / 'comprehensive_benchmark.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š ç»¼åˆæ€§èƒ½å›¾è¡¨å·²ä¿å­˜: {plot_path}")
        return plot_path

    def run_quick_benchmark(self, n_samples=10000, n_stocks=30):
        """å¿«é€ŸåŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¿«é€ŸåŸºå‡†æµ‹è¯•")
        print("=" * 50)
        
        df = self.generate_synthetic_data(n_samples, n_stocks)
        results = self.benchmark_parallel_anomaly_detection(df)
        self.save_benchmark_report(results)
        return results

    def run_scaling_benchmark(self):
        """æ‰©å±•æ€§åŸºå‡†æµ‹è¯•"""
        print("ğŸ“ˆ æ‰©å±•æ€§åŸºå‡†æµ‹è¯•")
        print("=" * 50)
        
        results = {}
        results['scaling'] = self.benchmark_scaling_test(5000, 25000, 5000)
        
        # ç”Ÿæˆæ‰©å±•æ€§å›¾è¡¨
        sample_sizes, trad_times, par_times, speedups = results['scaling']
        
        plt.figure(figsize=(12, 8))
        
        plt.subplot(2, 2, 1)
        plt.plot(sample_sizes, trad_times, 'o-', label='Traditional', linewidth=2)
        plt.plot(sample_sizes, par_times, 's-', label='Parallel', linewidth=2)
        plt.xlabel('Sample Size')
        plt.ylabel('Time (s)')
        plt.title('Execution Time vs Sample Size')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 2, 2)
        plt.plot(sample_sizes, speedups, '^-', color='green', linewidth=2)
        plt.axhline(y=1, color='red', linestyle='--', alpha=0.7)
        plt.xlabel('Sample Size')
        plt.ylabel('Speedup Ratio')
        plt.title('Parallel Speedup')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 2, 3)
        efficiency = np.array(speedups) / mp.cpu_count()
        plt.plot(sample_sizes, efficiency, 'D-', color='orange', linewidth=2)
        plt.xlabel('Sample Size')
        plt.ylabel('Efficiency')
        plt.title(f'Parallel Efficiency (/{mp.cpu_count()} cores)')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 2, 4)
        trad_throughput = np.array(sample_sizes) / np.array(trad_times)
        par_throughput = np.array(sample_sizes) / np.array(par_times)
        plt.plot(sample_sizes, trad_throughput, 'o-', label='Traditional', linewidth=2)
        plt.plot(sample_sizes, par_throughput, 's-', label='Parallel', linewidth=2)
        plt.xlabel('Sample Size')
        plt.ylabel('Throughput (samples/s)')
        plt.title('Processing Throughput')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = self.output_dir / 'scaling_benchmark.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ‰©å±•æ€§å›¾è¡¨å·²ä¿å­˜: {plot_path}")
        return results

    def save_benchmark_report(self, results):
        """ä¿å­˜åŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        report_path = self.output_dir / 'benchmark_report.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("Heatmap Analysis Benchmark Report\n")
            f.write("=" * 50 + "\n")
            f.write(f"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"CPU Cores: {mp.cpu_count()}\n\n")
            f.write(f"ä¼ ç»Ÿæ–¹æ³•: {results['traditional_time']:.2f}s\n")
            f.write(f"å¹¶å‘æ–¹æ³•: {results['parallel_time']:.2f}s\n")
            f.write(f"æ€§èƒ½æå‡: {results['speedup']:.2f}x\n")
        
        print(f"ğŸ“ åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    parser = argparse.ArgumentParser(description="Heatmap Analysis Benchmark Tools")
    parser.add_argument("--samples", type=int, default=20000, help="æµ‹è¯•æ ·æœ¬æ•°")
    parser.add_argument("--stocks", type=int, default=30, help="æµ‹è¯•è‚¡ç¥¨æ•°")
    parser.add_argument("--output_dir", default="benchmark_results", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--scaling_test", action="store_true", help="è¿è¡Œæ‰©å±•æ€§æµ‹è¯•")
    parser.add_argument("--quick_test", action="store_true", help="è¿è¡Œå¿«é€Ÿæµ‹è¯•")
    parser.add_argument("--max_workers", type=int, default=None, help="æœ€å¤§çº¿ç¨‹æ•°")
    
    args = parser.parse_args()
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å¥—ä»¶
    benchmark = BenchmarkSuite(args.output_dir)
    
    if args.scaling_test:
        print("ğŸš€ è¿è¡Œæ‰©å±•æ€§åŸºå‡†æµ‹è¯•...")
        benchmark.run_scaling_benchmark()
    elif args.quick_test or not any([args.scaling_test]):
        print("ğŸš€ è¿è¡Œå¿«é€ŸåŸºå‡†æµ‹è¯•...")
        benchmark.run_quick_benchmark(args.samples, args.stocks)
    
    print("\nğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {benchmark.output_dir}")

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹:

1. å¿«é€ŸåŸºå‡†æµ‹è¯•:
python benchmark_tools.py --quick_test --samples 20000 --stocks 30

2. æ‰©å±•æ€§æµ‹è¯•:
python benchmark_tools.py --scaling_test

3. è‡ªå®šä¹‰å‚æ•°:
python benchmark_tools.py --samples 50000 --stocks 50 --max_workers 16

4. æŒ‡å®šè¾“å‡ºç›®å½•:
python benchmark_tools.py --output_dir results/benchmarks
""" 