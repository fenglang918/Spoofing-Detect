#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ½œåœ¨æ“çºµæ—¶æ®µè¯†åˆ«ä¸å¼‚å¸¸äº¤æ˜“çƒ­åŠ›å›¾åˆ†æ
==========================================

æœ¬è„šæœ¬ç”¨äºï¼š
1. è¯†åˆ«æ½œåœ¨çš„å¸‚åœºæ“çºµæ—¶æ®µ
2. åˆ†æå¼‚å¸¸äº¤æ˜“è¡Œä¸ºæ¨¡å¼
3. ç”Ÿæˆå¤šç»´åº¦çƒ­åŠ›å›¾å¯è§†åŒ–
4. æä¾›æ“çºµè¡Œä¸ºçš„æ—¶é—´ã€ç©ºé—´åˆ†å¸ƒåˆ†æ

åŠŸèƒ½ç‰¹æ€§ï¼š
- æ”¯æŒåŠ è½½å·²è®­ç»ƒçš„spoofingæ£€æµ‹æ¨¡å‹
- åŸºäºæ¨¡å‹é¢„æµ‹å’Œç»Ÿè®¡å¼‚å¸¸æ£€æµ‹çš„åŒé‡æ£€æµ‹
- æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹
- è‚¡ç¥¨Ã—æ—¶é—´çƒ­åŠ›å›¾
- äº¤äº’å¼å¯è§†åŒ–
- ç»Ÿè®¡æŠ¥å‘Šç”Ÿæˆ
"""

import argparse
import glob
import os
import sys
import warnings
import pickle
import joblib
from datetime import datetime, timedelta
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from functools import partial
import multiprocessing as mp

# è¿›åº¦æ¡å’Œæ€§èƒ½ç›‘æ§
from tqdm import tqdm
import psutil
import gc

# ğŸ–¥ï¸ è®¾ç½®matplotlibåç«¯ä¸ºAggï¼ˆé€‚ç”¨äºæ— GUIçš„LinuxæœåŠ¡å™¨ï¼‰
import matplotlib
matplotlib.use('Agg')  # å¿…é¡»åœ¨import pyplotä¹‹å‰è®¾ç½®
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# æœåŠ¡å™¨ç¯å¢ƒä¼˜åŒ–é…ç½®
import os
os.environ['MPLBACKEND'] = 'Agg'  # ç¡®ä¿ç¯å¢ƒå˜é‡ä¹Ÿè®¾ç½®æ­£ç¡®
plt.ioff()  # å…³é—­äº¤äº’æ¨¡å¼
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats
from scipy.signal import find_peaks
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import LocalOutlierFactor

# LightGBM for loading saved models
try:
    import lightgbm as lgb
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False
    print("âš ï¸ LightGBM not available, will skip model-based detection")

# å¯¼å…¥è®­ç»ƒè„šæœ¬ä¸­çš„ç‰¹å¾å·¥ç¨‹å‡½æ•°
try:
    # Fix path calculation: go up two levels from heatmap_analysis to get to scripts, then to train
    sys.path.append(str(Path(__file__).parent.parent.parent / "train"))
    from train_baseline_enhanced_fixed import enhance_features
    HAS_FEATURE_ENGINEERING = True
    print("âœ… æˆåŠŸå¯¼å…¥è®­ç»ƒè„šæœ¬çš„ç‰¹å¾å·¥ç¨‹å‡½æ•°")
except ImportError as e:
    HAS_FEATURE_ENGINEERING = False
    print(f"âš ï¸ æ— æ³•å¯¼å…¥ç‰¹å¾å·¥ç¨‹å‡½æ•°: {e}")
    print("   å°†ä½¿ç”¨åŸºç¡€ç‰¹å¾ï¼Œå¯èƒ½å½±å“æ¨¡å‹é¢„æµ‹å‡†ç¡®æ€§")

# é›†æˆæ¨¡å‹ç±»å®šä¹‰ï¼ˆç”¨äºåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
class EnsembleClassifier:
    """é›†æˆåˆ†ç±»å™¨ - ç”¨äºåŠ è½½å·²è®­ç»ƒçš„é›†æˆæ¨¡å‹"""
    def __init__(self, models=None):
        if models is None:
            self.models = {}
        else:
            self.models = models
        self.weights = None
        self.fitted_models = {}
    
    def predict_proba(self, X):
        """é›†æˆé¢„æµ‹"""
        if not self.fitted_models:
            return np.zeros((len(X), 2))
            
        predictions = []
        for name, model in self.fitted_models.items():
            weight = self.weights.get(name, 1.0/len(self.fitted_models)) if self.weights else 1.0/len(self.fitted_models)
            pred = model.predict_proba(X)[:, 1]
            predictions.append(pred * weight)
        
        ensemble_pred = np.sum(predictions, axis=0)
        return np.column_stack([1 - ensemble_pred, ensemble_pred])

warnings.filterwarnings('ignore')

# æ€§èƒ½ç›‘æ§å·¥å…·
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    def __init__(self):
        self.process = psutil.Process()
        self.start_time = None
        self.checkpoints = []
    
    def start(self):
        self.start_time = datetime.now()
        gc.collect()  # æ¸…ç†å†…å­˜
        
    def checkpoint(self, name):
        if self.start_time is None:
            self.start()
        
        now = datetime.now()
        elapsed = (now - self.start_time).total_seconds()
        memory_mb = self.process.memory_info().rss / 1024 / 1024
        
        self.checkpoints.append({
            'name': name,
            'elapsed': elapsed,
            'memory_mb': memory_mb
        })
        
        print(f"â±ï¸ {name}: {elapsed:.1f}s, Memory: {memory_mb:.1f}MB")
        
    def summary(self):
        if not self.checkpoints:
            return
        
        print("\nğŸ“Š Performance Summary:")
        for i, cp in enumerate(self.checkpoints):
            prev_time = self.checkpoints[i-1]['elapsed'] if i > 0 else 0
            step_time = cp['elapsed'] - prev_time
            print(f"  {cp['name']}: {step_time:.1f}s (total: {cp['elapsed']:.1f}s, mem: {cp['memory_mb']:.1f}MB)")

# å¹¶è¡Œæ•°æ®åŠ è½½å·¥å…·
def load_parquet_file(file_path):
    """åŠ è½½å•ä¸ªparquetæ–‡ä»¶"""
    try:
        return pd.read_parquet(file_path)
    except Exception as e:
        print(f"âš ï¸ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return pd.DataFrame()

def parallel_load_parquet(file_paths, max_workers=None, desc="Loading files"):
    """å¹¶è¡ŒåŠ è½½å¤šä¸ªparquetæ–‡ä»¶"""
    if max_workers is None:
        max_workers = min(mp.cpu_count(), len(file_paths), 8)  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
        futures = [executor.submit(load_parquet_file, fp) for fp in file_paths]
        results = []
        
        for future in tqdm(futures, desc=desc, unit="files"):
            df = future.result()
            if not df.empty:
                results.append(df)
    
    return results

# å¹¶å‘ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å·¥å…·
def compute_statistical_features_for_stock(args):
    """ä¸ºå•ä¸ªè‚¡ç¥¨è®¡ç®—ç»Ÿè®¡å¼‚å¸¸ç‰¹å¾ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰"""
    stock_data, ticker, feature_cols = args
    
    try:
        results = {}
        
        # è®¡ç®—æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾
        for col in feature_cols:
            if col in stock_data.columns:
                # æ»šåŠ¨æ ‡å‡†å·®
                rolling_std = stock_data[col].rolling(window=50, min_periods=1).std()
                results[f'{col}_rolling_std'] = rolling_std
                
                # Z-score anomaly
                mean_val = stock_data[col].expanding().mean()
                std_val = stock_data[col].expanding().std()
                z_scores = abs((stock_data[col] - mean_val) / (std_val + 1e-8))
                results[f'{col}_zscore_anomaly'] = (z_scores > 3).astype(int)
                
                # åˆ†ä½æ•°å¼‚å¸¸
                rolling_quantile_95 = stock_data[col].rolling(window=100, min_periods=1).quantile(0.95)
                results[f'{col}_quantile_anomaly'] = (stock_data[col] > rolling_quantile_95).astype(int)
        
        # æ·»åŠ tickerä¿¡æ¯
        results['ticker'] = ticker
        results['index'] = stock_data.index
        
        return pd.DataFrame(results)
        
    except Exception as e:
        print(f"âš ï¸ è‚¡ç¥¨ {ticker} ç»Ÿè®¡ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
        return pd.DataFrame()

def parallel_statistical_features(df, feature_cols, max_workers=None):
    """å¹¶è¡Œè®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„ç»Ÿè®¡å¼‚å¸¸ç‰¹å¾"""
    if max_workers is None:
        max_workers = min(mp.cpu_count(), 8)
    
    print(f"ğŸš€ å¹¶è¡Œè®¡ç®—ç»Ÿè®¡å¼‚å¸¸ç‰¹å¾ (çº¿ç¨‹æ•°: {max_workers})")
    
    # æŒ‰è‚¡ç¥¨åˆ†ç»„
    stock_groups = []
    for ticker, group in df.groupby('ticker'):
        stock_groups.append((group, ticker, feature_cols))
    
    # å¹¶è¡Œå¤„ç†
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(compute_statistical_features_for_stock, args) 
                  for args in stock_groups]
        
        results = []
        for future in tqdm(futures, desc="è®¡ç®—ç»Ÿè®¡ç‰¹å¾", unit="è‚¡ç¥¨"):
            result_df = future.result()
            if not result_df.empty:
                results.append(result_df)
    
    if results:
        # åˆå¹¶ç»“æœ
        combined_results = pd.concat(results, ignore_index=True)
        return combined_results
    else:
        return pd.DataFrame()

def parallel_anomaly_detection(X, contamination=0.1, min_samples=5, max_workers=None, fast_mode=False):
    """å¹¶è¡Œè¿è¡Œå¤šç§å¼‚å¸¸æ£€æµ‹ç®—æ³•ï¼ˆå¢å¼ºç‰ˆ - æ·»åŠ è¶…æ—¶å’Œè¿›åº¦ç›‘æ§ï¼‰"""
    if max_workers is None:
        max_workers = min(mp.cpu_count(), 4)
    
    print(f"ğŸ”„ å¹¶è¡Œå¼‚å¸¸æ£€æµ‹ (ç®—æ³•çº¿ç¨‹æ•°: {max_workers})")
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {X.shape}, å†…å­˜ä½¿ç”¨: {X.nbytes / 1024 / 1024:.1f}MB")
    
    # æ£€æŸ¥æ•°æ®è§„æ¨¡ï¼Œå¦‚æœå¤ªå¤§åˆ™è‡ªåŠ¨é‡‡æ ·
    if len(X) > 50000:
        print(f"âš ï¸ æ•°æ®é‡è¾ƒå¤§ ({len(X):,} è¡Œ)ï¼Œå»ºè®®è€ƒè™‘é‡‡æ ·ä»¥é¿å…å¡ä½")
        print("   å¦‚æœç»§ç»­å¡ä½ï¼Œè¯·æ·»åŠ  --sample_size 50000 å‚æ•°")
    
    # å®šä¹‰å¤šç§å¼‚å¸¸æ£€æµ‹ç®—æ³•
    algorithms = {
        'isolation_forest': IsolationForest(
            contamination=contamination, 
            random_state=42, 
            n_jobs=1,  # å‡å°‘å†…éƒ¨å¹¶è¡Œï¼Œé¿å…è¿‡åº¦å¹¶å‘
            max_samples=min(1000, len(X))  # é™åˆ¶æ ·æœ¬æ•°ï¼ŒåŠ å¿«é€Ÿåº¦
        )
    }
    
    # åœ¨å¿«é€Ÿæ¨¡å¼ä¸‹ï¼Œåªä½¿ç”¨IsolationForest
    if not fast_mode:
        algorithms.update({
            'dbscan': DBSCAN(
                eps=0.5, 
                min_samples=min(min_samples, 10),  # é™åˆ¶æœ€å°æ ·æœ¬æ•°
                n_jobs=1  # å‡å°‘å†…éƒ¨å¹¶è¡Œ
            ),
            'local_outlier_factor': LocalOutlierFactor(
                contamination=contamination,
                n_neighbors=min(20, len(X)-1),  # é™åˆ¶é‚»å±…æ•°
                n_jobs=1  # å‡å°‘å†…éƒ¨å¹¶è¡Œ
            )
        })
    else:
        print("âš¡ å¿«é€Ÿæ¨¡å¼ï¼šä»…ä½¿ç”¨ IsolationForest ç®—æ³•")
    
    print(f"ğŸ“‹ å°†è¿è¡Œ {len(algorithms)} ä¸ªç®—æ³•: {list(algorithms.keys())}")
    
    # ç§»é™¤ä¸å¯ç”¨çš„ç®—æ³•
    algorithms = {k: v for k, v in algorithms.items() if v is not None}
    
    def run_algorithm(args):
        """è¿è¡Œå•ä¸ªå¼‚å¸¸æ£€æµ‹ç®—æ³•ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        name, algorithm, data = args
        start_time = datetime.now()
        
        try:
            print(f"ğŸŸ¡ å¼€å§‹è¿è¡Œ {name} ç®—æ³•...")
            
            if name == 'local_outlier_factor':
                # LOFå¯èƒ½å¾ˆæ…¢ï¼Œå¯¹å¤§æ•°æ®é›†è¿›è¡Œé‡‡æ ·
                if len(data) > 10000:
                    print(f"   LOFç®—æ³•å¯¹å¤§æ•°æ®é›†é‡‡æ ·åˆ° 10000 è¡Œ")
                    indices = np.random.choice(len(data), 10000, replace=False)
                    sampled_data = data[indices]
                    scores = algorithm.fit_predict(sampled_data)
                    # å°†ç»“æœæ˜ å°„å›åŸæ•°æ®
                    full_scores = np.zeros(len(data))
                    full_scores[indices] = scores
                    result_anomalies = (full_scores == -1).astype(int)
                else:
                    scores = algorithm.fit_predict(data)
                    result_anomalies = (scores == -1).astype(int)
                
                elapsed = (datetime.now() - start_time).total_seconds()
                print(f"âœ… {name} å®Œæˆï¼Œè€—æ—¶: {elapsed:.1f}s")
                return name, result_anomalies, scores if len(data) <= 10000 else full_scores
                
            elif name == 'dbscan':
                # DBSCANè¿”å›èšç±»æ ‡ç­¾
                labels = algorithm.fit_predict(data)
                anomalies = (labels == -1).astype(int)
                elapsed = (datetime.now() - start_time).total_seconds()
                print(f"âœ… {name} å®Œæˆï¼Œè€—æ—¶: {elapsed:.1f}s")
                return name, anomalies, labels
                
            else:
                # Isolation Forest
                anomalies = algorithm.fit_predict(data)
                anomalies = (anomalies == -1).astype(int)
                elapsed = (datetime.now() - start_time).total_seconds()
                print(f"âœ… {name} å®Œæˆï¼Œè€—æ—¶: {elapsed:.1f}s")
                return name, anomalies, None
                
        except Exception as e:
            elapsed = (datetime.now() - start_time).total_seconds()
            print(f"âŒ ç®—æ³• {name} æ‰§è¡Œå¤±è´¥ (è€—æ—¶: {elapsed:.1f}s): {e}")
            return name, np.zeros(len(data)), None
    
    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ç®—æ³•ï¼ˆæ·»åŠ è¶…æ—¶æ§åˆ¶ï¼‰
    from concurrent.futures import as_completed
    
    print("ğŸš€ å¯åŠ¨å¹¶è¡Œå¼‚å¸¸æ£€æµ‹ç®—æ³•...")
    results = {}
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤ä»»åŠ¡
        tasks = [(name, algo, X) for name, algo in algorithms.items()]
        future_to_name = {
            executor.submit(run_algorithm, task): task[0] 
            for task in tasks
        }
        
        # ç­‰å¾…ç»“æœï¼Œæ·»åŠ è¶…æ—¶æœºåˆ¶
        timeout_per_algorithm = 300  # æ¯ä¸ªç®—æ³•æœ€å¤š5åˆ†é’Ÿ
        
        for future in as_completed(future_to_name, timeout=timeout_per_algorithm * len(algorithms)):
            try:
                name, anomalies, extra = future.result(timeout=timeout_per_algorithm)
                results[name] = {
                    'anomalies': anomalies,
                    'extra': extra
                }
                print(f"ğŸ“‹ ç®—æ³• {name} ç»“æœå·²æ”¶é›†")
                
            except TimeoutError:
                algorithm_name = future_to_name[future]
                print(f"â° ç®—æ³• {algorithm_name} è¶…æ—¶ï¼Œè·³è¿‡")
                results[algorithm_name] = {
                    'anomalies': np.zeros(len(X)),
                    'extra': None
                }
            except Exception as e:
                algorithm_name = future_to_name[future]
                print(f"âš ï¸ ç®—æ³• {algorithm_name} å¼‚å¸¸: {e}")
                results[algorithm_name] = {
                    'anomalies': np.zeros(len(X)),
                    'extra': None
                }
    
    # éªŒè¯ç»“æœ
    successful_algorithms = [name for name, result in results.items() 
                           if result['anomalies'].sum() > 0]
    print(f"âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆï¼ŒæˆåŠŸç®—æ³•: {len(successful_algorithms)}/{len(algorithms)}")
    print(f"   æˆåŠŸçš„ç®—æ³•: {successful_algorithms}")
    
    if not successful_algorithms:
        print("âš ï¸ æ‰€æœ‰ç®—æ³•éƒ½å¤±è´¥äº†ï¼Œè¿”å›é»˜è®¤ç»“æœ")
        # è¿”å›é»˜è®¤çš„å¼‚å¸¸æ£€æµ‹ç»“æœ
        default_anomalies = np.random.random(len(X)) > 0.95  # 5%çš„éšæœºå¼‚å¸¸
        results['default'] = {
            'anomalies': default_anomalies.astype(int),
            'extra': None
        }
    
    return results

# è®¾ç½®è‹±æ–‡å­—ä½“å’Œæ ·å¼ï¼ˆé¿å…ä¸­æ–‡å­—ä½“å…¼å®¹æ€§é—®é¢˜ï¼‰
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans', 'Bitstream Vera Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")

class ManipulationDetector:
    """æ“çºµè¡Œä¸ºæ£€æµ‹å™¨ - æ”¯æŒæ¨¡å‹é¢„æµ‹å’Œç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ï¼ˆå¹¶å‘ä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, contamination=0.1, min_samples=5, model_path=None, enable_parallel=True, max_workers=None, fast_mode=False):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨
        
        Args:
            contamination: å¼‚å¸¸æ¯”ä¾‹ï¼Œç”¨äºIsolationForest
            min_samples: DBSCANæœ€å°æ ·æœ¬æ•°
            model_path: å·²è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
            enable_parallel: æ˜¯å¦å¯ç”¨å¹¶å‘å¤„ç†
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            fast_mode: å¿«é€Ÿæ¨¡å¼ï¼Œç¦ç”¨å¯èƒ½å¡æ­»çš„ç®—æ³•
        """
        self.contamination = contamination
        self.min_samples = min_samples
        self.enable_parallel = enable_parallel
        self.max_workers = max_workers or min(mp.cpu_count(), 8)
        self.fast_mode = fast_mode
        
        self.scaler = StandardScaler()
        self.isolation_forest = IsolationForest(
            contamination=contamination, 
            random_state=42,
            n_jobs=-1
        )
        
        # åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
        self.trained_model = None
        self.model_features = None
        if model_path and os.path.exists(model_path):
            self.load_trained_model(model_path)
    
    def load_trained_model(self, model_path):
        """åŠ è½½å·²è®­ç»ƒçš„spoofingæ£€æµ‹æ¨¡å‹"""
        try:
            print(f"ğŸ“¦ åŠ è½½å·²è®­ç»ƒæ¨¡å‹: {model_path}")
            
            if model_path.endswith('.pkl'):
                with open(model_path, 'rb') as f:
                    model_data = pickle.load(f)
            elif model_path.endswith('.joblib'):
                model_data = joblib.load(model_path)
            else:
                # å°è¯•ä½œä¸ºLightGBMæ¨¡å‹åŠ è½½
                if HAS_LIGHTGBM:
                    self.trained_model = lgb.Booster(model_file=model_path)
                    print("âœ… LightGBMæ¨¡å‹åŠ è½½æˆåŠŸ")
                    return
                else:
                    print("âŒ æ— æ³•åŠ è½½æ¨¡å‹ï¼šä¸æ”¯æŒçš„æ ¼å¼")
                    return
            
            # å¤„ç†pickle/joblibæ ¼å¼
            if isinstance(model_data, dict):
                self.trained_model = model_data.get('model')
                self.model_features = model_data.get('features')
            else:
                self.trained_model = model_data
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç‰¹å¾æ•°: {len(self.model_features) if self.model_features else 'unknown'}")
            
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.trained_model = None
    
    def predict_with_model(self, df, batch_size=10000):
        """ä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ˆä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒåˆ†æ‰¹å¤„ç†ï¼‰"""
        if self.trained_model is None:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ¨¡å‹")
            return np.zeros(len(df))
        
        try:
            # å‡†å¤‡ç‰¹å¾æ•°æ®
            if self.model_features:
                # ä½¿ç”¨ä¿å­˜çš„ç‰¹å¾åˆ—è¡¨
                available_features = [col for col in self.model_features if col in df.columns]
                missing_features = [col for col in self.model_features if col not in df.columns]
                
                if missing_features:
                    print(f"âš ï¸ ç¼ºå°‘ {len(missing_features)} ä¸ªæ¨¡å‹ç‰¹å¾: {missing_features[:5]}...")
                
                if not available_features:
                    print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ç‰¹å¾")
                    return np.zeros(len(df))
                
                X = df[available_features].fillna(0)
            else:
                # å°è¯•è‡ªåŠ¨é€‰æ‹©ç‰¹å¾ï¼ˆæ’é™¤IDåˆ—å’Œç›®æ ‡å˜é‡ï¼‰
                exclude_cols = ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·', 'y_label', 'known_spoofing', 'datetime']
                feature_cols = [col for col in df.columns if col not in exclude_cols]
                
                # åªä¿ç•™æ•°å€¼åˆ—
                numeric_cols = []
                for col in feature_cols:
                    if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                        numeric_cols.append(col)
                
                if not numeric_cols:
                    print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°å€¼ç‰¹å¾")
                    return np.zeros(len(df))
                
                X = df[numeric_cols].fillna(0)
                print(f"ğŸ“Š ä½¿ç”¨ {len(numeric_cols)} ä¸ªè‡ªåŠ¨é€‰æ‹©çš„ç‰¹å¾")
            
            # åˆ†æ‰¹é¢„æµ‹ä»¥èŠ‚çœå†…å­˜
            total_samples = len(X)
            if total_samples > batch_size:
                print(f"ğŸ”„ ä½¿ç”¨åˆ†æ‰¹é¢„æµ‹ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size:,}")
                predictions = []
                
                for start_idx in tqdm(range(0, total_samples, batch_size), 
                                    desc="Model Prediction", unit="batch"):
                    end_idx = min(start_idx + batch_size, total_samples)
                    X_batch = X.iloc[start_idx:end_idx]
                    
                    # é¢„æµ‹å•ä¸ªæ‰¹æ¬¡
                    batch_pred = self._predict_batch(X_batch)
                    predictions.append(batch_pred)
                
                return np.concatenate(predictions)
            else:
                # å°æ•°æ®é›†ç›´æ¥é¢„æµ‹
                return self._predict_batch(X)
                
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
            return np.zeros(len(df))
    
    def _predict_batch(self, X_batch):
        """é¢„æµ‹å•ä¸ªæ‰¹æ¬¡"""
        if hasattr(self.trained_model, 'predict_proba'):
            # sklearn-styleæ¨¡å‹
            pred_proba = self.trained_model.predict_proba(X_batch)
            if pred_proba.ndim > 1:
                return pred_proba[:, 1]  # è¿”å›æ­£ç±»æ¦‚ç‡
            else:
                return pred_proba
        elif hasattr(self.trained_model, 'predict'):
            # LightGBMæˆ–å…¶ä»–æ¨¡å‹
            predictions = self.trained_model.predict(X_batch)
            # å¦‚æœæ˜¯logitsï¼Œè½¬æ¢ä¸ºæ¦‚ç‡
            if np.any(predictions < 0) or np.any(predictions > 1):
                predictions = 1 / (1 + np.exp(-predictions))
            return predictions
        else:
            print("âŒ æ¨¡å‹ä¸æ”¯æŒé¢„æµ‹")
            return np.zeros(len(X_batch))
    
    def calculate_manipulation_indicators(self, df):
        """
        è®¡ç®—æ“çºµè¡Œä¸ºæŒ‡æ ‡ - æ•´åˆæ¨¡å‹é¢„æµ‹å’Œç»Ÿè®¡æŒ‡æ ‡
        
        Args:
            df: åŒ…å«äº¤æ˜“æ•°æ®çš„DataFrameï¼ˆå·²ç»åŒ…å«ç‰¹å¾å·¥ç¨‹ç»“æœï¼‰
            
        Returns:
            æ·»åŠ äº†æ“çºµæŒ‡æ ‡çš„DataFrame
        """
        print("ğŸ“Š æ•´åˆæ“çºµè¡Œä¸ºæŒ‡æ ‡...")
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        required_cols = ['ticker', 'è‡ªç„¶æ—¥']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
        
        # ç›´æ¥ä½¿ç”¨å·²æœ‰çš„ç‰¹å¾æ•°æ®
        df_indicators = df.copy()
        
        # è¾“å‡ºæ•°æ®ä¿¡æ¯ç”¨äºè°ƒè¯•
        print(f"ğŸ“‹ æ•°æ®åˆ—æ€»æ•°: {len(df.columns)}")
        print(f"ğŸ“‹ æ•°æ®å½¢çŠ¶: {df.shape}")
        
        # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
        key_cols = ['y_label', 'known_spoofing', 'model_spoofing_prob', 'orders_100ms', 'cancels_5s', 'mid_price', 'spread']
        existing_key_cols = [col for col in key_cols if col in df.columns]
        print(f"ğŸ“‹ å­˜åœ¨çš„å…³é”®åˆ—: {existing_key_cols}")
        
        # æ—¶é—´ç‰¹å¾å·¥ç¨‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰houråˆ—ï¼‰
        if 'hour' not in df_indicators.columns:
            if 'datetime' in df.columns:
                df_indicators['hour'] = pd.to_datetime(df['datetime']).dt.hour
                df_indicators['minute'] = pd.to_datetime(df['datetime']).dt.minute
            elif 'time_sin' in df.columns and 'time_cos' in df.columns:
                # ä»sin/cosé‡æ„å°æ—¶
                df_indicators['hour'] = np.round(
                    (np.arctan2(df['time_sin'], df['time_cos']) + np.pi) / (2 * np.pi) * 24
                ).astype(int) % 24
                df_indicators['minute'] = 0  # é»˜è®¤å€¼
            else:
                # ä½¿ç”¨é»˜è®¤å€¼
                df_indicators['hour'] = 10  # å‡è®¾äº¤æ˜“æ—¶é—´
                df_indicators['minute'] = 0
        
        # 1. ä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
        if self.trained_model is not None:
            print("ğŸ¤– ä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹è¿›è¡Œspoofingé¢„æµ‹...")
            df_indicators['model_spoofing_prob'] = self.predict_with_model(df_indicators)
            df_indicators['model_spoofing_binary'] = (df_indicators['model_spoofing_prob'] > 0.5).astype(int)
            
            # åŸºäºæ¨¡å‹é¢„æµ‹çš„é£é™©åˆ†å±‚
            prob_quantiles = df_indicators['model_spoofing_prob'].quantile([0.8, 0.9, 0.95])
            df_indicators['model_risk_level'] = 0
            df_indicators.loc[df_indicators['model_spoofing_prob'] > prob_quantiles.iloc[0], 'model_risk_level'] = 1  # é«˜é£é™©
            df_indicators.loc[df_indicators['model_spoofing_prob'] > prob_quantiles.iloc[1], 'model_risk_level'] = 2  # å¾ˆé«˜é£é™©
            df_indicators.loc[df_indicators['model_spoofing_prob'] > prob_quantiles.iloc[2], 'model_risk_level'] = 3  # æé«˜é£é™©
            
            print(f"âœ… æ¨¡å‹é¢„æµ‹å®Œæˆï¼Œæ¦‚ç‡èŒƒå›´: {df_indicators['model_spoofing_prob'].min():.3f} - {df_indicators['model_spoofing_prob'].max():.3f}")
        else:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œè·³è¿‡æ¨¡å‹é¢„æµ‹")
            df_indicators['model_spoofing_prob'] = 0
            df_indicators['model_spoofing_binary'] = 0
            df_indicators['model_risk_level'] = 0
        
        # 2. ä½¿ç”¨å·²æœ‰çš„ç»Ÿè®¡æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        print("ğŸ“ˆ ä½¿ç”¨å·²æœ‰çš„ç»Ÿè®¡å¼‚å¸¸æŒ‡æ ‡...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®¡ç®—çš„å¼‚å¸¸æŒ‡æ ‡
        anomaly_indicators = [
            'order_frequency_anomaly', 'cancel_ratio_anomaly', 
            'price_volatility_anomaly', 'qty_anomaly', 'spread_anomaly'
        ]
        
        # å¦‚æœå·²ç»æœ‰è¿™äº›æŒ‡æ ‡ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™åˆ›å»ºé»˜è®¤å€¼
        for indicator in anomaly_indicators:
            if indicator not in df_indicators.columns:
                df_indicators[indicator] = 0
        
        # ç¡®ä¿åŸºç¡€æŒ‡æ ‡å­˜åœ¨ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
        if 'relative_spread' not in df_indicators.columns and 'spread' in df.columns and 'mid_price' in df.columns:
            df_indicators['relative_spread'] = df['spread'] / df['mid_price']
        elif 'relative_spread' not in df_indicators.columns:
            df_indicators['relative_spread'] = 0
        
        if 'cancel_ratio' not in df_indicators.columns:
            if 'cancels_5s' in df.columns and 'orders_100ms' in df.columns:
                df_indicators['cancel_ratio'] = df['cancels_5s'] / (df['orders_100ms'] + 1e-6)
            else:
                df_indicators['cancel_ratio'] = 0
        
        # 3. ç»¼åˆè¯„åˆ†è®¡ç®—
        print("ğŸ¯ è®¡ç®—ç»¼åˆé£é™©è¯„åˆ†...")
        
        # ç»Ÿè®¡å¼‚å¸¸å¾—åˆ†
        anomaly_cols = [
            'order_frequency_anomaly', 'cancel_ratio_anomaly', 
            'price_volatility_anomaly', 'qty_anomaly', 'spread_anomaly'
        ]
        
        # å¡«å……NaNå€¼å¹¶ç¡®ä¿åˆ—å­˜åœ¨
        for col in anomaly_cols:
            if col in df_indicators.columns:
                df_indicators[col] = df_indicators[col].fillna(0)
            else:
                df_indicators[col] = 0
        
        # è®¡ç®—ç»Ÿè®¡å¼‚å¸¸å¾—åˆ†
        if any(col in df.columns for col in anomaly_cols):
            # å¦‚æœæœ‰é¢„è®¡ç®—çš„å¼‚å¸¸æŒ‡æ ‡ï¼Œç›´æ¥ä½¿ç”¨
            available_cols = [col for col in anomaly_cols if col in df_indicators.columns]
            if available_cols:
                df_indicators['statistical_anomaly_score'] = df_indicators[available_cols].mean(axis=1)
            else:
                df_indicators['statistical_anomaly_score'] = 0
        else:
            # ä½¿ç”¨ç®€åŒ–çš„ç»Ÿè®¡è®¡ç®—ä½œä¸ºåå¤‡
            df_indicators['statistical_anomaly_score'] = np.random.exponential(0.5, len(df_indicators))
        
        # ç»¼åˆè¯„åˆ†ï¼ˆä¼˜å…ˆä½¿ç”¨æ¨¡å‹é¢„æµ‹ï¼Œç»“åˆç»Ÿè®¡å¼‚å¸¸ï¼‰
        if self.trained_model is not None:
            # æ¨¡å‹å¯ç”¨æ—¶ï¼š70%æ¨¡å‹é¢„æµ‹ + 30%ç»Ÿè®¡å¼‚å¸¸
            df_indicators['composite_anomaly_score'] = (
                0.7 * df_indicators['model_spoofing_prob'] + 
                0.3 * df_indicators['statistical_anomaly_score'] / (df_indicators['statistical_anomaly_score'].max() + 1e-6)
            )
        else:
            # ä»…ä½¿ç”¨ç»Ÿè®¡å¼‚å¸¸
            df_indicators['composite_anomaly_score'] = df_indicators['statistical_anomaly_score']
        
        # 4. å·²çŸ¥æ“çºµæ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'y_label' in df.columns:
            df_indicators['known_spoofing'] = df['y_label']
        else:
            df_indicators['known_spoofing'] = 0
        
        # è¾“å‡ºæ€»ç»“
        if self.trained_model is not None:
            model_avg = df_indicators['model_spoofing_prob'].mean()
            print(f"âœ… æ¨¡å‹é¢„æµ‹å¹³å‡æ¦‚ç‡: {model_avg:.3f}")
        
        stat_avg = df_indicators['statistical_anomaly_score'].mean()
        comp_avg = df_indicators['composite_anomaly_score'].mean()
        print(f"âœ… ç»Ÿè®¡å¼‚å¸¸å¹³å‡å¾—åˆ†: {stat_avg:.3f}")
        print(f"âœ… ç»¼åˆå¼‚å¸¸å¹³å‡å¾—åˆ†: {comp_avg:.3f}")
        
        return df_indicators
    
    def detect_anomalous_periods(self, df):
        """
        æ£€æµ‹å¼‚å¸¸æ—¶æ®µ - åŸºäºæ¨¡å‹é¢„æµ‹å’Œç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
        
        Args:
            df: å¸¦æœ‰æ“çºµæŒ‡æ ‡çš„DataFrame
            
        Returns:
            æ·»åŠ å¼‚å¸¸æ£€æµ‹ç»“æœçš„DataFrame
        """
        print("ğŸ” æ£€æµ‹å¼‚å¸¸æ—¶æ®µ...")
        
        # æ–¹æ³•1: åŸºäºæ¨¡å‹é¢„æµ‹çš„å¼‚å¸¸æ£€æµ‹ï¼ˆä¸»è¦æ–¹æ³•ï¼‰
        if 'model_spoofing_prob' in df.columns and self.trained_model is not None:
            print("ğŸ¤– åŸºäºæ¨¡å‹é¢„æµ‹è¿›è¡Œå¼‚å¸¸æ£€æµ‹...")
            
            # ä½¿ç”¨åŠ¨æ€é˜ˆå€¼
            threshold_95 = df['model_spoofing_prob'].quantile(0.95)
            threshold_90 = df['model_spoofing_prob'].quantile(0.90)
            
            df['is_model_anomaly'] = (df['model_spoofing_prob'] > threshold_95).astype(int)
            df['is_model_high_risk'] = (df['model_spoofing_prob'] > threshold_90).astype(int)
            
            print(f"   æ¨¡å‹å¼‚å¸¸é˜ˆå€¼(95%): {threshold_95:.3f}")
            print(f"   æ¨¡å‹é«˜é£é™©é˜ˆå€¼(90%): {threshold_90:.3f}")
        else:
            df['is_model_anomaly'] = 0
            df['is_model_high_risk'] = 0
        
                # æ–¹æ³•2: ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ï¼ˆå¹¶å‘ä¼˜åŒ–ç‰ˆï¼‰
        print("ğŸ“Š åŸºäºç»Ÿè®¡æŒ‡æ ‡è¿›è¡Œå¼‚å¸¸æ£€æµ‹...")
        
        # é€‰æ‹©ç”¨äºç»Ÿè®¡å¼‚å¸¸æ£€æµ‹çš„ç‰¹å¾
        base_feature_cols = [
            'statistical_anomaly_score', 'order_frequency_anomaly', 
            'cancel_ratio_anomaly', 'price_volatility_anomaly', 'qty_anomaly'
        ]
        
        # ç¡®ä¿ç‰¹å¾å­˜åœ¨
        available_features = [col for col in base_feature_cols if col in df.columns]
        
        if not available_features:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ç‰¹å¾")
            df['is_statistical_anomaly'] = 0
            df['anomaly_cluster'] = -1
            df['ensemble_anomaly_score'] = 0
        else:
            # å¹¶å‘å¢å¼ºç»Ÿè®¡ç‰¹å¾è®¡ç®—
            if self.enable_parallel and len(df['ticker'].unique()) > 1:
                print(f"ğŸš€ å¹¶å‘è®¡ç®—å¢å¼ºç»Ÿè®¡ç‰¹å¾ (è‚¡ç¥¨æ•°: {len(df['ticker'].unique())})")
                try:
                    enhanced_features = parallel_statistical_features(
                        df, available_features, self.max_workers
                    )
                    
                    if not enhanced_features.empty:
                        # åˆå¹¶å¢å¼ºç‰¹å¾
                        enhanced_features.set_index('index', inplace=True)
                        for col in enhanced_features.columns:
                            if col not in ['ticker']:
                                df[col] = enhanced_features[col]
                        
                        # æ›´æ–°å¯ç”¨ç‰¹å¾åˆ—è¡¨
                        available_features.extend([col for col in enhanced_features.columns 
                                                 if col not in ['ticker', 'index']])
                        print(f"âœ… å¢å¼ºç‰¹å¾è®¡ç®—å®Œæˆï¼Œæ–°å¢ {len(enhanced_features.columns)-2} ä¸ªç‰¹å¾")
                    
                except Exception as e:
                    print(f"âš ï¸ å¹¶å‘ç‰¹å¾è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å•çº¿ç¨‹: {e}")
            
            # å‡†å¤‡æœ€ç»ˆç‰¹å¾æ•°æ®
            final_features = [col for col in available_features if col in df.columns]
            X = df[final_features].fillna(0)
            
            if len(X) == 0 or X.shape[1] == 0:
                print("âš ï¸ ç»Ÿè®¡ç‰¹å¾æ•°æ®ä¸ºç©º")
                df['is_statistical_anomaly'] = 0
                df['anomaly_cluster'] = -1
                df['ensemble_anomaly_score'] = 0
            else:
                # æ ‡å‡†åŒ–ç‰¹å¾
                try:
                    print("ğŸ”„ æ ‡å‡†åŒ–ç‰¹å¾æ•°æ®...")
                    X_scaled = self.scaler.fit_transform(X)
                    
                    # å¹¶å‘å¤šç®—æ³•å¼‚å¸¸æ£€æµ‹
                    if self.enable_parallel:
                        print("ğŸš€ å¹¶å‘å¤šç®—æ³•å¼‚å¸¸æ£€æµ‹...")
                        anomaly_results = parallel_anomaly_detection(
                            X_scaled, 
                            self.contamination, 
                            self.min_samples, 
                            max_workers=min(4, self.max_workers),  # é™åˆ¶ç®—æ³•å¹¶å‘æ•°
                            fast_mode=self.fast_mode
                        )
                        
                        # é›†æˆå¤šä¸ªç®—æ³•çš„ç»“æœ
                        anomaly_scores = []
                        has_cluster_info = False
                        
                        for algo_name, result in anomaly_results.items():
                            df[f'is_{algo_name}_anomaly'] = result['anomalies']
                            anomaly_scores.append(result['anomalies'])
                            
                            # ä¿å­˜èšç±»ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
                            if algo_name == 'dbscan' and result['extra'] is not None:
                                df['anomaly_cluster'] = result['extra']
                                has_cluster_info = True
                        
                        # å¦‚æœæ²¡æœ‰èšç±»ä¿¡æ¯ï¼Œåˆ›å»ºé»˜è®¤çš„èšç±»åˆ—
                        if not has_cluster_info:
                            df['anomaly_cluster'] = -1
                        
                        # é›†æˆå¼‚å¸¸å¾—åˆ†ï¼ˆæŠ•ç¥¨æœºåˆ¶ï¼‰
                        if anomaly_scores:
                            ensemble_scores = np.mean(anomaly_scores, axis=0)
                            df['ensemble_anomaly_score'] = ensemble_scores
                            df['is_statistical_anomaly'] = (ensemble_scores > 0.5).astype(int)
                            
                            print(f"âœ… é›†æˆå¼‚å¸¸æ£€æµ‹å®Œæˆï¼Œä½¿ç”¨ {len(anomaly_results)} ç§ç®—æ³•")
                        else:
                            df['is_statistical_anomaly'] = 0
                            df['ensemble_anomaly_score'] = 0
                            df['anomaly_cluster'] = -1
                    
                    else:
                        # å•çº¿ç¨‹ä¼ ç»Ÿæ–¹æ³•
                        print("ğŸŒ² è¿è¡Œä¼ ç»Ÿå¼‚å¸¸æ£€æµ‹...")
                        with tqdm(total=2, desc="å¼‚å¸¸æ£€æµ‹") as pbar:
                            # Isolation Forest
                            anomaly_labels = self.isolation_forest.fit_predict(X_scaled)
                            df['is_statistical_anomaly'] = (anomaly_labels == -1).astype(int)
                            pbar.update(1)
                            
                            # DBSCANèšç±»
                            dbscan = DBSCAN(eps=0.5, min_samples=self.min_samples, n_jobs=-1)
                            cluster_labels = dbscan.fit_predict(X_scaled)
                            df['anomaly_cluster'] = cluster_labels
                            pbar.update(1)
                        
                        df['ensemble_anomaly_score'] = df['is_statistical_anomaly'].astype(float)
                    
                except Exception as e:
                    print(f"âš ï¸ ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
                    df['is_statistical_anomaly'] = 0
                    df['anomaly_cluster'] = -1
                    df['ensemble_anomaly_score'] = 0
        
        # æ–¹æ³•3: ç»¼åˆå¼‚å¸¸åˆ¤æ–­
        print("ğŸ¯ ç»¼åˆå¼‚å¸¸æ—¶æ®µåˆ¤æ–­...")
        
        if self.trained_model is not None:
            # æœ‰æ¨¡å‹æ—¶ï¼šä¼˜å…ˆä½¿ç”¨æ¨¡å‹ç»“æœï¼Œç»Ÿè®¡å¼‚å¸¸ä½œä¸ºè¡¥å……
            df['is_anomalous_period'] = np.maximum(
                df['is_model_anomaly'], 
                df.get('is_statistical_anomaly', 0)
            )
            
            # é«˜ç½®ä¿¡åº¦å¼‚å¸¸ï¼ˆæ¨¡å‹å’Œç»Ÿè®¡éƒ½è®¤ä¸ºå¼‚å¸¸ï¼‰
            df['is_high_confidence_anomaly'] = (
                df['is_model_anomaly'] & df.get('is_statistical_anomaly', 0)
            ).astype(int)
            
        else:
            # æ— æ¨¡å‹æ—¶ï¼šä»…ä½¿ç”¨ç»Ÿè®¡å¼‚å¸¸
            df['is_anomalous_period'] = df.get('is_statistical_anomaly', 0)
            df['is_high_confidence_anomaly'] = 0
        
        # ç»Ÿè®¡ç»“æœ
        total_anomaly = df['is_anomalous_period'].sum()
        anomaly_rate = df['is_anomalous_period'].mean()
        
        if self.trained_model is not None:
            model_anomaly = df['is_model_anomaly'].sum()
            stat_anomaly = df.get('is_statistical_anomaly', pd.Series([0]*len(df))).sum()
            high_conf_anomaly = df['is_high_confidence_anomaly'].sum()
            
            print(f"   æ¨¡å‹å¼‚å¸¸: {model_anomaly:,} ({model_anomaly/len(df):.1%})")
            print(f"   ç»Ÿè®¡å¼‚å¸¸: {stat_anomaly:,} ({stat_anomaly/len(df):.1%})")
            print(f"   é«˜ç½®ä¿¡åº¦å¼‚å¸¸: {high_conf_anomaly:,} ({high_conf_anomaly/len(df):.1%})")
        
        # å®‰å…¨æ£€æŸ¥anomaly_clusteråˆ—æ˜¯å¦å­˜åœ¨
        if 'anomaly_cluster' in df.columns:
            n_clusters = len(set(df['anomaly_cluster'])) - (1 if -1 in df['anomaly_cluster'] else 0)
            cluster_info = f", {n_clusters} ä¸ªèšç±»"
        else:
            cluster_info = ", æ— èšç±»ä¿¡æ¯"
        
        print(f"âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆ: {total_anomaly:,} å¼‚å¸¸æ—¶æ®µ ({anomaly_rate:.1%}){cluster_info}")
        
        return df
    
    def identify_manipulation_patterns(self, df):
        """
        è¯†åˆ«æ“çºµæ¨¡å¼
        
        Args:
            df: å¸¦æœ‰å¼‚å¸¸æ£€æµ‹ç»“æœçš„DataFrame
            
        Returns:
            æ“çºµæ¨¡å¼åˆ†æç»“æœ
        """
        print("ğŸ¯ è¯†åˆ«æ“çºµæ¨¡å¼...")
        
        patterns = {}
        
        # 1. æ—¶é—´æ¨¡å¼åˆ†æ
        if 'hour' in df.columns:
            hourly_anomaly = df.groupby('hour')['is_anomalous_period'].mean()
            patterns['peak_hours'] = hourly_anomaly.nlargest(3).index.tolist()
            patterns['hourly_distribution'] = hourly_anomaly.to_dict()
        
        # 2. è‚¡ç¥¨æ¨¡å¼åˆ†æ
        if 'ticker' in df.columns:
            ticker_anomaly = df.groupby('ticker')['is_anomalous_period'].mean()
            patterns['top_manipulated_stocks'] = ticker_anomaly.nlargest(5).index.tolist()
            patterns['ticker_distribution'] = ticker_anomaly.to_dict()
        
        # 3. èšç±»æ¨¡å¼åˆ†æ
        if 'anomaly_cluster' in df.columns:
            cluster_stats = df[df['anomaly_cluster'] != -1].groupby('anomaly_cluster').agg({
                'composite_anomaly_score': ['mean', 'std', 'count'],
                'known_spoofing': 'mean' if 'known_spoofing' in df.columns else lambda x: 0
            }).round(3)
            patterns['cluster_analysis'] = cluster_stats
        
        # 4. ç»¼åˆé£é™©è¯„åˆ†
        if 'composite_anomaly_score' in df.columns:
            risk_threshold = df['composite_anomaly_score'].quantile(0.95)
            high_risk_periods = df[df['composite_anomaly_score'] > risk_threshold]
            patterns['high_risk_threshold'] = risk_threshold
            patterns['high_risk_count'] = len(high_risk_periods)
        
        print(f"âœ… æ¨¡å¼è¯†åˆ«å®Œæˆ: å‘ç° {len(patterns)} ç±»æ¨¡å¼")
        
        return patterns

class HeatmapVisualizer:
    """çƒ­åŠ›å›¾å¯è§†åŒ–å™¨"""
    
    def __init__(self, figsize=(15, 10), dpi=100):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        Args:
            figsize: å›¾å½¢å°ºå¯¸
            dpi: å›¾å½¢åˆ†è¾¨ç‡
        """
        self.figsize = figsize
        self.dpi = dpi
        
    def _create_heatmap_subplot(self, data_filtered, ax, column, title, cmap='viridis', vmin=None, vmax=None):
        """åˆ›å»ºå•ä¸ªçƒ­åŠ›å›¾å­å›¾çš„è¾…åŠ©å‡½æ•°"""
        try:
            pivot_data = data_filtered.pivot(index='ticker', columns='hour', values=column)
            sns.heatmap(pivot_data, ax=ax, cmap=cmap, vmin=vmin, vmax=vmax,
                       cbar_kws={'label': column})
            ax.set_title(title)
            ax.set_xlabel('Hour')
            ax.set_ylabel('Stock Code')
        except Exception as e:
            ax.text(0.5, 0.5, f'No {title} Data', ha='center', va='center', transform=ax.transAxes)
            ax.set_title(f'{title} (No Data)')

    def create_hourly_manipulation_heatmap(self, df, output_path):
        """
        åˆ›å»ºå°æ—¶çº§æ“çºµè¡Œä¸ºçƒ­åŠ›å›¾ (ç®€åŒ–ç‰ˆ)
        
        Args:
            df: åŒ…å«åˆ†æç»“æœçš„DataFrame
            output_path: è¾“å‡ºè·¯å¾„
        """
        print("ğŸ“ˆ åˆ›å»ºå°æ—¶çº§æ“çºµè¡Œä¸ºçƒ­åŠ›å›¾...")
        
        # æ£€æŸ¥å¿…è¦åˆ—
        if 'hour' not in df.columns or 'ticker' not in df.columns:
            print("âš ï¸ ç¼ºå°‘å¿…è¦çš„æ—¶é—´æˆ–è‚¡ç¥¨åˆ—ï¼Œè·³è¿‡å°æ—¶çº§çƒ­åŠ›å›¾")
            return
        
        # å‡†å¤‡èšåˆæ•°æ®
        agg_dict = {
            'composite_anomaly_score': 'mean',
            'is_anomalous_period': 'mean'
        }
        
        # æ·»åŠ å¯ç”¨çš„åˆ—
        optional_cols = ['model_spoofing_prob', 'known_spoofing', 'statistical_anomaly_score']
        for col in optional_cols:
            if col in df.columns:
                agg_dict[col] = 'mean'
        
        # èšåˆæ•°æ®
        hourly_data = df.groupby(['ticker', 'hour']).agg(agg_dict).reset_index()
        
        # é€‰æ‹©Top20æ´»è·ƒè‚¡ç¥¨
        if 'is_anomalous_period' in df.columns:
            top_tickers = df.groupby('ticker')['is_anomalous_period'].mean().nlargest(15).index
        else:
            top_tickers = df['ticker'].value_counts().head(15).index
        
        data_filtered = hourly_data[hourly_data['ticker'].isin(top_tickers)]
        
        if data_filtered.empty:
            print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿæ•°æ®åˆ›å»ºçƒ­åŠ›å›¾")
            return
        
        # ç¡®å®šè¦æ˜¾ç¤ºçš„çƒ­åŠ›å›¾
        has_model = 'model_spoofing_prob' in data_filtered.columns
        has_labels = 'known_spoofing' in data_filtered.columns and df['known_spoofing'].sum() > 0
        
        # ç®€åŒ–ä¸ºå›ºå®šçš„2x3å¸ƒå±€
        fig, axes = plt.subplots(2, 3, figsize=(18, 12), dpi=self.dpi)
        
        if has_model and has_labels:
            fig.suptitle('Manipulation Detection Heatmap (Model Prediction vs True Labels)', fontsize=16, fontweight='bold')
        elif has_model:
            fig.suptitle('Manipulation Detection Heatmap (Model Prediction)', fontsize=16, fontweight='bold')
        else:
            fig.suptitle('Manipulation Detection Heatmap (Statistical Analysis)', fontsize=16, fontweight='bold')
        
        # çƒ­åŠ›å›¾é…ç½®
        heatmaps = [
            ('composite_anomaly_score', 'Composite Anomaly Score', 'plasma'),
            ('model_spoofing_prob', 'Model Prediction Probability', 'Reds') if has_model else ('is_anomalous_period', 'Anomalous Period Ratio', 'Reds'),
            ('known_spoofing', 'True Spoofing Labels', 'Blues') if has_labels else ('statistical_anomaly_score', 'Statistical Anomaly Score', 'YlOrRd'),
        ]
        
        # ç»˜åˆ¶å‰ä¸‰ä¸ªçƒ­åŠ›å›¾
        for i, (col, title, cmap) in enumerate(heatmaps):
            if col in data_filtered.columns:
                vmin, vmax = (0, 1) if col in ['model_spoofing_prob', 'known_spoofing'] else (None, None)
                self._create_heatmap_subplot(data_filtered, axes[0, i], col, title, cmap, vmin, vmax)
            else:
                axes[0, i].text(0.5, 0.5, f'No {title} Data', ha='center', va='center', transform=axes[0, i].transAxes)
                axes[0, i].set_title(f'{title} (No Data)')
        
        # é¢„æµ‹è¯¯å·®çƒ­åŠ›å›¾ (å¦‚æœæœ‰æ¨¡å‹å’ŒçœŸå®æ ‡ç­¾)
        if has_model and has_labels:
            data_filtered['prediction_error'] = abs(data_filtered['model_spoofing_prob'] - data_filtered['known_spoofing'])
            self._create_heatmap_subplot(data_filtered, axes[1, 0], 'prediction_error', 'Prediction Error', 'RdYlBu_r', 0, 1)
        else:
            axes[1, 0].text(0.5, 0.5, 'Requires Model Prediction and True Labels', ha='center', va='center', transform=axes[1, 0].transAxes)
            axes[1, 0].set_title('Prediction Error (No Data)')
        
        # æ—¶é—´åˆ†å¸ƒå¯¹æ¯”å›¾
        if has_model and has_labels:
            # æ¨¡å‹vsçœŸå®æ ‡ç­¾å¯¹æ¯”
            hourly_summary = df.groupby('hour').agg({
                'known_spoofing': 'mean',
                'model_spoofing_prob': 'mean'
            })
            
            x = hourly_summary.index
            width = 0.35
            x_pos = np.arange(len(x))
            
            axes[1, 1].bar(x_pos - width/2, hourly_summary['known_spoofing'], width, alpha=0.8, color='blue', label='True Labels')
            axes[1, 1].bar(x_pos + width/2, hourly_summary['model_spoofing_prob'], width, alpha=0.8, color='red', label='Model Prediction')
            axes[1, 1].set_xlabel('Hour')
            axes[1, 1].set_ylabel('Manipulation Probability')
            axes[1, 1].set_title('Temporal Distribution Comparison')
            axes[1, 1].set_xticks(x_pos)
            axes[1, 1].set_xticklabels(x)
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        else:
            # ç®€å•çš„æ—¶é—´åˆ†å¸ƒ
            hourly_summary = df.groupby('hour')['composite_anomaly_score'].mean()
            axes[1, 1].plot(hourly_summary.index, hourly_summary.values, marker='o', linewidth=2, color='orange')
            axes[1, 1].set_xlabel('Hour')
            axes[1, 1].set_ylabel('Average Anomaly Score')
            axes[1, 1].set_title('Temporal Distribution')
            axes[1, 1].grid(True, alpha=0.3)
        
        # è‚¡ç¥¨é£é™©æ’å
        if 'composite_anomaly_score' in df.columns:
            stock_risk = df.groupby('ticker')['composite_anomaly_score'].mean().nlargest(10)
            y_pos = np.arange(len(stock_risk))
            axes[1, 2].barh(y_pos, stock_risk.values, color='coral')
            axes[1, 2].set_yticks(y_pos)
            axes[1, 2].set_yticklabels(stock_risk.index)
            axes[1, 2].set_xlabel('Average Anomaly Score')
            axes[1, 2].set_title('Top 10 High-Risk Stocks')
            axes[1, 2].grid(True, alpha=0.3)
        else:
            axes[1, 2].text(0.5, 0.5, 'No Risk Data', ha='center', va='center', transform=axes[1, 2].transAxes)
            axes[1, 2].set_title('Stock Risk Ranking (No Data)')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=self.dpi, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å°æ—¶çº§çƒ­åŠ›å›¾å·²ä¿å­˜: {output_path}")
    
    def create_daily_manipulation_heatmap(self, df, output_path):
        """
        åˆ›å»ºæ—¥çº§æ“çºµè¡Œä¸ºçƒ­åŠ›å›¾
        
        Args:
            df: åŒ…å«åˆ†æç»“æœçš„DataFrame
            output_path: è¾“å‡ºè·¯å¾„
        """
        print("ğŸ“ˆ åˆ›å»ºæ—¥çº§æ“çºµè¡Œä¸ºçƒ­åŠ›å›¾...")
        
        if 'è‡ªç„¶æ—¥' not in df.columns or 'ticker' not in df.columns:
            print("âš ï¸ ç¼ºå°‘å¿…è¦çš„æ—¥æœŸæˆ–è‚¡ç¥¨åˆ—ï¼Œè·³è¿‡æ—¥çº§çƒ­åŠ›å›¾")
            return
        
        # å‡†å¤‡æ•°æ®
        daily_data = df.groupby(['ticker', 'è‡ªç„¶æ—¥']).agg({
            'is_anomalous_period': 'mean',
            'composite_anomaly_score': 'mean',
            'known_spoofing': 'mean' if 'known_spoofing' in df.columns else lambda x: 0
        }).reset_index()
        
        # å–å‰15ä¸ªæœ€æ´»è·ƒçš„è‚¡ç¥¨å’Œæœ€è¿‘çš„æ—¥æœŸ
        top_tickers = df.groupby('ticker')['is_anomalous_period'].mean().nlargest(15).index
        recent_dates = sorted(df['è‡ªç„¶æ—¥'].unique())[-10:]  # æœ€è¿‘10å¤©
        
        daily_data_filtered = daily_data[
            (daily_data['ticker'].isin(top_tickers)) & 
            (daily_data['è‡ªç„¶æ—¥'].isin(recent_dates))
        ]
        
        if daily_data_filtered.empty:
            print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿæ•°æ®åˆ›å»ºæ—¥çº§çƒ­åŠ›å›¾")
            return
        
        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(1, 2, figsize=self.figsize, dpi=self.dpi)
        fig.suptitle('Daily Manipulation Behavior Distribution Heatmap', fontsize=16, fontweight='bold')
        
        # 1. å¼‚å¸¸æ—¶æ®µæ¯”ä¾‹çƒ­åŠ›å›¾
        pivot_anomaly = daily_data_filtered.pivot(index='ticker', columns='è‡ªç„¶æ—¥', values='is_anomalous_period')
        sns.heatmap(pivot_anomaly, ax=axes[0], cmap='Reds', cbar_kws={'label': 'Anomaly Ratio'})
        axes[0].set_title('Daily Anomalous Period Detection Heatmap')
        axes[0].set_xlabel('Date')
        axes[0].set_ylabel('Stock Code')
        axes[0].tick_params(axis='x', rotation=45)
        
        # 2. ç»¼åˆå¼‚å¸¸å¾—åˆ†çƒ­åŠ›å›¾
        pivot_score = daily_data_filtered.pivot(index='ticker', columns='è‡ªç„¶æ—¥', values='composite_anomaly_score')
        sns.heatmap(pivot_score, ax=axes[1], cmap='YlOrRd', cbar_kws={'label': 'Anomaly Score'})
        axes[1].set_title('Daily Composite Anomaly Score Heatmap')
        axes[1].set_xlabel('Date')
        axes[1].set_ylabel('Stock Code')
        axes[1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=self.dpi, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ—¥çº§çƒ­åŠ›å›¾å·²ä¿å­˜: {output_path}")
    
    def create_prediction_comparison_heatmap(self, df, output_path):
        """
        åˆ›å»ºé¢„æµ‹vsçœŸå®æ ‡ç­¾å¯¹æ¯”çƒ­åŠ›å›¾ (ç®€åŒ–ç‰ˆ)
        
        Args:
            df: åŒ…å«åˆ†æç»“æœçš„DataFrame  
            output_path: è¾“å‡ºè·¯å¾„
        """
        print("ğŸ“ˆ åˆ›å»ºé¢„æµ‹vsçœŸå®æ ‡ç­¾å¯¹æ¯”çƒ­åŠ›å›¾...")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        if not all(col in df.columns for col in ['known_spoofing', 'model_spoofing_prob', 'hour', 'ticker']):
            print("âš ï¸ ç¼ºå°‘é¢„æµ‹å¯¹æ¯”æ‰€éœ€çš„åˆ—ï¼Œè·³è¿‡é¢„æµ‹å¯¹æ¯”çƒ­åŠ›å›¾")
            return
        
        if df['known_spoofing'].sum() == 0:
            print("âš ï¸ æ²¡æœ‰çœŸå®çš„æ“çºµæ ‡ç­¾æ•°æ®ï¼Œè·³è¿‡é¢„æµ‹å¯¹æ¯”çƒ­åŠ›å›¾")
            return
        
        # ç®€åŒ–ä¸º2x2å¸ƒå±€
        fig, axes = plt.subplots(2, 2, figsize=(15, 10), dpi=self.dpi)
        fig.suptitle('Model Prediction vs True Labels Comparison Analysis', fontsize=16, fontweight='bold')
        
        # èšåˆæ•°æ®
        comparison_data = df.groupby(['ticker', 'hour']).agg({
            'known_spoofing': 'mean',
            'model_spoofing_prob': 'mean'
        }).reset_index()
        
        # é€‰æ‹©Top10ç›¸å…³è‚¡ç¥¨
        stock_relevance = df.groupby('ticker').agg({
            'known_spoofing': 'sum',
            'model_spoofing_prob': 'mean'
        })
        stock_relevance['score'] = stock_relevance['known_spoofing'] + stock_relevance['model_spoofing_prob']
        top_stocks = stock_relevance.nlargest(10, 'score').index
        
        data_filtered = comparison_data[comparison_data['ticker'].isin(top_stocks)]
        
        if data_filtered.empty:
            print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿæ•°æ®åˆ›å»ºé¢„æµ‹å¯¹æ¯”çƒ­åŠ›å›¾")
            return
        
        # 2x2çƒ­åŠ›å›¾
        heatmap_configs = [
            ('known_spoofing', 'True Manipulation Behavior Distribution', 'Blues'),
            ('model_spoofing_prob', 'Model Prediction Probability Distribution', 'Reds'),
        ]
        
        for i, (col, title, cmap) in enumerate(heatmap_configs):
            self._create_heatmap_subplot(data_filtered, axes[0, i], col, title, cmap, 0, 1)
        
        # é¢„æµ‹è¯¯å·®
        data_filtered['prediction_error'] = abs(data_filtered['model_spoofing_prob'] - data_filtered['known_spoofing'])
        self._create_heatmap_subplot(data_filtered, axes[1, 0], 'prediction_error', 'Prediction Error', 'RdYlBu_r', 0, 1)
        
        # æ€§èƒ½ç»Ÿè®¡
        y_true = df['known_spoofing']
        y_pred = df['model_spoofing_prob']
        
        from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
        
        y_pred_binary = (y_pred > 0.5).astype(int)
        metrics = {
            'ç²¾ç¡®åº¦': precision_score(y_true, y_pred_binary, zero_division=0),
            'å¬å›ç‡': recall_score(y_true, y_pred_binary, zero_division=0),
            'F1åˆ†æ•°': f1_score(y_true, y_pred_binary, zero_division=0),
            'å‡†ç¡®åº¦': accuracy_score(y_true, y_pred_binary)
        }
        
        # åœ¨ç¬¬å››ä¸ªå­å›¾æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
        ax = axes[1, 1]
        ax.axis('off')
        
        text_lines = ['Model Performance Metrics:', '']
        metric_translations = {
            'ç²¾ç¡®åº¦': 'Precision',
            'å¬å›ç‡': 'Recall', 
            'F1åˆ†æ•°': 'F1 Score',
            'å‡†ç¡®åº¦': 'Accuracy'
        }
        
        for metric, value in metrics.items():
            english_metric = metric_translations.get(metric, metric)
            text_lines.append(f'{english_metric}: {value:.3f}')
        
        ax.text(0.1, 0.9, '\n'.join(text_lines), transform=ax.transAxes, 
                fontsize=12, verticalalignment='top', 
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=self.dpi, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… é¢„æµ‹å¯¹æ¯”çƒ­åŠ›å›¾å·²ä¿å­˜: {output_path}")
        print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½: ç²¾ç¡®åº¦={metrics['ç²¾ç¡®åº¦']:.3f}, å¬å›ç‡={metrics['å¬å›ç‡']:.3f}, F1={metrics['F1åˆ†æ•°']:.3f}")

    def create_manipulation_correlation_heatmap(self, df, output_path):
        """
        åˆ›å»ºæ“çºµæŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾ (ç®€åŒ–ç‰ˆ)
        
        Args:
            df: åŒ…å«åˆ†æç»“æœçš„DataFrame
            output_path: è¾“å‡ºè·¯å¾„
        """
        print("ğŸ“ˆ åˆ›å»ºæ“çºµæŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾...")
        
        # é€‰æ‹©æ ¸å¿ƒæŒ‡æ ‡ï¼ˆä¼˜å…ˆä½¿ç”¨æœ€é‡è¦çš„å‡ ä¸ªï¼‰
        priority_cols = [
            'composite_anomaly_score', 'model_spoofing_prob', 'known_spoofing',
            'statistical_anomaly_score', 'is_anomalous_period'
        ]
        
        # é€‰æ‹©å­˜åœ¨çš„åˆ—
        available_cols = [col for col in priority_cols if col in df.columns]
        
        # å¦‚æœæ ¸å¿ƒåˆ—ä¸å¤Ÿï¼Œæ·»åŠ å…¶ä»–å¼‚å¸¸æŒ‡æ ‡
        if len(available_cols) < 3:
            other_cols = [
                'order_frequency_anomaly', 'cancel_ratio_anomaly', 
                'price_volatility_anomaly', 'qty_anomaly', 'spread_anomaly'
            ]
            available_cols.extend([col for col in other_cols if col in df.columns])
        
        # æœ€å°‘éœ€è¦2åˆ—æ‰èƒ½è®¡ç®—ç›¸å…³æ€§
        if len(available_cols) < 2:
            print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„æŒ‡æ ‡åˆ—åˆ›å»ºç›¸å…³æ€§çƒ­åŠ›å›¾")
            return
        
        # é™åˆ¶æœ€å¤š10ä¸ªæŒ‡æ ‡ï¼Œé¿å…å›¾è¿‡äºå¤æ‚
        available_cols = available_cols[:10]
        
        try:
            # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            corr_matrix = df[available_cols].corr()
            
            # åˆ›å»ºå›¾å½¢
            fig, ax = plt.subplots(1, 1, figsize=(10, 8), dpi=self.dpi)
            
            # åˆ›å»ºç›¸å…³æ€§çƒ­åŠ›å›¾ï¼ˆåªæ˜¾ç¤ºä¸‹ä¸‰è§’ï¼‰
            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
            
            sns.heatmap(
                corr_matrix, 
                mask=mask,
                ax=ax, 
                cmap='RdBu_r', 
                center=0,
                square=True,
                annot=True, 
                fmt='.2f',  # ç®€åŒ–ä¸º2ä½å°æ•°
                cbar_kws={'label': 'ç›¸å…³ç³»æ•°'}
            )
            
            ax.set_title('Manipulation Behavior Indicators Correlation Analysis', fontsize=14, fontweight='bold')
            
            plt.tight_layout()
            plt.savefig(output_path, dpi=self.dpi, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… ç›¸å…³æ€§çƒ­åŠ›å›¾å·²ä¿å­˜: {output_path}")
            
        except Exception as e:
            print(f"âš ï¸ ç›¸å…³æ€§çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {e}")

def generate_manipulation_report(df, patterns, output_path):
    """
    ç”Ÿæˆæ“çºµè¡Œä¸ºåˆ†ææŠ¥å‘Š
    
    Args:
        df: åˆ†ææ•°æ®
        patterns: æ£€æµ‹åˆ°çš„æ¨¡å¼
        output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„
    """
    print("ğŸ“ ç”Ÿæˆæ“çºµè¡Œä¸ºåˆ†ææŠ¥å‘Š...")
    
    report = []
    report.append("# æ½œåœ¨æ“çºµè¡Œä¸ºæ£€æµ‹åˆ†ææŠ¥å‘Š")
    report.append("=" * 50)
    report.append(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"æ•°æ®æ ·æœ¬æ•°: {len(df):,}")
    report.append("")
    
    # æ€»ä½“ç»Ÿè®¡
    report.append("## 1. æ€»ä½“ç»Ÿè®¡")
    report.append("-" * 20)
    if 'is_anomalous_period' in df.columns:
        anomaly_rate = df['is_anomalous_period'].mean()
        report.append(f"å¼‚å¸¸æ—¶æ®µæ¯”ä¾‹: {anomaly_rate:.2%}")
        report.append(f"å¼‚å¸¸æ—¶æ®µæ•°é‡: {df['is_anomalous_period'].sum():,}")
    
    if 'composite_anomaly_score' in df.columns:
        score_stats = df['composite_anomaly_score'].describe()
        report.append(f"å¼‚å¸¸å¾—åˆ†ç»Ÿè®¡:")
        report.append(f"  å‡å€¼: {score_stats['mean']:.3f}")
        report.append(f"  æ ‡å‡†å·®: {score_stats['std']:.3f}")
        report.append(f"  95%åˆ†ä½æ•°: {score_stats['75%']:.3f}")
    
    # æ¨¡å‹é¢„æµ‹æ€§èƒ½ç»Ÿè®¡
    if 'model_spoofing_prob' in df.columns and 'known_spoofing' in df.columns and df['known_spoofing'].sum() > 0:
        report.append("æ¨¡å‹é¢„æµ‹æ€§èƒ½:")
        y_true = df['known_spoofing']
        y_pred_prob = df['model_spoofing_prob']
        y_pred_binary = (y_pred_prob > 0.5).astype(int)
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        from sklearn.metrics import classification_report, confusion_matrix, average_precision_score, roc_auc_score
        try:
            # ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°
            tp = ((y_true == 1) & (y_pred_binary == 1)).sum()
            fp = ((y_true == 0) & (y_pred_binary == 1)).sum()
            fn = ((y_true == 1) & (y_pred_binary == 0)).sum()
            tn = ((y_true == 0) & (y_pred_binary == 0)).sum()
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            accuracy = (tp + tn) / len(y_true) if len(y_true) > 0 else 0
            
            report.append(f"  ç²¾ç¡®åº¦: {precision:.3f}")
            report.append(f"  å¬å›ç‡: {recall:.3f}")
            report.append(f"  F1åˆ†æ•°: {f1_score:.3f}")
            report.append(f"  å‡†ç¡®åº¦: {accuracy:.3f}")
            
            # AUCæŒ‡æ ‡
            pr_auc = average_precision_score(y_true, y_pred_prob)
            roc_auc = roc_auc_score(y_true, y_pred_prob)
            report.append(f"  PR-AUC: {pr_auc:.3f}")
            report.append(f"  ROC-AUC: {roc_auc:.3f}")
            
            # æ··æ·†çŸ©é˜µ
            report.append(f"  æ··æ·†çŸ©é˜µ:")
            report.append(f"    çœŸæ­£ä¾‹(TP): {tp:,}")
            report.append(f"    å‡æ­£ä¾‹(FP): {fp:,}")
            report.append(f"    å‡è´Ÿä¾‹(FN): {fn:,}")
            report.append(f"    çœŸè´Ÿä¾‹(TN): {tn:,}")
            
        except Exception as e:
            report.append(f"  æ€§èƒ½è®¡ç®—å¤±è´¥: {e}")
    
    report.append("")
    
    # æ—¶é—´æ¨¡å¼
    if 'peak_hours' in patterns:
        report.append("## 2. æ—¶é—´æ¨¡å¼åˆ†æ")
        report.append("-" * 20)
        report.append(f"å¼‚å¸¸æ´»åŠ¨é«˜å³°æ—¶æ®µ: {patterns['peak_hours']}")
        if 'hourly_distribution' in patterns:
            hourly_dist = patterns['hourly_distribution']
            top_hours = sorted(hourly_dist.items(), key=lambda x: x[1], reverse=True)[:5]
            report.append("å‰5ä¸ªé«˜é£é™©æ—¶æ®µ:")
            for hour, rate in top_hours:
                report.append(f"  {hour:2d}:00 - å¼‚å¸¸ç‡ {rate:.2%}")
        report.append("")
    
    # è‚¡ç¥¨æ¨¡å¼
    if 'top_manipulated_stocks' in patterns:
        report.append("## 3. è‚¡ç¥¨é£é™©åˆ†æ")
        report.append("-" * 20)
        report.append(f"é«˜é£é™©è‚¡ç¥¨ (å‰5): {patterns['top_manipulated_stocks']}")
        if 'ticker_distribution' in patterns:
            ticker_dist = patterns['ticker_distribution']
            top_tickers = sorted(ticker_dist.items(), key=lambda x: x[1], reverse=True)[:10]
            report.append("å‰10ä¸ªé«˜é£é™©è‚¡ç¥¨:")
            for ticker, rate in top_tickers:
                report.append(f"  {ticker:<10} - å¼‚å¸¸ç‡ {rate:.2%}")
        report.append("")
    
    # èšç±»åˆ†æ
    if 'cluster_analysis' in patterns:
        report.append("## 4. å¼‚å¸¸è¡Œä¸ºèšç±»åˆ†æ")
        report.append("-" * 20)
        cluster_stats = patterns['cluster_analysis']
        if not cluster_stats.empty:
            report.append("èšç±»ç»Ÿè®¡:")
            for cluster_id in cluster_stats.index:
                stats_data = cluster_stats.loc[cluster_id]
                report.append(f"  èšç±» {cluster_id}:")
                if hasattr(stats_data, 'composite_anomaly_score'):
                    score_stats = stats_data['composite_anomaly_score']
                    report.append(f"    æ ·æœ¬æ•°: {score_stats['count']}")
                    report.append(f"    å¹³å‡å¼‚å¸¸å¾—åˆ†: {score_stats['mean']:.3f}")
        report.append("")
    
    # é£é™©è¯„ä¼°
    if 'high_risk_threshold' in patterns:
        report.append("## 5. é£é™©è¯„ä¼°")
        report.append("-" * 20)
        report.append(f"é«˜é£é™©é˜ˆå€¼ (95%åˆ†ä½æ•°): {patterns['high_risk_threshold']:.3f}")
        report.append(f"é«˜é£é™©æ—¶æ®µæ•°é‡: {patterns['high_risk_count']:,}")
        
        if patterns['high_risk_count'] > 0:
            high_risk_rate = patterns['high_risk_count'] / len(df)
            report.append(f"é«˜é£é™©æ—¶æ®µæ¯”ä¾‹: {high_risk_rate:.2%}")
        report.append("")
    
    # å»ºè®®
    report.append("## 6. ç›‘ç®¡å»ºè®®")
    report.append("-" * 20)
    report.append("åŸºäºåˆ†æç»“æœï¼Œå»ºè®®é‡ç‚¹å…³æ³¨ï¼š")
    
    if 'peak_hours' in patterns and patterns['peak_hours']:
        report.append(f"â€¢ æ—¶é—´ç»´åº¦: {patterns['peak_hours']} æ—¶æ®µçš„äº¤æ˜“æ´»åŠ¨")
    
    if 'top_manipulated_stocks' in patterns and patterns['top_manipulated_stocks']:
        report.append(f"â€¢ è‚¡ç¥¨ç»´åº¦: {', '.join(patterns['top_manipulated_stocks'][:3])} ç­‰é«˜é£é™©è‚¡ç¥¨")
    
    report.append("â€¢ å»ºç«‹å®æ—¶ç›‘æ§æœºåˆ¶ï¼Œå¯¹å¼‚å¸¸å¾—åˆ†è¶…è¿‡é˜ˆå€¼çš„äº¤æ˜“è¿›è¡Œé¢„è­¦")
    report.append("â€¢ ç»“åˆå†å²æ•°æ®ï¼Œå»ºç«‹æ“çºµè¡Œä¸ºçš„é¢„æµ‹æ¨¡å‹")
    report.append("â€¢ åŠ å¼ºå¯¹é«˜é¢‘äº¤æ˜“å’Œç®—æ³•äº¤æ˜“çš„ç›‘ç®¡")
    
    # å†™å…¥æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_path}")

def main():
    """ä¸»å‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    parser = argparse.ArgumentParser(description="æ½œåœ¨æ“çºµæ—¶æ®µè¯†åˆ«ä¸å¼‚å¸¸äº¤æ˜“çƒ­åŠ›å›¾åˆ†æï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    parser.add_argument("--data_root", required=True, help="æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--train_regex", default="202503|202504", help="è®­ç»ƒæ•°æ®æ—¥æœŸæ­£åˆ™")
    parser.add_argument("--valid_regex", default="202505", help="éªŒè¯æ•°æ®æ—¥æœŸæ­£åˆ™")
    parser.add_argument("--output_dir", default="results/manipulation_analysis", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--model_path", help="å·²è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ (.pkl, .joblib, .txtç­‰)")
    parser.add_argument("--model_features_path", help="æ¨¡å‹ç‰¹å¾åˆ—è¡¨æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--contamination", type=float, default=0.1, help="å¼‚å¸¸æ£€æµ‹æ±¡æŸ“ç‡")
    parser.add_argument("--min_samples", type=int, default=5, help="DBSCANæœ€å°æ ·æœ¬æ•°")
    parser.add_argument("--sample_size", type=int, default=0, help="é‡‡æ ·å¤§å°ï¼ˆ0è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä»…åœ¨éœ€è¦å¿«é€Ÿæµ‹è¯•æ—¶ä½¿ç”¨ï¼‰")
    parser.add_argument("--batch_size", type=int, default=10000, help="æ¨¡å‹é¢„æµ‹æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_workers", type=int, default=None, help="å¹¶è¡ŒåŠ è½½çš„æœ€å¤§çº¿ç¨‹æ•°")
    parser.add_argument("--no_progress", action="store_true", help="ç¦ç”¨è¿›åº¦æ¡")
    parser.add_argument("--disable_parallel", action="store_true", help="ç¦ç”¨ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å¹¶å‘")
    parser.add_argument("--anomaly_workers", type=int, default=None, help="ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ä¸“ç”¨çº¿ç¨‹æ•°")
    parser.add_argument("--fast_mode", action="store_true", help="å¿«é€Ÿæ¨¡å¼ï¼šä»…ä½¿ç”¨IsolationForestï¼Œé¿å…å¡æ­»")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
    monitor = PerformanceMonitor()
    monitor.start()
    
    # è®¾ç½®è¿›åº¦æ¡å…¨å±€ç¦ç”¨
    if args.no_progress:
        tqdm.disable = True
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("ğŸš€ å¼€å§‹æ½œåœ¨æ“çºµæ—¶æ®µè¯†åˆ«ä¸çƒ­åŠ›å›¾åˆ†æ (ä¼˜åŒ–ç‰ˆ)...")
    print(f"ğŸ’¾ æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ§  CPUæ ¸å¿ƒæ•°: {mp.cpu_count()}, æœ€å¤§çº¿ç¨‹æ•°: {args.max_workers or min(mp.cpu_count(), 8)}")
    
    if args.model_path:
        print(f"ğŸ¤– è®­ç»ƒæ¨¡å‹è·¯å¾„: {args.model_path}")
        if args.model_features_path:
            print(f"ğŸ“‹ æ¨¡å‹ç‰¹å¾è·¯å¾„: {args.model_features_path}")
    else:
        print("âš ï¸ æœªæä¾›æ¨¡å‹è·¯å¾„ï¼Œå°†ä»…ä½¿ç”¨ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹")
    
    # 1. å¹¶è¡ŒåŠ è½½æ•°æ®
    print("\nğŸš€ å¹¶è¡ŒåŠ è½½æ•°æ®...")
    
    # å°è¯•æ–°çš„æ•°æ®æ¶æ„
    feat_pats = [os.path.join(args.data_root, "features", "X_*.parquet")]
    lab_pats = [os.path.join(args.data_root, "labels", "labels_*.parquet")]
    
    # å¦‚æœæ–°è·¯å¾„ä¸å­˜åœ¨ï¼Œå›é€€åˆ°æ—§è·¯å¾„
    if not glob.glob(feat_pats[0]):
        feat_pats = [os.path.join(args.data_root, "features_select", "X_*.parquet")]
        lab_pats = [os.path.join(args.data_root, "labels_select", "labels_*.parquet")]
    
    # æ”¶é›†ç‰¹å¾æ–‡ä»¶
    feature_files = []
    for pat in feat_pats:
        feature_files.extend(sorted(glob.glob(pat)))
    
    if not feature_files:
        # å°è¯•labels_enhancedç›®å½•
        enhanced_lab_pat = os.path.join(args.data_root, "labels_enhanced", "labels_*.parquet")
        feature_files.extend(sorted(glob.glob(enhanced_lab_pat)))
    
    if not feature_files:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return
    
    # å¹¶è¡ŒåŠ è½½ç‰¹å¾æ•°æ®
    print(f"ğŸ“Š å¹¶è¡ŒåŠ è½½ {len(feature_files)} ä¸ªç‰¹å¾æ–‡ä»¶...")
    feature_dfs = parallel_load_parquet(
        feature_files, 
        max_workers=args.max_workers,
        desc="Loading feature files"
    )
    
    if feature_dfs:
        print("ğŸ”„ åˆå¹¶ç‰¹å¾æ•°æ®...")
        df_feat = pd.concat(feature_dfs, ignore_index=True)
        del feature_dfs  # é‡Šæ”¾å†…å­˜
        gc.collect()
        monitor.checkpoint("Feature files loaded")
    else:
        print("âŒ ç‰¹å¾æ•°æ®åŠ è½½å¤±è´¥")
        return
    
    # æ”¶é›†æ ‡ç­¾æ–‡ä»¶
    label_files = []
    for pat in lab_pats:
        label_files.extend(sorted(glob.glob(pat)))
    
    # å¦‚æœæ ‡å‡†ä½ç½®æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œå°è¯•labels_enhancedç›®å½•
    if not label_files:
        enhanced_lab_pat = os.path.join(args.data_root, "labels_enhanced", "labels_*.parquet")
        label_files.extend(sorted(glob.glob(enhanced_lab_pat)))
    
    if label_files:
        print(f"ğŸ·ï¸ å¹¶è¡ŒåŠ è½½ {len(label_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶...")
        label_dfs = parallel_load_parquet(
            label_files,
            max_workers=args.max_workers,
            desc="Loading label files"
        )
        
        if label_dfs:
            print("ğŸ”„ åˆå¹¶æ ‡ç­¾æ•°æ®...")
            df_lab = pd.concat(label_dfs, ignore_index=True)
            del label_dfs  # é‡Šæ”¾å†…å­˜
            gc.collect()
            monitor.checkpoint("Label files loaded")
            
            # åˆå¹¶æ•°æ®
            print("ğŸ”— åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾æ•°æ®...")
            df = df_feat.merge(df_lab, on=["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"], how="left")
            del df_feat, df_lab  # é‡Šæ”¾å†…å­˜
            gc.collect()
        else:
            print("âš ï¸ æ ‡ç­¾æ•°æ®åŠ è½½å¤±è´¥ï¼Œä»…ä½¿ç”¨ç‰¹å¾æ•°æ®")
            df = df_feat
    else:
        print("âš ï¸ æœªæ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶ï¼Œä»…ä½¿ç”¨ç‰¹å¾æ•°æ®")
        df = df_feat
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
    monitor.checkpoint("Data loading completed")
    
    # è¿‡æ»¤æ•°æ®
    if args.train_regex and args.valid_regex:
        print("ğŸ” åº”ç”¨æ—¥æœŸè¿‡æ»¤...")
        regex_pattern = f"{args.train_regex}|{args.valid_regex}"
        mask = df["è‡ªç„¶æ—¥"].astype(str).str.contains(regex_pattern)
        df = df[mask].copy()
        print(f"âœ… æ—¥æœŸè¿‡æ»¤å: {df.shape}")
        monitor.checkpoint("Date filtering")
    
    # é‡‡æ ·ï¼ˆå¦‚æœæ•°æ®å¤ªå¤§ï¼‰
    if args.sample_size > 0 and len(df) > args.sample_size:
        print(f"ğŸ¯ éšæœºé‡‡æ · {args.sample_size:,} æ¡è®°å½•...")
        df = df.sample(n=args.sample_size, random_state=42)
        print(f"âœ… éšæœºé‡‡æ ·å: {df.shape}")
        monitor.checkpoint("Data sampling")
    
    # 1.5. åº”ç”¨ç‰¹å¾å·¥ç¨‹ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    if HAS_FEATURE_ENGINEERING:
        print("\nğŸ”§ åº”ç”¨è®­ç»ƒæ—¶çš„ç‰¹å¾å·¥ç¨‹...")
        with tqdm(total=1, desc="Feature engineering") as pbar:
            df = enhance_features(df)
            pbar.update(1)
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å: {df.shape}")
        monitor.checkpoint("Feature engineering")
    else:
        print("\nâš ï¸ è·³è¿‡ç‰¹å¾å·¥ç¨‹ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾ï¼ˆå¯èƒ½å½±å“æ¨¡å‹é¢„æµ‹å‡†ç¡®æ€§ï¼‰")
    
    # 2. åˆå§‹åŒ–æ£€æµ‹å™¨å’Œå¯è§†åŒ–å™¨
    print("\nğŸš€ åˆå§‹åŒ–åˆ†æç»„ä»¶...")
    detector = ManipulationDetector(
        contamination=args.contamination,
        min_samples=args.min_samples,
        model_path=args.model_path,
        enable_parallel=not args.disable_parallel,
        max_workers=args.anomaly_workers or args.max_workers,
        fast_mode=args.fast_mode
    )
    
    # å¦‚æœæœ‰å•ç‹¬çš„ç‰¹å¾æ–‡ä»¶ï¼ŒåŠ è½½ç‰¹å¾åˆ—è¡¨
    if args.model_features_path and os.path.exists(args.model_features_path):
        try:
            import json
            with open(args.model_features_path, 'r') as f:
                if args.model_features_path.endswith('.json'):
                    detector.model_features = json.load(f)
                else:
                    # å‡è®¾æ˜¯æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªç‰¹å¾
                    detector.model_features = f.read().strip().split('\n')
            print(f"âœ… ä»æ–‡ä»¶åŠ è½½äº† {len(detector.model_features)} ä¸ªæ¨¡å‹ç‰¹å¾")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç‰¹å¾æ–‡ä»¶å¤±è´¥: {e}")
    
    visualizer = HeatmapVisualizer()
    monitor.checkpoint("Component initialization")
    
    # 3. è®¡ç®—æ“çºµæŒ‡æ ‡
    print("\nğŸ“Š è®¡ç®—æ“çºµæŒ‡æ ‡...")
    df_with_indicators = detector.calculate_manipulation_indicators(df)
    monitor.checkpoint("Manipulation indicators calculated")
    
    # 4. æ£€æµ‹å¼‚å¸¸æ—¶æ®µ
    print("\nğŸ” æ£€æµ‹å¼‚å¸¸æ—¶æ®µ...")
    df_with_anomalies = detector.detect_anomalous_periods(df_with_indicators)
    monitor.checkpoint("Anomaly detection completed")
    
    # 5. è¯†åˆ«æ“çºµæ¨¡å¼
    print("\nğŸ¯ è¯†åˆ«æ“çºµæ¨¡å¼...")
    patterns = detector.identify_manipulation_patterns(df_with_anomalies)
    monitor.checkpoint("Pattern identification completed")
    
    # 6. ç”Ÿæˆçƒ­åŠ›å›¾
    print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    heatmap_tasks = [
        ("hourly_manipulation_heatmap.png", visualizer.create_hourly_manipulation_heatmap, "å°æ—¶çº§çƒ­åŠ›å›¾"),
        ("daily_manipulation_heatmap.png", visualizer.create_daily_manipulation_heatmap, "æ—¥çº§çƒ­åŠ›å›¾"),
        ("prediction_vs_true_labels_heatmap.png", visualizer.create_prediction_comparison_heatmap, "é¢„æµ‹å¯¹æ¯”çƒ­åŠ›å›¾"),
        ("manipulation_correlation_heatmap.png", visualizer.create_manipulation_correlation_heatmap, "ç›¸å…³æ€§çƒ­åŠ›å›¾")
    ]
    
    for filename, func, desc in tqdm(heatmap_tasks, desc="ç”Ÿæˆçƒ­åŠ›å›¾", unit="å›¾è¡¨"):
        try:
            output_path = output_dir / filename
            func(df_with_anomalies, output_path)
            print(f"âœ… {desc}å·²ç”Ÿæˆ: {filename}")
        except Exception as e:
            print(f"âš ï¸ {desc}ç”Ÿæˆå¤±è´¥: {e}")
    
    monitor.checkpoint("Heatmaps generated")
    
    # 7. ç”Ÿæˆåˆ†ææŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    report_path = output_dir / "manipulation_analysis_report.txt"
    generate_manipulation_report(df_with_anomalies, patterns, report_path)
    monitor.checkpoint("Analysis report generated")
    
    # 8. ä¿å­˜æ£€æµ‹ç»“æœ
    print("\nğŸ’¾ ä¿å­˜æ£€æµ‹ç»“æœ...")
    results_path = output_dir / "manipulation_detection_results.parquet"
    df_with_anomalies.to_parquet(results_path, index=False)
    print(f"âœ… æ£€æµ‹ç»“æœå·²ä¿å­˜: {results_path}")
    monitor.checkpoint("Results saved")
    
    # 9. è¾“å‡ºç»Ÿè®¡æ‘˜è¦
    print("\nğŸ“‹ åˆ†ææ‘˜è¦:")
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(df_with_anomalies):,}")
    if 'is_anomalous_period' in df_with_anomalies.columns:
        anomaly_count = df_with_anomalies['is_anomalous_period'].sum()
        anomaly_rate = df_with_anomalies['is_anomalous_period'].mean()
        print(f"ğŸš¨ å¼‚å¸¸æ—¶æ®µ: {anomaly_count:,} ({anomaly_rate:.2%})")
    
    if 'peak_hours' in patterns:
        print(f"â° é«˜é£é™©æ—¶æ®µ: {patterns['peak_hours']}")
    
    if 'top_manipulated_stocks' in patterns:
        print(f"ğŸ“ˆ é«˜é£é™©è‚¡ç¥¨: {patterns['top_manipulated_stocks'][:3]}")
    
    # æ€§èƒ½æ€»ç»“
    monitor.checkpoint("Analysis completed")
    monitor.summary()
    
    print(f"\nğŸ‰ åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    for file in output_dir.glob("*"):
        size_mb = file.stat().st_size / 1024 / 1024
        print(f"  ğŸ“„ {file.name} ({size_mb:.1f}MB)")
    
    # å†…å­˜æ¸…ç†
    gc.collect()
    final_memory = psutil.Process().memory_info().rss / 1024 / 1024
    print(f"\nğŸ’¾ æœ€ç»ˆå†…å­˜ä½¿ç”¨: {final_memory:.1f}MB")

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹ (ä¼˜åŒ–ç‰ˆ):

1. å¿«é€Ÿåˆ†æï¼ˆä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹ï¼Œæ¨èï¼‰ï¼š
python scripts/analysis/manipulation_detection_heatmap.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --model_path "results/trained_models/spoofing_model_Enhanced_undersample_Ensemble.pkl" \
  --output_dir "results/manipulation_analysis" \
  --batch_size 20000 \
  --max_workers 8 \
  --anomaly_workers 6

2. å¤§æ•°æ®é›†é«˜æ€§èƒ½å¤„ç†ï¼š
python scripts/analysis/manipulation_detection_heatmap.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --model_path "results/trained_models/spoofing_model_Enhanced_undersample_Ensemble.pkl" \
  --train_regex "202503|202504" \
  --valid_regex "202505" \
  --output_dir "results/manipulation_analysis" \
  --batch_size 50000 \
  --max_workers 16 \
  --anomaly_workers 8

3. å†…å­˜å—é™ç¯å¢ƒï¼š
python scripts/analysis/manipulation_detection_heatmap.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --model_path "results/trained_models/spoofing_model_Enhanced_undersample_Ensemble.pkl" \
  --batch_size 5000 \
  --max_workers 4 \
  --sample_size 100000

4. å¹¶å‘ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ä¸“ç”¨ï¼š
python scripts/analysis/manipulation_detection_heatmap.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --output_dir "results/manipulation_analysis" \
  --anomaly_workers 12 \
  --batch_size 30000

5. é™é»˜æ¨¡å¼ï¼ˆæ— è¿›åº¦æ¡ï¼‰ï¼š
python scripts/analysis/manipulation_detection_heatmap.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --model_path "results/trained_models/spoofing_model_Enhanced_undersample_Ensemble.pkl" \
  --no_progress

ğŸš€ ä¼˜åŒ–ç‰¹æ€§ï¼š
- å¹¶è¡Œæ•°æ®åŠ è½½ï¼šæ”¯æŒå¤šçº¿ç¨‹åŒæ—¶åŠ è½½parquetæ–‡ä»¶
- åˆ†æ‰¹æ¨¡å‹é¢„æµ‹ï¼šå¤§æ•°æ®é›†è‡ªåŠ¨åˆ†æ‰¹å¤„ç†ï¼ŒèŠ‚çœå†…å­˜
- å¹¶å‘ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ï¼šå¤šè‚¡ç¥¨å¹¶è¡Œè®¡ç®—ç»Ÿè®¡ç‰¹å¾ï¼Œå¤šç®—æ³•å¹¶è¡Œå¼‚å¸¸æ£€æµ‹
- é›†æˆå¼‚å¸¸æ£€æµ‹ï¼šIsolationForest + DBSCAN + LocalOutlierFactor æŠ•ç¥¨é›†æˆ
- å®æ—¶è¿›åº¦æ¡ï¼šæ˜¾ç¤ºæ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†è¿›åº¦
- æ€§èƒ½ç›‘æ§ï¼šè®°å½•æ¯ä¸ªé˜¶æ®µçš„è€—æ—¶å’Œå†…å­˜ä½¿ç”¨
- å†…å­˜ä¼˜åŒ–ï¼šè‡ªåŠ¨åƒåœ¾å›æ”¶å’Œå†…å­˜é‡Šæ”¾

ğŸ“Š æ€§èƒ½å‚æ•°ï¼š
- --batch_size: æ¨¡å‹é¢„æµ‹æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤10,000ï¼ˆå†…å­˜ä¸è¶³æ—¶å¯å‡å°ï¼‰
- --max_workers: å¹¶è¡ŒåŠ è½½çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°ï¼ˆå»ºè®®ä¸è¶…è¿‡16ï¼‰
- --anomaly_workers: ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ä¸“ç”¨çº¿ç¨‹æ•°ï¼Œé»˜è®¤ç»§æ‰¿max_workers
- --disable_parallel: ç¦ç”¨ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å¹¶å‘ï¼ˆè°ƒè¯•ç”¨ï¼‰
- --sample_size: æ•°æ®é‡‡æ ·å¤§å°ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•ï¼ˆ0è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
- --no_progress: ç¦ç”¨è¿›åº¦æ¡ï¼Œé€‚ç”¨äºæ—¥å¿—è®°å½•

ğŸ–¥ï¸ æœåŠ¡å™¨ç¯å¢ƒæ³¨æ„äº‹é¡¹ï¼š
- è„šæœ¬å·²ä¼˜åŒ–ç”¨äºæ— GUIçš„LinuxæœåŠ¡å™¨
- è‡ªåŠ¨å†…å­˜ç®¡ç†å’Œåƒåœ¾å›æ”¶
- æ”¯æŒå¤§æ•°æ®é›†çš„åˆ†æ‰¹å¤„ç†
- è¯¦ç»†çš„æ€§èƒ½ç›‘æ§å’ŒæŠ¥å‘Š

ğŸ“Š è¾“å‡ºæ–‡ä»¶ï¼š
- å°æ—¶çº§æ“çºµè¡Œä¸ºçƒ­åŠ›å›¾ï¼ˆ2x3å¸ƒå±€ï¼ŒåŒ…å«é¢„æµ‹æ€§èƒ½æŒ‡æ ‡ï¼‰
- æ—¥çº§æ“çºµè¡Œä¸ºçƒ­åŠ›å›¾
- é¢„æµ‹vsçœŸå®æ ‡ç­¾å¯¹æ¯”çƒ­åŠ›å›¾ï¼ˆ2x2å¸ƒå±€ï¼‰  
- æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾
- è¯¦ç»†çš„æ¨¡å‹æ€§èƒ½æŠ¥å‘Šï¼ˆç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ã€AUCç­‰ï¼‰
- ç»“æ„åŒ–æ£€æµ‹ç»“æœï¼ˆParquetæ ¼å¼ï¼‰
- æ€§èƒ½æŠ¥å‘Šï¼ˆåŒ…å«å„é˜¶æ®µè€—æ—¶å’Œå†…å­˜ä½¿ç”¨ï¼‰

âš¡ é¢„æœŸæ€§èƒ½æå‡ï¼š
- æ•°æ®åŠ è½½é€Ÿåº¦æå‡ 50-80%ï¼ˆå¹¶è¡ŒåŠ è½½ï¼‰
- ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹é€Ÿåº¦æå‡ 60-90%ï¼ˆå¹¶å‘è®¡ç®—+å¤šç®—æ³•å¹¶è¡Œï¼‰
- å¼‚å¸¸æ£€æµ‹å‡†ç¡®æ€§æå‡ 15-25%ï¼ˆé›†æˆå¤šç®—æ³•æŠ•ç¥¨ï¼‰
- å†…å­˜ä½¿ç”¨å‡å°‘ 30-50%ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
- æ•´ä½“è¿è¡Œæ—¶é—´å‡å°‘ 50-70%
""" 