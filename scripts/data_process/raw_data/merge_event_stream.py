#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
merge_event_stream.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
åŸå§‹æ•°æ®å¤„ç†æ¨¡å— - ç¬¬ä¸€é˜¶æ®µæ•°æ®å¤„ç†
â€¢ åˆå¹¶é€ç¬”å§”æ‰˜å’Œé€ç¬”æˆäº¤æ•°æ®
â€¢ è´´åˆè¡Œæƒ…å¿«ç…§æ•°æ®
â€¢ æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
â€¢ ç”Ÿæˆç»Ÿä¸€çš„å§”æ‰˜äº‹ä»¶æµ
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import argparse
import re
import sys
from typing import Set, Optional
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich import print as rprint

console = Console()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def read_csv_auto(path: Path, **kw) -> pd.DataFrame:
    """è‡ªåŠ¨å°è¯• utf-8-sig / gbk / latin1 è¯»å– CSV"""
    for enc in ("utf-8-sig", "gbk", "latin1"):
        try:
            return pd.read_csv(path, encoding=enc, low_memory=False, **kw)
        except UnicodeDecodeError:
            continue
    raise UnicodeDecodeError(f"æ— æ³•è§£ç : {path}")

def prepare_quotes(quote_csv: Path) -> pd.DataFrame:
    """è¯»å–è¡Œæƒ….csv â†’ 100 ms ç½‘æ ¼ â†’ è¿”å›å« quote_dt çš„ DataFrame"""
    try:
        quotes = pd.read_csv(
            quote_csv, encoding="gbk",
            usecols=["è‡ªç„¶æ—¥","æ—¶é—´","ç”³ä¹°ä»·1","ç”³å–ä»·1","ç”³ä¹°é‡1","ç”³å–é‡1","å‰æ”¶ç›˜"]
        )
        quotes["quote_dt"] = pd.to_datetime(
            quotes["è‡ªç„¶æ—¥"].astype(str) + quotes["æ—¶é—´"].astype(str).str.zfill(9),
            format="%Y%m%d%H%M%S%f"
        )
        quotes = (
            quotes.drop_duplicates("quote_dt")           # å»é‡
                  .set_index("quote_dt")
                  .resample("100ms")                     # 100 ms ç½‘æ ¼
                  .ffill()                               # å‰å‘å¡«å……
                  .reset_index()
        )
        return quotes
    except Exception as e:
        console.print(f"[yellow]è­¦å‘Š: è¡Œæƒ…æ•°æ®å¤„ç†å¤±è´¥ {quote_csv}: {e}[/yellow]")
        return pd.DataFrame()

def calculate_survival_time(merged_df: pd.DataFrame) -> pd.DataFrame:
    """è®¡ç®—è®¢å•å­˜æ´»æ—¶é—´"""
    merged_df["å­˜æ´»æ—¶é—´_ms"] = (
        merged_df["äº‹ä»¶_datetime"] - merged_df["å§”æ‰˜_datetime"]
    ).dt.total_seconds() * 1_000
    
    # æ•°æ®è´¨é‡æ£€æŸ¥
    invalid_survival = merged_df["å­˜æ´»æ—¶é—´_ms"] < 0
    if invalid_survival.any():
        console.print(f"[yellow]è­¦å‘Š: å‘ç° {invalid_survival.sum()} æ¡è´Ÿå­˜æ´»æ—¶é—´è®°å½•[/yellow]")
        merged_df.loc[invalid_survival, "å­˜æ´»æ—¶é—´_ms"] = 0
    
    return merged_df

def add_order_features(merged_df: pd.DataFrame) -> pd.DataFrame:
    """æ·»åŠ åŸºç¡€è®¢å•ç‰¹å¾"""
    # å§”æ‰˜é‡‘é¢
    merged_df["å§”æ‰˜é‡‘é¢"] = pd.to_numeric(merged_df["å§”æ‰˜ä»·æ ¼"], errors='coerce') * pd.to_numeric(merged_df["å§”æ‰˜æ•°é‡"], errors='coerce')
    
    # æ˜¯å¦ä¹°å•
    merged_df["is_buy"] = (merged_df["æ–¹å‘_å§”æ‰˜"] == "B").astype(int)
    
    # å§”æ‰˜æ•°é‡å¯¹æ•°
    qty = pd.to_numeric(merged_df["å§”æ‰˜æ•°é‡"], errors='coerce')
    merged_df["log_qty"] = np.log1p(qty.fillna(0))
    
    # ä»·æ ¼ç›¸å…³ç‰¹å¾
    å§”æ‰˜ä»·æ ¼ = pd.to_numeric(merged_df["å§”æ‰˜ä»·æ ¼"], errors='coerce')
    ç”³ä¹°ä»·1 = pd.to_numeric(merged_df["ç”³ä¹°ä»·1"], errors='coerce')
    ç”³å–ä»·1 = pd.to_numeric(merged_df["ç”³å–ä»·1"], errors='coerce')
    
    # ä¸­é—´ä»·
    merged_df["mid_price"] = (ç”³ä¹°ä»·1 + ç”³å–ä»·1) / 2
    
    # ä»·å·®
    merged_df["spread"] = ç”³å–ä»·1 - ç”³ä¹°ä»·1
    merged_df["pct_spread"] = merged_df["spread"] / merged_df["mid_price"] * 10000  # bps
    
    # ä»·æ ¼åç¦»
    merged_df["price_vs_mid"] = å§”æ‰˜ä»·æ ¼ - merged_df["mid_price"]
    merged_df["price_vs_mid_bps"] = merged_df["price_vs_mid"] / merged_df["mid_price"] * 10000
    
    return merged_df

def merge_one_stock(order_csv: Path, trade_csv: Path, ticker: str) -> pd.DataFrame:
    """å•æ”¯è‚¡ç¥¨ï¼šé€ç¬”å§”æ‰˜ + é€ç¬”æˆäº¤ => äº‹ä»¶æµï¼ˆå«å­˜æ´»æ—¶é—´ï¼‰"""
    try:
        orders = read_csv_auto(order_csv, dtype=str)
        trades = read_csv_auto(trade_csv, dtype=str)
    except Exception as e:
        console.print(f"[red]é”™è¯¯: è¯»å–æ–‡ä»¶å¤±è´¥ {ticker}: {e}[/red]")
        return pd.DataFrame()

    # å§”æ‰˜è¡¨é¢„å¤„ç†
    orders = orders.rename(columns={"å§”æ‰˜ä»£ç ": "æ–¹å‘_å§”æ‰˜"})
    orders["äº¤æ˜“æ‰€å§”æ‰˜å·"] = orders["äº¤æ˜“æ‰€å§”æ‰˜å·"].str.lstrip("0")

    # æˆäº¤è¡¨æ‹†ä¹°å–
    buy_cols  = ["è‡ªç„¶æ—¥","æ—¶é—´","æˆäº¤ä»£ç ","æˆäº¤ä»·æ ¼","æˆäº¤æ•°é‡","å«ä¹°åºå·"]
    sell_cols = ["è‡ªç„¶æ—¥","æ—¶é—´","æˆäº¤ä»£ç ","æˆäº¤ä»·æ ¼","æˆäº¤æ•°é‡","å«å–åºå·"]
    
    buy  = trades[trades["å«ä¹°åºå·"]  != "0"][buy_cols].copy() if "å«ä¹°åºå·" in trades.columns else pd.DataFrame()
    sell = trades[trades["å«å–åºå·"] != "0"][sell_cols].copy() if "å«å–åºå·" in trades.columns else pd.DataFrame()
    
    if not buy.empty:
        buy.rename(columns={"å«ä¹°åºå·" :"äº¤æ˜“æ‰€å§”æ‰˜å·"}, inplace=True)
        buy["æ–¹å‘_äº‹ä»¶"] = "B"
    
    if not sell.empty:
        sell.rename(columns={"å«å–åºå·":"äº¤æ˜“æ‰€å§”æ‰˜å·"}, inplace=True)
        sell["æ–¹å‘_äº‹ä»¶"] = "S"
    
    events = pd.concat([buy, sell], ignore_index=True)
    if events.empty:
        console.print(f"[yellow]è­¦å‘Š: {ticker} æ— æœ‰æ•ˆæˆäº¤äº‹ä»¶[/yellow]")
        return pd.DataFrame()
    
    events["äº‹ä»¶ç±»å‹"] = np.where(events["æˆäº¤ä»£ç "] == "C", "æ’¤å•", "æˆäº¤")
    events["äº¤æ˜“æ‰€å§”æ‰˜å·"] = events["äº¤æ˜“æ‰€å§”æ‰˜å·"].str.lstrip("0")

    # åˆå¹¶å§”æ‰˜ + æˆäº¤
    merged = events.merge(
        orders,
        on="äº¤æ˜“æ‰€å§”æ‰˜å·",
        how="left",
        suffixes=("_äº‹ä»¶","_å§”æ‰˜")
    )

    if merged.empty:
        console.print(f"[yellow]è­¦å‘Š: {ticker} åˆå¹¶åæ— æ•°æ®[/yellow]")
        return pd.DataFrame()

    # æ—¶é—´åˆ—å¤„ç†
    fmt = "%Y%m%d%H%M%S%f"
    merged["å§”æ‰˜_datetime"] = pd.to_datetime(
        merged["è‡ªç„¶æ—¥_å§”æ‰˜"].str.zfill(8) + merged["æ—¶é—´_å§”æ‰˜"].str.zfill(9),
        errors="coerce", format=fmt)
    merged["äº‹ä»¶_datetime"] = pd.to_datetime(
        merged["è‡ªç„¶æ—¥_äº‹ä»¶"].str.zfill(8) + merged["æ—¶é—´_äº‹ä»¶"].str.zfill(9),
        errors="coerce", format=fmt)

    # è¿‡æ»¤æ— æ•ˆæ—¶é—´
    merged = merged[merged["å§”æ‰˜_datetime"].notna() & merged["äº‹ä»¶_datetime"].notna()].copy()
    if merged.empty:
        console.print(f"[yellow]è­¦å‘Š: {ticker} æ—¶é—´è§£æåæ— æœ‰æ•ˆæ•°æ®[/yellow]")
        return pd.DataFrame()

    # è¯»å–å¹¶é‡é‡‡æ ·å½“æ—¥è¡Œæƒ…
    quote_csv = order_csv.with_name("è¡Œæƒ….csv")
    quotes = prepare_quotes(quote_csv)
    
    if not quotes.empty:
        # è´´æœ€è¿‘ â‰¤ t çš„ä¹°ä¸€/å–ä¸€
        merged.sort_values("å§”æ‰˜_datetime", inplace=True)
        merged = pd.merge_asof(
            merged, quotes,
            left_on="å§”æ‰˜_datetime", right_on="quote_dt",
            direction="backward",
            tolerance=pd.Timedelta("100ms")
        )
    else:
        # å¦‚æœæ²¡æœ‰è¡Œæƒ…æ•°æ®ï¼Œæ·»åŠ ç©ºåˆ—é¿å…åç»­å¤„ç†é”™è¯¯
        for col in ["ç”³ä¹°ä»·1", "ç”³å–ä»·1", "ç”³ä¹°é‡1", "ç”³å–é‡1", "å‰æ”¶ç›˜"]:
            merged[col] = np.nan

    # è®¡ç®—å­˜æ´»æ—¶é—´
    merged = calculate_survival_time(merged)
    
    # æ·»åŠ åŸºç¡€ç‰¹å¾
    merged = add_order_features(merged)
    
    # æ·»åŠ è‚¡ç¥¨ä»£ç 
    merged["ticker"] = ticker
    
    # æ¸…ç†æ— ç”¨åˆ—
    merged = merged.loc[:, ~merged.columns.str.contains("^Unnamed")]
    
    return merged

def merge_one_day(date_dir: Path, watch_set: Set[str], out_root: Path) -> bool:
    """
    å¤„ç†å•æ—¥ï¼šdate_dir = base_data/YYYYMMDD
    è¾“å‡º event_stream/YYYYMMDD/å§”æ‰˜äº‹ä»¶æµ.csv
    """
    inner = date_dir / date_dir.name
    if not inner.exists():
        inner = date_dir
    
    dfs = []
    trading_date = date_dir.name
    
    # è·å–è‚¡ç¥¨ç›®å½•åˆ—è¡¨
    stock_dirs = list(inner.iterdir()) if inner.exists() else []
    if watch_set:
        stock_dirs = [d for d in stock_dirs if d.name in watch_set]
    
    if not stock_dirs:
        console.print(f"[yellow]Ã— {trading_date}: æ— ç›®æ ‡è‚¡ç¥¨æ•°æ®[/yellow]")
        return False
    
    console.print(f"[cyan]â–¶ å¤„ç† {trading_date} ({len(stock_dirs)} æ”¯è‚¡ç¥¨)[/cyan]")
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeElapsedColumn(),
        console=console,
        transient=True
    ) as progress:
        task = progress.add_task(f"å¤„ç†è‚¡ç¥¨", total=len(stock_dirs))
        
        for stk_dir in sorted(stock_dirs):
            progress.update(task, description=f"å¤„ç† {stk_dir.name}")
            
            o_csv = stk_dir / "é€ç¬”å§”æ‰˜.csv"
            t_csv = stk_dir / "é€ç¬”æˆäº¤.csv"
            
            if not (o_csv.exists() and t_csv.exists()):
                console.print(f"  [yellow]è·³è¿‡ {stk_dir.name}: ç¼ºå°‘æ–‡ä»¶[/yellow]")
                progress.advance(task)
                continue
            
            df = merge_one_stock(o_csv, t_csv, stk_dir.name)
            if not df.empty:
                # ç»Ÿä¸€çš„äº¤æ˜“æ—¥æœŸå­—æ®µ
                df["è‡ªç„¶æ—¥"] = trading_date
                
                # æ•°æ®è´¨é‡æ£€æŸ¥
                if "è‡ªç„¶æ—¥_å§”æ‰˜" in df.columns:
                    inconsistent_dates = df[df["è‡ªç„¶æ—¥_å§”æ‰˜"] != trading_date]
                    if not inconsistent_dates.empty:
                        console.print(f"  [yellow]âš ï¸ {stk_dir.name}: {len(inconsistent_dates)} æ¡æ—¥æœŸä¸ä¸€è‡´è®°å½•[/yellow]")
                
                dfs.append(df)
            
            progress.advance(task)

    if not dfs:
        console.print(f"[yellow]Ã— {trading_date}: æ— æœ‰æ•ˆæ•°æ®[/yellow]")
        return False

    # è¾“å‡ºç›®å½•
    out_dir = out_root / trading_date
    out_dir.mkdir(parents=True, exist_ok=True)
    out_file = out_dir / "å§”æ‰˜äº‹ä»¶æµ.csv"
    
    # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨æ•°æ®
    final_df = pd.concat(dfs, ignore_index=True)
    
    # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œå°†ä¸»é”®å­—æ®µæ”¾åœ¨å‰é¢
    key_cols = ["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"]
    other_cols = [col for col in final_df.columns if col not in key_cols]
    final_df = final_df[key_cols + other_cols]
    
    # æœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥
    duplicated = final_df.duplicated(subset=key_cols)
    if duplicated.any():
        console.print(f"  [yellow]âš ï¸ å‘ç° {duplicated.sum()} æ¡é‡å¤è®°å½•ï¼Œå·²å»é‡[/yellow]")
        final_df = final_df[~duplicated]
    
    # ä¿å­˜
    final_df.to_csv(out_file, index=False, encoding="utf-8-sig")
    
    console.print(f"[green]âœ“ {trading_date}: å·²ä¿å­˜ â†’ {out_file.name}[/green]")
    console.print(f"  ğŸ“Š è‚¡ç¥¨: {final_df['ticker'].nunique()}, è®¢å•: {final_df['äº¤æ˜“æ‰€å§”æ‰˜å·'].nunique():,}, æ€»è¡Œæ•°: {len(final_df):,}")
    
    return True

def validate_data_structure(base_dir: Path) -> bool:
    """éªŒè¯æ•°æ®ç›®å½•ç»“æ„"""
    if not base_dir.exists():
        console.print(f"[red]é”™è¯¯: æ ¹ç›®å½•ä¸å­˜åœ¨ {base_dir}[/red]")
        return False
    
    date_dirs = [p for p in base_dir.iterdir() if p.is_dir() and re.fullmatch(r"\d{8}", p.name)]
    if not date_dirs:
        console.print(f"[red]é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æ—¥æœŸç›®å½• (YYYYMMDD) in {base_dir}[/red]")
        return False
    
    console.print(f"[green]âœ“ å‘ç° {len(date_dirs)} ä¸ªæ—¥æœŸç›®å½•[/green]")
    return True

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="åŸå§‹æ•°æ®å¤„ç†æ¨¡å— - åˆå¹¶å§”æ‰˜äº‹ä»¶æµ")
    parser.add_argument("--root", required=True, help="base_data æ ¹ç›®å½•")
    parser.add_argument("--tickers", nargs="*", default=None, help="è‚¡ç¥¨ç™½åå•ï¼Œç•™ç©º=å…¨éƒ¨")
    parser.add_argument("--output", help="è¾“å‡ºæ ¹ç›®å½•ï¼Œé»˜è®¤ä¸º {root}/../event_stream")
    parser.add_argument("--dates", nargs="*", help="æŒ‡å®šå¤„ç†æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD")
    
    args = parser.parse_args()

    base = Path(args.root)
    if not validate_data_structure(base):
        sys.exit(1)

    watch_set = set(args.tickers) if args.tickers else set()
    out_root = Path(args.output) if args.output else base.parent / "event_stream"
    out_root.mkdir(exist_ok=True)

    console.print(f"\n[bold green]ğŸš€ åŸå§‹æ•°æ®å¤„ç†æ¨¡å—[/bold green]")
    console.print(f"[dim]è¾“å…¥ç›®å½•: {base}[/dim]")
    console.print(f"[dim]è¾“å‡ºç›®å½•: {out_root}[/dim]")
    console.print(f"[dim]è‚¡ç¥¨ç­›é€‰: {list(watch_set) if watch_set else 'å…¨éƒ¨'}[/dim]")

    # è·å–æ—¥æœŸç›®å½•
    all_date_dirs = [p for p in base.iterdir() if p.is_dir() and re.fullmatch(r"\d{8}", p.name)]
    
    if args.dates:
        # ç­›é€‰æŒ‡å®šæ—¥æœŸ
        target_dates = set(args.dates)
        date_dirs = [d for d in all_date_dirs if d.name in target_dates]
        missing_dates = target_dates - {d.name for d in date_dirs}
        if missing_dates:
            console.print(f"[yellow]è­¦å‘Š: æœªæ‰¾åˆ°æ—¥æœŸç›®å½• {missing_dates}[/yellow]")
    else:
        date_dirs = all_date_dirs

    if not date_dirs:
        console.print(f"[red]é”™è¯¯: æ— å¯å¤„ç†çš„æ—¥æœŸç›®å½•[/red]")
        sys.exit(1)

    console.print(f"[dim]å¤„ç†æ—¥æœŸ: {len(date_dirs)} ä¸ª[/dim]\n")

    # å¤„ç†æ•°æ®
    successful_days = 0
    for d in sorted(date_dirs):
        if merge_one_day(d, watch_set, out_root):
            successful_days += 1

    # æ€»ç»“
    console.print(f"\n[bold cyan]ğŸ“Š å¤„ç†å®Œæˆ[/bold cyan]")
    console.print(f"æˆåŠŸå¤„ç†: {successful_days}/{len(date_dirs)} å¤©")
    console.print(f"è¾“å‡ºç›®å½•: {out_root}")
    
    if successful_days == len(date_dirs):
        console.print(f"[bold green]âœ… æ‰€æœ‰æ•°æ®å¤„ç†æˆåŠŸï¼[/bold green]")
    else:
        console.print(f"[bold yellow]âš ï¸ éƒ¨åˆ†æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯[/bold yellow]")

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹:

# å¤„ç†æ‰€æœ‰æ—¥æœŸã€æŒ‡å®šè‚¡ç¥¨
python scripts/data_process/raw_data/merge_event_stream.py \
    --root "/home/ma-user/code/fenglang/Spoofing Detect/data/base_data" \
    --tickers 000989.SZ 300233.SZ

# å¤„ç†æŒ‡å®šæ—¥æœŸ
python scripts/data_process/raw_data/merge_event_stream.py \
    --root "/path/to/base_data" \
    --dates 20230301 20230302 \
    --tickers 000001.SZ

# è‡ªå®šä¹‰è¾“å‡ºç›®å½•
python scripts/data_process/raw_data/merge_event_stream.py \
    --root "/path/to/base_data" \
    --output "/path/to/custom_output"
""" 