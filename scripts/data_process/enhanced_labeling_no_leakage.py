#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Enhanced Labeling System (No Data Leakage)
------------------------------------------
åŸºäºå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ä¿¡æ¯çš„å¢å¼ºæ ‡ç­¾ç³»ç»Ÿ
â€¢ åªä½¿ç”¨å§”æ‰˜æäº¤æ—¶åˆ»å·²çŸ¥çš„ä¿¡æ¯
â€¢ æ„é€ å¤šç§è™šå‡æŠ¥å•æ£€æµ‹è§„åˆ™
â€¢ æ‰©å¤§æ­£æ ·æœ¬é›†åˆï¼Œé™ä½é¢„æµ‹éš¾åº¦
â€¢ æä¾›æ›´ä¸°å¯Œçš„æ¬ºè¯ˆæ¨¡å¼è¯†åˆ«
"""

import pandas as pd
import numpy as np
from pathlib import Path
import argparse
import warnings
warnings.filterwarnings('ignore')

def detect_suspicious_price_patterns(df: pd.DataFrame) -> pd.DataFrame:
    """æ£€æµ‹å¯ç–‘ä»·æ ¼æ¨¡å¼ï¼ˆåŸºäºå§”æ‰˜æ—¶åˆ»ä¿¡æ¯ï¼‰"""
    print("ğŸ” Detecting suspicious price patterns...")
    
    # è§„åˆ™1: æç«¯ä»·æ ¼åç¦» - è¿œç¦»å¸‚åœºä»·æ ¼çš„å¤§å•
    df['extreme_price_deviation'] = (
        (df['price_dev_prevclose'].abs() > 0.05) &  # åç¦»å‰æ”¶ç›˜ä»·>5%
        (df['pct_spread'] > 5.0) &  # è¿œç¦»ä¹°å–ä»·å·®
        (df['log_qty'] > df.groupby('ticker')['log_qty'].transform('quantile', 0.8))  # å¤§å•
    ).astype(int)
    
    # è§„åˆ™2: ä»·æ ¼æ¿€è¿›æ€§ - ç©¿è¶Šä¹°å–ä»·å·®çš„è®¢å•
    df['aggressive_pricing'] = (
        ((df['is_buy'] == 1) & (df['delta_mid'] > df['spread'] * 0.5)) |  # ä¹°å•ä»·æ ¼è¿‡é«˜
        ((df['is_buy'] == 0) & (df['delta_mid'] < -df['spread'] * 0.5))   # å–å•ä»·æ ¼è¿‡ä½
    ).astype(int)
    
    # è§„åˆ™3: å¼‚å¸¸ä»·æ ¼å±‚çº§ - ç²¾ç¡®åœ¨ä¹°ä¸€å–ä¸€ä»·ä½çš„å¤§å•
    df['at_touch_large_order'] = (
        (((df['å§”æ‰˜ä»·æ ¼'] == df['bid1']) & (df['æ–¹å‘_å§”æ‰˜'] == 'ä¹°')) |
         ((df['å§”æ‰˜ä»·æ ¼'] == df['ask1']) & (df['æ–¹å‘_å§”æ‰˜'] == 'å–'))) &
        (df['å§”æ‰˜æ•°é‡'] > df.groupby('ticker')['å§”æ‰˜æ•°é‡'].transform('quantile', 0.9))
    ).astype(int)
    
    return df

def detect_timing_patterns(df: pd.DataFrame) -> pd.DataFrame:
    """æ£€æµ‹æ—¶é—´æ¨¡å¼å¼‚å¸¸ï¼ˆåŸºäºå§”æ‰˜æ—¶åˆ»ä¿¡æ¯ï¼‰"""
    print("â° Detecting timing patterns...")
    
    # è§„åˆ™4: å¸‚åœºæ´»è·ƒæ—¶æ®µå¼‚å¸¸ - å¼€ç›˜/æ”¶ç›˜æ—¶æ®µçš„å¼‚å¸¸è¡Œä¸º
    market_open = ((df['å§”æ‰˜_datetime'].dt.time >= pd.to_datetime('09:30').time()) & 
                   (df['å§”æ‰˜_datetime'].dt.time <= pd.to_datetime('09:45').time()))
    market_close = ((df['å§”æ‰˜_datetime'].dt.time >= pd.to_datetime('14:45').time()) & 
                    (df['å§”æ‰˜_datetime'].dt.time <= pd.to_datetime('15:00').time()))
    
    df['volatile_period_anomaly'] = (
        (market_open | market_close) &
        (df['å§”æ‰˜æ•°é‡'] > df.groupby('ticker')['å§”æ‰˜æ•°é‡'].transform('median') * 3) &
        (df['pct_spread'] > 2.0)
    ).astype(int)
    
    # è§„åˆ™5: é›†ä¸­ä¸‹å•æ¨¡å¼ - çŸ­æ—¶é—´å†…å¤§é‡è®¢å•
    df['burst_order_pattern'] = (
        (df['orders_100ms'] > 5) &  # 100mså†…è¶…è¿‡5ç¬”è®¢å•
        (df['å§”æ‰˜æ•°é‡'] > df.groupby('ticker')['å§”æ‰˜æ•°é‡'].transform('quantile', 0.7))
    ).astype(int)
    
    return df

def detect_quantity_patterns(df: pd.DataFrame) -> pd.DataFrame:
    """æ£€æµ‹æ•°é‡æ¨¡å¼å¼‚å¸¸ï¼ˆåŸºäºå§”æ‰˜æ—¶åˆ»ä¿¡æ¯ï¼‰"""
    print("ğŸ“Š Detecting quantity patterns...")
    
    # è§„åˆ™6: å¼‚å¸¸å¤§å• - è¿œè¶…æ­£å¸¸äº¤æ˜“è§„æ¨¡
    df['abnormal_large_order'] = (
        df['å§”æ‰˜æ•°é‡'] > df.groupby('ticker')['å§”æ‰˜æ•°é‡'].transform('quantile', 0.95)
    ).astype(int)
    
    # è§„åˆ™7: æ•´æ•°å€å¼‚å¸¸ - å¯ç–‘çš„æ•´æ•°å€æ•°é‡
    df['round_number_pattern'] = (
        ((df['å§”æ‰˜æ•°é‡'] % 10000 == 0) & (df['å§”æ‰˜æ•°é‡'] >= 50000)) |  # å¤§é¢æ•´ä¸‡
        ((df['å§”æ‰˜æ•°é‡'] % 1000 == 0) & (df['å§”æ‰˜æ•°é‡'] >= 10000))    # ä¸­é¢æ•´åƒ
    ).astype(int)
    
    return df

def detect_market_microstructure_patterns(df: pd.DataFrame) -> pd.DataFrame:
    """æ£€æµ‹å¸‚åœºå¾®è§‚ç»“æ„å¼‚å¸¸ï¼ˆåŸºäºå§”æ‰˜æ—¶åˆ»ä¿¡æ¯ï¼‰"""
    print("ğŸ—ï¸ Detecting market microstructure patterns...")
    
    # è§„åˆ™8: è®¢å•ç°¿ä¸å¹³è¡¡åˆ©ç”¨ - åˆ©ç”¨ä¹°å–å¤±è¡¡çš„è®¢å•
    df['imbalance_exploitation'] = (
        (df['book_imbalance'].abs() > 0.5) &  # å¼ºçƒˆçš„ä¹°å–ä¸å¹³è¡¡
        (((df['book_imbalance'] > 0) & (df['æ–¹å‘_å§”æ‰˜'] == 'å–')) |  # ä¹°ç›˜å¼ºåŠ¿æ—¶å–å‡º
         ((df['book_imbalance'] < 0) & (df['æ–¹å‘_å§”æ‰˜'] == 'ä¹°'))) &  # å–ç›˜å¼ºåŠ¿æ—¶ä¹°å…¥
        (df['å§”æ‰˜æ•°é‡'] > df.groupby('ticker')['å§”æ‰˜æ•°é‡'].transform('median') * 2)
    ).astype(int)
    
    # è§„åˆ™9: ä»·å·®æ“çºµè¿¹è±¡ - å¯èƒ½å½±å“ä»·å·®çš„è®¢å•
    df['spread_manipulation_signal'] = (
        (df['spread'] > 0) &
        (df['å§”æ‰˜æ•°é‡'] > df.groupby('ticker')['å§”æ‰˜æ•°é‡'].transform('quantile', 0.8)) &
        (((df['æ–¹å‘_å§”æ‰˜'] == 'ä¹°') & (df['å§”æ‰˜ä»·æ ¼'] >= df['ask1'] - df['spread'] * 0.1)) |
         ((df['æ–¹å‘_å§”æ‰˜'] == 'å–') & (df['å§”æ‰˜ä»·æ ¼'] <= df['bid1'] + df['spread'] * 0.1)))
    ).astype(int)
    
    return df

def detect_behavioral_patterns(df: pd.DataFrame) -> pd.DataFrame:
    """æ£€æµ‹è¡Œä¸ºæ¨¡å¼å¼‚å¸¸ï¼ˆåŸºäºå†å²è§‚æµ‹ä¿¡æ¯ï¼‰"""
    print("ğŸ­ Detecting behavioral patterns...")
    
    # è§„åˆ™10: é¢‘ç¹æ’¤å•å†å² - åŸºäºè¿‡å»çš„æ’¤å•è¡Œä¸º
    df['frequent_canceller'] = (
        df['cancels_5s'] > 3  # è¿‡å»5ç§’å†…æ’¤å•è¶…è¿‡3æ¬¡
    ).astype(int)
    
    # è§„åˆ™11: æ‹æ¿æ„æ„¿å¼‚å¸¸ - ä»·æ ¼æ¿€è¿›ä½†å¯èƒ½æ˜¯è™šå‡æ„å›¾
    df['false_aggressive_intent'] = (
        (df['price_aggressiveness'] > 1.0) &  # ä»·æ ¼æ¿€è¿›
        (df['å§”æ‰˜æ•°é‡'] > df.groupby('ticker')['å§”æ‰˜æ•°é‡'].transform('quantile', 0.75)) &
        (df['orders_100ms'] > 2)  # çŸ­æ—¶é—´å†…å¤šç¬”è®¢å•
    ).astype(int)
    
    return df

def create_enhanced_labels(df: pd.DataFrame) -> pd.DataFrame:
    """åˆ›å»ºç»¼åˆçš„å¢å¼ºæ ‡ç­¾"""
    print("\nğŸ·ï¸ Creating enhanced spoofing labels...")
    
    # åº”ç”¨æ‰€æœ‰æ£€æµ‹è§„åˆ™
    df = detect_suspicious_price_patterns(df)
    df = detect_timing_patterns(df)  
    df = detect_quantity_patterns(df)
    df = detect_market_microstructure_patterns(df)
    df = detect_behavioral_patterns(df)
    
    # å•é¡¹è§„åˆ™æ ‡ç­¾
    pattern_rules = [
        'extreme_price_deviation', 'aggressive_pricing', 'at_touch_large_order',
        'volatile_period_anomaly', 'burst_order_pattern', 'abnormal_large_order', 
        'round_number_pattern', 'imbalance_exploitation', 'spread_manipulation_signal',
        'frequent_canceller', 'false_aggressive_intent'
    ]
    
    # ç»¼åˆæ ‡ç­¾1ï¼šä»»æ„è§„åˆ™è§¦å‘ï¼ˆå®½æ¾ï¼‰
    df['enhanced_spoofing_liberal'] = (
        df[pattern_rules].sum(axis=1) >= 1
    ).astype(int)
    
    # ç»¼åˆæ ‡ç­¾2ï¼šå¤šä¸ªè§„åˆ™è§¦å‘ï¼ˆä¸­ç­‰ï¼‰
    df['enhanced_spoofing_moderate'] = (
        df[pattern_rules].sum(axis=1) >= 2
    ).astype(int)
    
    # ç»¼åˆæ ‡ç­¾3ï¼šä¸¥æ ¼è§„åˆ™ï¼ˆä¿å®ˆï¼‰
    df['enhanced_spoofing_strict'] = (
        df[pattern_rules].sum(axis=1) >= 3
    ).astype(int)
    
    # åˆ†ç±»æ ‡ç­¾ï¼šåŸºäºæœ€å¼ºä¿¡å·
    max_signals = df[pattern_rules].sum(axis=1)
    df['pattern_strength'] = max_signals
    
    # é«˜è´¨é‡æ ‡ç­¾ï¼šç»“åˆåŸå§‹è§„åˆ™ + å¢å¼ºæ¨¡å¼
    if 'y_label' in df.columns:
        df['enhanced_combined'] = (
            (df['y_label'] == 1) |  # åŸå§‹è§„åˆ™
            (df['enhanced_spoofing_moderate'] == 1)  # æˆ–å¢å¼ºè§„åˆ™
        ).astype(int)
    else:
        df['enhanced_combined'] = df['enhanced_spoofing_moderate']
    
    return df

def analyze_enhanced_labels(df: pd.DataFrame):
    """åˆ†æå¢å¼ºæ ‡ç­¾è´¨é‡"""
    print("\nğŸ“Š Enhanced Label Analysis:")
    
    label_cols = [col for col in df.columns if 'enhanced' in col or 'pattern' in col or 
                  col in ['extreme_price_deviation', 'aggressive_pricing', 'at_touch_large_order',
                          'volatile_period_anomaly', 'burst_order_pattern', 'abnormal_large_order']]
    
    for col in label_cols:
        if col in df.columns:
            pos_count = df[col].sum()
            pos_rate = pos_count / len(df) * 100 if len(df) > 0 else 0
            print(f"  {col:<30}: {pos_count:>8,} ({pos_rate:>6.3f}%)")
    
    # å¯¹æ¯”åŸå§‹æ ‡ç­¾
    if 'y_label' in df.columns:
        original_pos = df['y_label'].sum()
        enhanced_pos = df['enhanced_combined'].sum() if 'enhanced_combined' in df.columns else 0
        print(f"\nğŸ“ˆ Label Comparison:")
        print(f"  Original y_label:           {original_pos:>8,} ({original_pos/len(df)*100:>6.3f}%)")
        print(f"  Enhanced combined:          {enhanced_pos:>8,} ({enhanced_pos/len(df)*100:>6.3f}%)")
        if original_pos > 0:
            improvement = (enhanced_pos - original_pos) / original_pos * 100
            print(f"  Improvement:                {enhanced_pos-original_pos:>8,} ({improvement:>+6.1f}%)")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_root", required=True, help="æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--output_suffix", default="_enhanced", help="è¾“å‡ºæ–‡ä»¶åç¼€")
    parser.add_argument("--train_regex", default="202503|202504", help="è®­ç»ƒæ•°æ®æ—¥æœŸæ­£åˆ™")
    parser.add_argument("--valid_regex", default="202505", help="éªŒè¯æ•°æ®æ—¥æœŸæ­£åˆ™")
    
    args = parser.parse_args()
    
    print("ğŸš€ Enhanced Labeling System (No Data Leakage)")
    print("=" * 60)
    
    # è¯»å–ç‰¹å¾æ•°æ®
    feat_dir = Path(args.data_root) / "features_select"
    feat_files = list(feat_dir.glob("X_*.parquet"))
    
    if not feat_files:
        print("âŒ No feature files found")
        return
    
    print(f"ğŸ“ Found {len(feat_files)} feature files")
    
    # è¯»å–æ ‡ç­¾æ•°æ®  
    label_dir = Path(args.data_root) / "labels_select"
    label_files = list(label_dir.glob("labels_*.parquet"))
    
    print(f"ğŸ“ Found {len(label_files)} label files")
    
    # åˆå¹¶æ•°æ®
    df_features = pd.concat([pd.read_parquet(f) for f in feat_files], ignore_index=True)
    df_labels = pd.concat([pd.read_parquet(f) for f in label_files], ignore_index=True)
    
    print(f"ğŸ“Š Features shape: {df_features.shape}")
    print(f"ğŸ“Š Labels shape: {df_labels.shape}")
    
    # åˆå¹¶
    df = df_features.merge(df_labels, on=['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·'], how='inner')
    print(f"ğŸ“Š Merged shape: {df.shape}")
    
    # æ•°æ®é¢„å¤„ç†
    df['å§”æ‰˜_datetime'] = pd.to_datetime(df['è‡ªç„¶æ—¥'].astype(str).str[:8], format='%Y%m%d')
    
    # å¡«å……ç¼ºå¤±å€¼
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df[numeric_cols] = df[numeric_cols].fillna(0)
    
    # åˆ›å»ºå¢å¼ºæ ‡ç­¾
    df = create_enhanced_labels(df)
    
    # åˆ†æç»“æœ
    analyze_enhanced_labels(df)
    
    # ä¿å­˜å¢å¼ºæ ‡ç­¾ç»“æœ
    output_dir = Path(args.data_root) / f"labels_enhanced{args.output_suffix}"
    output_dir.mkdir(exist_ok=True)
    
    # æŒ‰æ—¥æœŸä¿å­˜
    label_cols = ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·', 'y_label'] + [
        col for col in df.columns if 'enhanced' in col or 'pattern' in col or
        col in ['extreme_price_deviation', 'aggressive_pricing', 'at_touch_large_order',
                'volatile_period_anomaly', 'burst_order_pattern', 'abnormal_large_order',
                'round_number_pattern', 'imbalance_exploitation', 'spread_manipulation_signal',
                'frequent_canceller', 'false_aggressive_intent']
    ]
    
    label_cols = [col for col in label_cols if col in df.columns]
    
    for date in df['è‡ªç„¶æ—¥'].unique():
        date_data = df[df['è‡ªç„¶æ—¥'] == date][label_cols]
        output_file = output_dir / f"enhanced_labels_{date}.parquet"
        date_data.to_parquet(output_file, index=False)
        print(f"ğŸ’¾ Saved: {output_file}")
    
    print(f"\nâœ… Enhanced labeling completed!")
    print(f"ğŸ“ Results saved to: {output_dir}")

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# åˆ›å»ºå¢å¼ºæ ‡ç­¾
python scripts/data_process/enhanced_labeling_no_leakage.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data"

# åˆ›å»ºæ›´ä¸¥æ ¼çš„å¢å¼ºæ ‡ç­¾
python scripts/data_process/enhanced_labeling_no_leakage.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --output_suffix "_strict"
""" 