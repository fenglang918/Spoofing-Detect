#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
merge_order_trade.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
åŠŸèƒ½ï¼š
1. å°† <é€ç¬”å§”æ‰˜.csv> ä¸ <é€ç¬”æˆäº¤.csv> åˆå¹¶ç”Ÿæˆ "å§”æ‰˜äº‹ä»¶æµ.csv"
2. å…ˆå°è¯• utf-8-sig â†’ gbk â†’ latin1 è‡ªåŠ¨è¯†åˆ«ç¼–ç 
3. å¯æŒ‡å®š watch_listï¼›ç•™ç©º=å…¨éƒ¨è‚¡ç¥¨
4. æ‰¹é‡å¤„ç† base_data/ä¸‹æ‰€æœ‰æ—¥æœŸ

ç›®å½•é¢„æœŸï¼š
base_data/
  20250303/20250303/000989.SZ/é€ç¬”å§”æ‰˜.csv
                                 é€ç¬”æˆäº¤.csv
  20250304/20250304/300233.SZ/...
  ...

è¿è¡Œç¤ºä¾‹ï¼ˆå…¨éƒ¨æ—¥æœŸã€ä¸¤åªè‚¡ç¥¨ï¼‰:
    python merge_order_trade.py \
        --root "/obs/users/fenglang/general/Spoofing Detect/data/base_data" \
        --tickers 000989.SZ 300233.SZ
"""

import pandas as pd, numpy as np
from pathlib import Path
from datetime import datetime
import argparse, re, sys

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def read_csv_auto(path: Path, **kw) -> pd.DataFrame:
    """è‡ªåŠ¨å°è¯• utf-8-sig / gbk / latin1 è¯»å– CSV"""
    for enc in ("utf-8-sig", "gbk", "latin1"):
        try:
            return pd.read_csv(path, encoding=enc, low_memory=False, **kw)
        except UnicodeDecodeError:
            continue
    raise UnicodeDecodeError(f"æ— æ³•è§£ç : {path}")

def prepare_quotes(quote_csv: Path) -> pd.DataFrame:
    """è¯»å–è¡Œæƒ….csv â†’ 100 ms ç½‘æ ¼ â†’ è¿”å›å« quote_dt çš„ DataFrame"""
    quotes = pd.read_csv(
        quote_csv, encoding="gbk",
        usecols=["è‡ªç„¶æ—¥","æ—¶é—´","ç”³ä¹°ä»·1","ç”³å–ä»·1","ç”³ä¹°é‡1","ç”³å–é‡1","å‰æ”¶ç›˜"]
    )
    quotes["quote_dt"] = pd.to_datetime(
        quotes["è‡ªç„¶æ—¥"].astype(str) + quotes["æ—¶é—´"].astype(str).str.zfill(9),
        format="%Y%m%d%H%M%S%f"
    )
    quotes = (
        quotes.drop_duplicates("quote_dt")           # å»é‡
              .set_index("quote_dt")
              .resample("100ms")                     # â€”â€” â‘  å…³é”®ï¼š100 ms ç½‘æ ¼
              .ffill()                               # â€”â€” â‘¡ å‰å‘å¡«å……
              .reset_index()
    )
    return quotes

def merge_one_stock(order_csv: Path, trade_csv: Path) -> pd.DataFrame:
    """å•æ”¯è‚¡ç¥¨ï¼šé€ç¬”å§”æ‰˜ + é€ç¬”æˆäº¤ => äº‹ä»¶æµï¼ˆå«å­˜æ´»æ—¶é—´ï¼‰"""
    orders = read_csv_auto(order_csv, dtype=str)
    trades = read_csv_auto(trade_csv, dtype=str)

    # å§”æ‰˜è¡¨é¢„å¤„ç†
    orders = orders.rename(columns={"å§”æ‰˜ä»£ç ": "æ–¹å‘_å§”æ‰˜"})
    orders["äº¤æ˜“æ‰€å§”æ‰˜å·"] = orders["äº¤æ˜“æ‰€å§”æ‰˜å·"].str.lstrip("0")

    # æˆäº¤è¡¨æ‹†ä¹°å–
    buy_cols  = ["è‡ªç„¶æ—¥","æ—¶é—´","æˆäº¤ä»£ç ","æˆäº¤ä»·æ ¼","æˆäº¤æ•°é‡","å«ä¹°åºå·"]
    sell_cols = ["è‡ªç„¶æ—¥","æ—¶é—´","æˆäº¤ä»£ç ","æˆäº¤ä»·æ ¼","æˆäº¤æ•°é‡","å«å–åºå·"]
    buy  = trades[trades["å«ä¹°åºå·"]  != "0"][buy_cols].copy()
    sell = trades[trades["å«å–åºå·"] != "0"][sell_cols].copy()
    buy .rename(columns={"å«ä¹°åºå·" :"äº¤æ˜“æ‰€å§”æ‰˜å·"}, inplace=True)
    sell.rename(columns={"å«å–åºå·":"äº¤æ˜“æ‰€å§”æ‰˜å·"}, inplace=True)
    buy ["æ–¹å‘_äº‹ä»¶"]  = "B"
    sell["æ–¹å‘_äº‹ä»¶"] = "S"
    events = pd.concat([buy, sell], ignore_index=True)
    events["äº‹ä»¶ç±»å‹"] = np.where(events["æˆäº¤ä»£ç "] == "C", "æ’¤å•", "æˆäº¤")
    events["äº¤æ˜“æ‰€å§”æ‰˜å·"] = events["äº¤æ˜“æ‰€å§”æ‰˜å·"].str.lstrip("0")

    # 0) åˆå¹¶å§”æ‰˜ + æˆäº¤
    merged = events.merge(
        orders,
        on="äº¤æ˜“æ‰€å§”æ‰˜å·",
        how="left",
        suffixes=("_äº‹ä»¶","_å§”æ‰˜")
    )

    # æ—¶é—´åˆ—
    fmt = "%Y%m%d%H%M%S%f"
    merged["å§”æ‰˜_datetime"] = pd.to_datetime(
        merged["è‡ªç„¶æ—¥_å§”æ‰˜"].str.zfill(8) + merged["æ—¶é—´_å§”æ‰˜"].str.zfill(9),
        errors="coerce", format=fmt)
    merged["äº‹ä»¶_datetime"] = pd.to_datetime(
        merged["è‡ªç„¶æ—¥_äº‹ä»¶"].str.zfill(8) + merged["æ—¶é—´_äº‹ä»¶"].str.zfill(9),
        errors="coerce", format=fmt)

    # 1) è¯»å–å¹¶é‡é‡‡æ ·å½“æ—¥è¡Œæƒ…
    quote_csv = order_csv.with_name("è¡Œæƒ….csv")      # ä¸é€ç¬”å§”æ‰˜åŒç›®å½•
    quotes = prepare_quotes(quote_csv)

    # 2) è´´æœ€è¿‘ â‰¤ t çš„ä¹°ä¸€/å–ä¸€
    merged = merged[merged["å§”æ‰˜_datetime"].notna()].copy()
    merged.sort_values("å§”æ‰˜_datetime", inplace=True)
    merged = pd.merge_asof(
        merged, quotes,
        left_on="å§”æ‰˜_datetime", right_on="quote_dt",
        direction="backward",
        tolerance=pd.Timedelta("100ms")              # å®¹å¿ 100 ms
    )

    merged["å­˜æ´»æ—¶é—´_ms"] = (
        merged["äº‹ä»¶_datetime"] - merged["å§”æ‰˜_datetime"]
    ).dt.total_seconds() * 1_000

    merged = merged.loc[:, ~merged.columns.str.contains("^Unnamed")]
    return merged

def merge_one_day(date_dir: Path, watch_set: set, out_root: Path):
    """
    å¤„ç†å•æ—¥ï¼šdate_dir = base_data/YYYYMMDD
    è¾“å‡º event_stream/YYYYMMDD/å§”æ‰˜äº‹ä»¶æµ.csv
    """
    inner = date_dir / date_dir.name
    if not inner.exists():
        inner = date_dir
    dfs = []
    
    # ä»ç›®å½•åæå–æ—¥æœŸ
    trading_date = date_dir.name
    
    for stk_dir in sorted(inner.iterdir()):
        if watch_set and stk_dir.name not in watch_set:
            continue
        o_csv = stk_dir / "é€ç¬”å§”æ‰˜.csv"
        t_csv = stk_dir / "é€ç¬”æˆäº¤.csv"
        if not (o_csv.exists() and t_csv.exists()):
            continue
        df = merge_one_stock(o_csv, t_csv)
        df["ticker"] = stk_dir.name
        
        # æ·»åŠ ç»Ÿä¸€çš„äº¤æ˜“æ—¥æœŸå­—æ®µï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
        df["è‡ªç„¶æ—¥"] = trading_date  # ç»Ÿä¸€çš„æ—¥æœŸå­—æ®µï¼Œç”¨äºåç»­ä¸»é”®
        
        # æ•°æ®è´¨é‡æ£€æŸ¥ï¼šç¡®ä¿æ—¥æœŸä¸€è‡´æ€§
        if "è‡ªç„¶æ—¥_å§”æ‰˜" in df.columns:
            inconsistent_dates = df[df["è‡ªç„¶æ—¥_å§”æ‰˜"] != trading_date]
            if not inconsistent_dates.empty:
                print(f"  âš ï¸  Warning: Found {len(inconsistent_dates)} records with inconsistent dates in {stk_dir.name}")
        
        dfs.append(df)

    if not dfs:
        print(f"  Ã— {date_dir.name}: no target-stock data")
        return

    out_dir = out_root / date_dir.name
    out_dir.mkdir(parents=True, exist_ok=True)
    out_file = out_dir / "å§”æ‰˜äº‹ä»¶æµ.csv"
    
    # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨æ•°æ®
    final_df = pd.concat(dfs, ignore_index=True)
    
    # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œå°†ä¸»é”®å­—æ®µæ”¾åœ¨å‰é¢
    key_cols = ["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"]
    other_cols = [col for col in final_df.columns if col not in key_cols]
    final_df = final_df[key_cols + other_cols]
    
    final_df.to_csv(out_file, index=False, encoding="utf-8-sig")
    print(f"  âœ“ {date_dir.name}: saved â†’ {out_file.name}  ({len(final_df):,} rows)")
    print(f"    ğŸ“Š Stocks: {final_df['ticker'].nunique()}, Orders: {final_df['äº¤æ˜“æ‰€å§”æ‰˜å·'].nunique():,}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--root", required=True,
        help="base_data æ ¹ç›®å½•")
    parser.add_argument("--tickers", nargs="*", default=None,
        help="è‚¡ç¥¨ç™½åå•ï¼Œç•™ç©º=å…¨éƒ¨")
    args = parser.parse_args()

    base = Path(args.root)
    if not base.exists():
        sys.exit(f"[ERROR] æ ¹ç›®å½•ä¸å­˜åœ¨: {base}")
    watch_set = set(args.tickers) if args.tickers else set()

    out_root = base.parent / "event_stream"
    out_root.mkdir(exist_ok=True)

    # æ‰¾åˆ°å½¢å¦‚ 8 ä½æ—¥æœŸçš„å­ç›®å½•
    date_dirs = [p for p in base.iterdir() if p.is_dir() and re.fullmatch(r"\d{8}", p.name)]
    if not date_dirs:
        sys.exit("[ERROR] æœªæ‰¾åˆ°ä»»ä½•æ—¥æœŸç›®å½• (YYYYMMDD)")

    for d in sorted(date_dirs):
        print(f"â–¶ merging {d.name}")
        merge_one_day(d, watch_set, out_root)

if __name__ == "__main__":
    main()


"""
python scripts/data_process/merge_order_trade.py \
  --root "/obs/users/fenglang/general/Spoofing Detect/data/base_data" \
  --tickers 000989.SZ 300233.SZ


"""