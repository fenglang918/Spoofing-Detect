#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
data_integration.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ•°æ®æ•´åˆåˆ†ææ¨¡å— - ç¬¬ä¸‰é˜¶æ®µæ•°æ®æ•´åˆä¸è¯„ä¼°
â€¢ æ•´åˆç‰¹å¾å’Œæ ‡ç­¾æ•°æ®
â€¢ å…¨å±€æ•°æ®è´¨é‡åˆ†æ
â€¢ ç‰¹å¾è´¨é‡è¯„ä¼°å’Œç­›é€‰
â€¢ ç”Ÿæˆè®­ç»ƒå°±ç»ªæ•°æ®
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple
import argparse
import json
import warnings
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich import print as rprint

console = Console()
warnings.filterwarnings('ignore')

class DataIntegrator:
    """æ•°æ®æ•´åˆå™¨"""
    
    def __init__(self, target_col: str = "y_label"):
        """
        åˆå§‹åŒ–æ•°æ®æ•´åˆå™¨
        
        Args:
            target_col: ç›®æ ‡æ ‡ç­¾åˆ—å
        """
        self.target_col = target_col
        self.console = Console()
        
        # å®šä¹‰ä¿¡æ¯æ³„éœ²ç‰¹å¾æ¨¡å¼
        self.leakage_patterns = [
            r'total_.*',           # æ€»é‡ç»Ÿè®¡
            r'num_.*',            # æ•°é‡ç»Ÿè®¡  
            r'final_.*',          # æœ€ç»ˆçŠ¶æ€
            r'.*_survival.*',     # ç”Ÿå­˜æ—¶é—´ç›¸å…³
            r'is_fully_filled',   # æˆäº¤çŠ¶æ€
            r'flag_R[12]',        # ä¸­é—´æ ‡ç­¾å˜é‡
            r'.*_cancel$',        # æ’¤å•ç›¸å…³ï¼ˆä½†ä¿ç•™is_cancel_eventï¼‰
            r'å­˜æ´»æ—¶é—´_ms',       # åŸå§‹å­˜æ´»æ—¶é—´
        ]
        
        # å®šä¹‰å®‰å…¨çš„æ ¸å¿ƒç‰¹å¾ï¼ˆä¸åº”è¢«ç§»é™¤ï¼‰
        self.safe_core_features = {
            # ä¸»é”®å­—æ®µ
            'è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·',
            # ç›˜å£å¿«ç…§ç‰¹å¾
            'bid1', 'ask1', 'mid_price', 'spread', 'prev_close', 
            'bid_vol1', 'ask_vol1', 'pct_spread',
            # è®¢å•åŸºç¡€ç‰¹å¾
            'log_qty', 'is_buy', 'delta_mid', 'price_dev_prevclose_bps',
            # çŸ­æœŸå†å²çª—å£ç‰¹å¾
            'orders_100ms', 'orders_1s', 'cancels_100ms', 'cancels_1s', 'cancels_5s',
            'cancel_ratio_100ms', 'cancel_ratio_1s', 'trades_1s',
            # æ—¶é—´å’Œå¸‚åœºçŠ¶æ€ç‰¹å¾
            'time_sin', 'time_cos', 'in_auction',
            # è®¢å•ç°¿å‹åŠ›ç‰¹å¾
            'book_imbalance', 'price_aggressiveness', 'cluster_score',
            # äº‹ä»¶ç±»å‹
            'is_cancel_event'
        }
    
    def load_and_integrate_data(self, 
                               features_dir: Path, 
                               labels_dir: Path,
                               feature_pattern: str = "X_*.parquet",
                               label_pattern: str = "labels_*.parquet",
                               dates: Optional[List[str]] = None) -> pd.DataFrame:
        """
        åŠ è½½å¹¶æ•´åˆç‰¹å¾å’Œæ ‡ç­¾æ•°æ®
        
        Args:
            features_dir: ç‰¹å¾æ•°æ®ç›®å½•
            labels_dir: æ ‡ç­¾æ•°æ®ç›®å½•
            feature_pattern: ç‰¹å¾æ–‡ä»¶æ¨¡å¼
            label_pattern: æ ‡ç­¾æ–‡ä»¶æ¨¡å¼
            dates: æŒ‡å®šå¤„ç†çš„æ—¥æœŸåˆ—è¡¨
            
        Returns:
            æ•´åˆåçš„DataFrame
        """
        self.console.print(f"[bold green]ğŸ“‚ åŠ è½½å¹¶æ•´åˆæ•°æ®...[/bold green]")
        
        # æŸ¥æ‰¾ç‰¹å¾æ–‡ä»¶
        feat_files = list(features_dir.glob(feature_pattern))
        if not feat_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç‰¹å¾æ–‡ä»¶: {features_dir}/{feature_pattern}")
        
        # æŸ¥æ‰¾æ ‡ç­¾æ–‡ä»¶
        label_files = list(labels_dir.glob(label_pattern))
        if not label_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶: {labels_dir}/{label_pattern}")
        
        # æ—¥æœŸç­›é€‰
        if dates:
            target_dates = set(dates)
            feat_files = [f for f in feat_files if any(d in f.name for d in target_dates)]
            label_files = [f for f in label_files if any(d in f.name for d in target_dates)]
        
        self.console.print(f"æ‰¾åˆ° {len(feat_files)} ä¸ªç‰¹å¾æ–‡ä»¶, {len(label_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶")
        
        # åŠ è½½ç‰¹å¾æ•°æ®
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            console=self.console,
            transient=True
        ) as progress:
            
            # åŠ è½½ç‰¹å¾
            feat_task = progress.add_task("åŠ è½½ç‰¹å¾æ•°æ®...", total=len(feat_files))
            feat_dfs = []
            for f in feat_files:
                try:
                    df = pd.read_parquet(f)
                    feat_dfs.append(df)
                    progress.advance(feat_task)
                except Exception as e:
                    self.console.print(f"[yellow]è­¦å‘Š: åŠ è½½ç‰¹å¾æ–‡ä»¶å¤±è´¥ {f}: {e}[/yellow]")
                    progress.advance(feat_task)
            
            # åŠ è½½æ ‡ç­¾
            label_task = progress.add_task("åŠ è½½æ ‡ç­¾æ•°æ®...", total=len(label_files))
            label_dfs = []
            for f in label_files:
                try:
                    df = pd.read_parquet(f)
                    label_dfs.append(df)
                    progress.advance(label_task)
                except Exception as e:
                    self.console.print(f"[yellow]è­¦å‘Š: åŠ è½½æ ‡ç­¾æ–‡ä»¶å¤±è´¥ {f}: {e}[/yellow]")
                    progress.advance(label_task)
            
            # åˆå¹¶æ•°æ®
            merge_task = progress.add_task("åˆå¹¶æ•°æ®...", total=100)
            
            if not feat_dfs or not label_dfs:
                raise ValueError("æ— æœ‰æ•ˆçš„ç‰¹å¾æˆ–æ ‡ç­¾æ•°æ®")
            
            df_features = pd.concat(feat_dfs, ignore_index=True)
            df_labels = pd.concat(label_dfs, ignore_index=True)
            
            progress.update(merge_task, advance=50)
            
            # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
            df_integrated = df_features.merge(
                df_labels, 
                on=['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·'], 
                how='inner'
            )
            
            progress.update(merge_task, advance=50)
        
        self.console.print(f"[green]âœ… æ•´åˆå®Œæˆ: {df_integrated.shape}[/green]")
        return df_integrated
    
    def analyze_data_quality(self, df: pd.DataFrame) -> Dict:
        """
        å…¨é¢åˆ†ææ•°æ®è´¨é‡
        
        Args:
            df: æ•´åˆåçš„æ•°æ®
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        self.console.print(f"\n[bold cyan]ğŸ” æ•°æ®è´¨é‡åˆ†æ[/bold cyan]")
        
        results = {}
        
        # 1. åŸºç¡€ç»Ÿè®¡
        results['basic_stats'] = self._get_basic_stats(df)
        
        # 2. è¯†åˆ«é—®é¢˜ç‰¹å¾
        results['problematic_features'] = self._identify_problematic_features(df)
        
        # 3. ä¿¡æ¯æ³„éœ²æ£€æµ‹
        results['leakage_features'] = self._detect_information_leakage(df)
        
        # 4. ç‰¹å¾é‡è¦æ€§åˆ†æ
        if self.target_col in df.columns:
            results['feature_importance'] = self._analyze_feature_importance(df)
        
        # 5. æ•°æ®åˆ†å¸ƒåˆ†æ
        results['distribution_analysis'] = self._analyze_distributions(df)
        
        # 6. ç”Ÿæˆè¿‡æ»¤å»ºè®®
        results['filter_recommendations'] = self._generate_filter_recommendations(results)
        
        return results
    
    def _get_basic_stats(self, df: pd.DataFrame) -> Dict:
        """è·å–åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        feature_cols = [col for col in df.columns 
                       if col not in ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·', self.target_col]]
        numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
        
        stats = {
            'total_features': len(feature_cols),
            'numeric_features': len(numeric_cols),
            'categorical_features': len(feature_cols) - len(numeric_cols),
            'total_samples': len(df),
            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
        }
        
        # æŒ‰æ—¥æœŸç»Ÿè®¡
        if 'è‡ªç„¶æ—¥' in df.columns:
            stats['date_distribution'] = df['è‡ªç„¶æ—¥'].value_counts().head(10).to_dict()
            stats['unique_dates'] = df['è‡ªç„¶æ—¥'].nunique()
        
        # æŒ‰è‚¡ç¥¨ç»Ÿè®¡
        if 'ticker' in df.columns:
            stats['ticker_distribution'] = df['ticker'].value_counts().head(10).to_dict()
            stats['unique_tickers'] = df['ticker'].nunique()
        
        # æ ‡ç­¾åˆ†å¸ƒ
        if self.target_col in df.columns:
            stats['label_distribution'] = df[self.target_col].value_counts().to_dict()
            stats['positive_rate'] = df[self.target_col].mean()
        
        return stats
    
    def _identify_problematic_features(self, df: pd.DataFrame) -> Dict:
        """è¯†åˆ«é—®é¢˜ç‰¹å¾"""
        feature_cols = [col for col in df.columns 
                       if col not in ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·', self.target_col]]
        
        problems = {
            'constant_features': [],      # å¸¸æ•°åˆ—
            'low_variance_features': [],  # ä½æ–¹å·®ç‰¹å¾
            'high_missing_features': [],  # é«˜ç¼ºå¤±ç‡ç‰¹å¾
            'duplicate_features': [],     # é‡å¤ç‰¹å¾
            'inf_nan_features': [],       # åŒ…å«æ— ç©·å€¼çš„ç‰¹å¾
            'outlier_features': [],       # å¼‚å¸¸å€¼è¿‡å¤šçš„ç‰¹å¾
        }
        
        for col in feature_cols:
            # å¸¸æ•°åˆ—æ£€æµ‹
            if df[col].nunique() <= 1:
                problems['constant_features'].append(col)
                continue
            
            # é«˜ç¼ºå¤±ç‡æ£€æµ‹
            missing_rate = df[col].isnull().mean()
            if missing_rate > 0.9:
                problems['high_missing_features'].append(col)
                continue
            
            # æ•°å€¼ç‰¹å¾çš„è¿›ä¸€æ­¥æ£€æµ‹
            if df[col].dtype in ['int64', 'float64']:
                # ä½æ–¹å·®æ£€æµ‹
                try:
                    if df[col].var() < 1e-10:
                        problems['low_variance_features'].append(col)
                except:
                    pass
                
                # æ— ç©·å€¼æ£€æµ‹
                if np.isinf(df[col]).any():
                    problems['inf_nan_features'].append(col)
                
                # å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆIQRæ–¹æ³•ï¼‰
                try:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    outliers = ((df[col] < (Q1 - 3 * IQR)) | (df[col] > (Q3 + 3 * IQR))).mean()
                    if outliers > 0.1:  # è¶…è¿‡10%çš„å¼‚å¸¸å€¼
                        problems['outlier_features'].append((col, outliers))
                except:
                    pass
        
        # é‡å¤ç‰¹å¾æ£€æµ‹
        numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            try:
                corr_matrix = df[numeric_cols].corr().abs()
                upper_triangle = corr_matrix.where(
                    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
                )
                
                duplicate_pairs = []
                for col in upper_triangle.columns:
                    high_corr_features = upper_triangle.index[upper_triangle[col] > 0.99].tolist()
                    for feat in high_corr_features:
                        duplicate_pairs.append((feat, col))
                
                # åªä¿ç•™ä¸€ä¸ªï¼Œç§»é™¤å…¶ä»–
                to_remove = set()
                for feat1, feat2 in duplicate_pairs:
                    to_remove.add(feat2)
                
                problems['duplicate_features'] = list(to_remove)
            except:
                pass
        
        return problems
    
    def _detect_information_leakage(self, df: pd.DataFrame) -> Dict:
        """æ£€æµ‹ä¿¡æ¯æ³„éœ²ç‰¹å¾"""
        import re
        
        leakage_info = {
            'pattern_matches': {},
            'suspicious_features': [],
            'high_correlation_features': {}
        }
        
        feature_cols = [col for col in df.columns 
                       if col not in ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·']]
        
        # 1. æ¨¡å¼åŒ¹é…æ£€æµ‹
        for pattern in self.leakage_patterns:
            matches = [col for col in feature_cols if re.match(pattern, col)]
            if matches:
                leakage_info['pattern_matches'][pattern] = matches
        
        # 2. ä¸ç›®æ ‡å˜é‡çš„å¼‚å¸¸ç›¸å…³æ€§æ£€æµ‹
        if self.target_col in df.columns:
            numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if col != self.target_col:
                    try:
                        corr = df[col].corr(df[self.target_col])
                        if abs(corr) > 0.8:  # å¼‚å¸¸é«˜çš„ç›¸å…³æ€§
                            leakage_info['high_correlation_features'][col] = corr
                    except:
                        pass
        
        # 3. å·²çŸ¥çš„ä¿¡æ¯æ³„éœ²ç‰¹å¾
        known_leakage = [
            'total_events', 'total_traded_qty', 'num_trades', 'num_cancels',
            'final_survival_time_ms', 'is_fully_filled', 'survival_time_ms',
            'final_state', 'is_cancel', 'å­˜æ´»æ—¶é—´_ms'
        ]
        
        leakage_info['suspicious_features'] = [
            col for col in feature_cols if col in known_leakage
        ]
        
        return leakage_info
    
    def _analyze_feature_importance(self, df: pd.DataFrame) -> Dict:
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        feature_cols = [col for col in df.columns 
                       if col not in ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·', self.target_col]]
        numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
        
        importance_info = {
            'correlations': {},
            'mutual_information': {},
            'statistical_tests': {}
        }
        
        if len(numeric_cols) == 0:
            return importance_info
        
        # 1. ç›¸å…³æ€§åˆ†æ
        for col in numeric_cols:
            try:
                corr = df[col].corr(df[self.target_col])
                if not np.isnan(corr):
                    importance_info['correlations'][col] = abs(corr)
            except:
                pass
        
        # 2. ç»Ÿè®¡æ£€éªŒï¼ˆå¡æ–¹æ£€éªŒã€tæ£€éªŒç­‰ï¼‰
        try:
            from scipy.stats import chi2_contingency, ttest_ind
            
            for col in numeric_cols[:20]:  # é™åˆ¶æ•°é‡é¿å…è®¡ç®—è¿‡ä¹…
                try:
                    # å¯¹äºè¿ç»­å˜é‡ï¼Œè¿›è¡Œtæ£€éªŒ
                    group1 = df[df[self.target_col] == 1][col].dropna()
                    group0 = df[df[self.target_col] == 0][col].dropna()
                    
                    if len(group1) > 10 and len(group0) > 10:
                        _, p_value = ttest_ind(group1, group0)
                        importance_info['statistical_tests'][col] = 1 - p_value  # è½¬æ¢ä¸ºé‡è¦æ€§åˆ†æ•°
                except:
                    pass
        except ImportError:
            pass
        
        return importance_info
    
    def _analyze_distributions(self, df: pd.DataFrame) -> Dict:
        """åˆ†ææ•°æ®åˆ†å¸ƒ"""
        distribution_info = {
            'skewness': {},
            'kurtosis': {},
            'zero_ratios': {},
            'unique_ratios': {}
        }
        
        feature_cols = [col for col in df.columns 
                       if col not in ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·', self.target_col]]
        numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            try:
                # ååº¦
                distribution_info['skewness'][col] = df[col].skew()
                
                # å³°åº¦
                distribution_info['kurtosis'][col] = df[col].kurtosis()
                
                # é›¶å€¼æ¯”ä¾‹
                distribution_info['zero_ratios'][col] = (df[col] == 0).mean()
                
                # å”¯ä¸€å€¼æ¯”ä¾‹
                distribution_info['unique_ratios'][col] = df[col].nunique() / len(df)
                
            except:
                pass
        
        return distribution_info
    
    def _generate_filter_recommendations(self, analysis_results: Dict) -> Dict:
        """ç”Ÿæˆè¿‡æ»¤å»ºè®®"""
        recommendations = {
            'features_to_remove': set(),
            'features_to_investigate': set(),
            'safe_features': set(),
            'priority_features': set(),
            'reasons': {}
        }
        
        # 1. å¿…é¡»ç§»é™¤çš„ç‰¹å¾
        problems = analysis_results.get('problematic_features', {})
        
        for feat in problems.get('constant_features', []):
            recommendations['features_to_remove'].add(feat)
            recommendations['reasons'][feat] = 'å¸¸æ•°åˆ—'
        
        for feat in problems.get('low_variance_features', []):
            recommendations['features_to_remove'].add(feat)
            recommendations['reasons'][feat] = 'ä½æ–¹å·®'
        
        for feat in problems.get('high_missing_features', []):
            recommendations['features_to_remove'].add(feat)
            recommendations['reasons'][feat] = 'é«˜ç¼ºå¤±ç‡'
        
        for feat in problems.get('duplicate_features', []):
            recommendations['features_to_remove'].add(feat)
            recommendations['reasons'][feat] = 'é‡å¤ç‰¹å¾'
        
        # 2. ä¿¡æ¯æ³„éœ²ç‰¹å¾
        leakage = analysis_results.get('leakage_features', {})
        
        for pattern, matches in leakage.get('pattern_matches', {}).items():
            for feat in matches:
                if feat not in self.safe_core_features:
                    recommendations['features_to_remove'].add(feat)
                    recommendations['reasons'][feat] = f'ç–‘ä¼¼æ³„éœ²(æ¨¡å¼:{pattern})'
        
        for feat in leakage.get('suspicious_features', []):
            if feat not in self.safe_core_features:
                recommendations['features_to_remove'].add(feat)
                recommendations['reasons'][feat] = 'å·²çŸ¥ä¿¡æ¯æ³„éœ²ç‰¹å¾'
        
        for feat, corr in leakage.get('high_correlation_features', {}).items():
            if feat not in self.safe_core_features:
                recommendations['features_to_investigate'].add(feat)
                recommendations['reasons'][feat] = f'ä¸ç›®æ ‡å¼‚å¸¸ç›¸å…³({corr:.3f})'
        
        # 3. å®‰å…¨ç‰¹å¾
        recommendations['safe_features'] = self.safe_core_features.copy()
        
        # 4. ä¼˜å…ˆç‰¹å¾ï¼ˆåŸºäºé‡è¦æ€§ï¼‰
        importance = analysis_results.get('feature_importance', {})
        correlations = importance.get('correlations', {})
        
        if correlations:
            sorted_corr = sorted(correlations.items(), key=lambda x: x[1], reverse=True)
            top_n = min(20, max(5, len(sorted_corr) // 5))
            for feat, _ in sorted_corr[:top_n]:
                if feat not in recommendations['features_to_remove']:
                    recommendations['priority_features'].add(feat)
        
        return recommendations
    
    def apply_filters(self, df: pd.DataFrame, filter_recommendations: Dict) -> pd.DataFrame:
        """åº”ç”¨è¿‡æ»¤å»ºè®®"""
        features_to_remove = filter_recommendations.get('features_to_remove', set())
        
        # ç¡®ä¿ä¸ç§»é™¤ä¸»é”®å’Œç›®æ ‡åˆ—
        protected_cols = {'è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·', self.target_col}
        features_to_remove = features_to_remove - protected_cols
        
        if features_to_remove:
            self.console.print(f"[yellow]ç§»é™¤ {len(features_to_remove)} ä¸ªç‰¹å¾[/yellow]")
            df_filtered = df.drop(columns=list(features_to_remove), errors='ignore')
        else:
            df_filtered = df.copy()
        
        return df_filtered
    
    def print_analysis_report(self, analysis_results: Dict):
        """æ‰“å°åˆ†ææŠ¥å‘Š"""
        self.console.print(f"\n[bold green]ğŸ“‹ æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š[/bold green]")
        self.console.print("=" * 80)
        
        # 1. åŸºç¡€ç»Ÿè®¡
        stats = analysis_results.get('basic_stats', {})
        table = Table(title="æ•°æ®æ¦‚è§ˆ")
        table.add_column("æŒ‡æ ‡", style="cyan")
        table.add_column("æ•°å€¼", style="magenta")
        
        table.add_row("æ€»ç‰¹å¾æ•°", str(stats.get('total_features', 0)))
        table.add_row("æ•°å€¼ç‰¹å¾", str(stats.get('numeric_features', 0)))
        table.add_row("æ€»æ ·æœ¬æ•°", f"{stats.get('total_samples', 0):,}")
        table.add_row("å†…å­˜ä½¿ç”¨", f"{stats.get('memory_usage_mb', 0):.1f} MB")
        table.add_row("æ—¥æœŸæ•°é‡", str(stats.get('unique_dates', 0)))
        table.add_row("è‚¡ç¥¨æ•°é‡", str(stats.get('unique_tickers', 0)))
        
        if 'positive_rate' in stats:
            table.add_row("æ­£æ ·æœ¬æ¯”ä¾‹", f"{stats['positive_rate']*100:.4f}%")
        
        self.console.print(table)
        
        # 2. é—®é¢˜ç‰¹å¾
        problems = analysis_results.get('problematic_features', {})
        if any(problems.values()):
            self.console.print(f"\n[bold red]ğŸš« é—®é¢˜ç‰¹å¾ç»Ÿè®¡[/bold red]")
            for problem_type, features in problems.items():
                if features:
                    if problem_type == 'outlier_features':
                        self.console.print(f"  â€¢ {problem_type}: {len(features)} ä¸ª")
                    else:
                        self.console.print(f"  â€¢ {problem_type}: {len(features)} ä¸ª")
                        if len(features) <= 5:
                            self.console.print(f"    {features}")
                        else:
                            self.console.print(f"    {features[:5]}...")
        
        # 3. ä¿¡æ¯æ³„éœ²
        leakage = analysis_results.get('leakage_features', {})
        total_leakage = (len(leakage.get('suspicious_features', [])) + 
                        sum(len(matches) for matches in leakage.get('pattern_matches', {}).values()))
        
        self.console.print(f"\n[bold yellow]ğŸ” ä¿¡æ¯æ³„éœ²æ£€æµ‹[/bold yellow]")
        self.console.print(f"  â€¢ ç–‘ä¼¼æ³„éœ²ç‰¹å¾: {total_leakage} ä¸ª")
        self.console.print(f"  â€¢ å¼‚å¸¸é«˜ç›¸å…³æ€§: {len(leakage.get('high_correlation_features', {}))} ä¸ª")
        
        # 4. è¿‡æ»¤å»ºè®®
        recommendations = analysis_results.get('filter_recommendations', {})
        if recommendations:
            self.console.print(f"\n[bold cyan]ğŸ’¡ è¿‡æ»¤å»ºè®®[/bold cyan]")
            self.console.print(f"  â€¢ å»ºè®®ç§»é™¤: {len(recommendations.get('features_to_remove', set()))} ä¸ªç‰¹å¾")
            self.console.print(f"  â€¢ éœ€è¦è°ƒæŸ¥: {len(recommendations.get('features_to_investigate', set()))} ä¸ªç‰¹å¾")
            self.console.print(f"  â€¢ ä¼˜å…ˆç‰¹å¾: {len(recommendations.get('priority_features', set()))} ä¸ªç‰¹å¾")
            self.console.print(f"  â€¢ å®‰å…¨ç‰¹å¾: {len(recommendations.get('safe_features', set()))} ä¸ªç‰¹å¾")

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="æ•°æ®æ•´åˆåˆ†ææ¨¡å—")
    parser.add_argument("--features_dir", required=True, help="ç‰¹å¾æ•°æ®ç›®å½•")
    parser.add_argument("--labels_dir", required=True, help="æ ‡ç­¾æ•°æ®ç›®å½•")
    parser.add_argument("--output_dir", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--target_col", default="y_label", help="ç›®æ ‡æ ‡ç­¾åˆ—å")
    parser.add_argument("--dates", nargs="*", help="æŒ‡å®šå¤„ç†æ—¥æœŸ")
    parser.add_argument("--apply_filter", action="store_true", help="åº”ç”¨è¿‡æ»¤å»ºè®®")
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºæ•´åˆå™¨
        integrator = DataIntegrator(target_col=args.target_col)
        
        # åŠ è½½å¹¶æ•´åˆæ•°æ®
        df_integrated = integrator.load_and_integrate_data(
            features_dir=Path(args.features_dir),
            labels_dir=Path(args.labels_dir),
            dates=args.dates
        )
        
        # åˆ†ææ•°æ®è´¨é‡
        analysis_results = integrator.analyze_data_quality(df_integrated)
        
        # æ‰“å°æŠ¥å‘Š
        integrator.print_analysis_report(analysis_results)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜åˆ†æç»“æœ
        with open(output_dir / "data_analysis.json", "w", encoding="utf-8") as f:
            # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
            json_results = {}
            for key, value in analysis_results.items():
                if isinstance(value, dict):
                    json_results[key] = {}
                    for k, v in value.items():
                        if isinstance(v, set):
                            json_results[key][k] = list(v)
                        else:
                            json_results[key][k] = v
                else:
                    json_results[key] = value
            
            json.dump(json_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜åŸå§‹æ•´åˆæ•°æ®
        df_integrated.to_parquet(output_dir / "integrated_data.parquet", index=False)
        console.print(f"[green]ğŸ’¾ åŸå§‹æ•´åˆæ•°æ®å·²ä¿å­˜: {output_dir}/integrated_data.parquet[/green]")
        
        # åº”ç”¨è¿‡æ»¤ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.apply_filter:
            df_filtered = integrator.apply_filters(
                df_integrated, 
                analysis_results.get('filter_recommendations', {})
            )
            
            df_filtered.to_parquet(output_dir / "filtered_data.parquet", index=False)
            console.print(f"[green]ğŸ’¾ è¿‡æ»¤åæ•°æ®å·²ä¿å­˜: {output_dir}/filtered_data.parquet[/green]")
            console.print(f"æ•°æ®å½¢çŠ¶å˜åŒ–: {df_integrated.shape} â†’ {df_filtered.shape}")
        
        console.print(f"[green]ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜: {output_dir}/data_analysis.json[/green]")
        console.print(f"[bold green]âœ… æ•°æ®æ•´åˆåˆ†æå®Œæˆï¼[/bold green]")
        
    except Exception as e:
        console.print(f"[red]âŒ é”™è¯¯: {e}[/red]")
        raise

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹:

# åŸºç¡€æ•´åˆåˆ†æ
python scripts/data_process/analysis/data_integration.py \
    --features_dir "/path/to/features" \
    --labels_dir "/path/to/labels" \
    --output_dir "/path/to/analysis_results"

# æ•´åˆåˆ†æå¹¶åº”ç”¨è¿‡æ»¤
python scripts/data_process/analysis/data_integration.py \
    --features_dir "/path/to/features" \
    --labels_dir "/path/to/labels" \
    --output_dir "/path/to/analysis_results" \
    --apply_filter

# æŒ‡å®šæ—¥æœŸå’Œç›®æ ‡åˆ—
python scripts/data_process/analysis/data_integration.py \
    --features_dir "/path/to/features" \
    --labels_dir "/path/to/labels" \
    --output_dir "/path/to/analysis_results" \
    --dates 20230301 20230302 \
    --target_col "spoofing_label" \
    --apply_filter
""" 