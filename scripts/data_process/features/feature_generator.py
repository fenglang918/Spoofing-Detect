#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
feature_generator.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ç‰¹å¾ç”Ÿæˆæ¨¡å— - ç¬¬äºŒé˜¶æ®µç‰¹å¾è®¡ç®—
â€¢ ä»å§”æ‰˜äº‹ä»¶æµè®¡ç®—å„ç±»ç‰¹å¾
â€¢ æ”¯æŒæ‰¹é‡å¤„ç†å¤šæ—¥æœŸæ•°æ®
â€¢ æ¨¡å—åŒ–ç‰¹å¾è®¡ç®—æµæ°´çº¿
â€¢ ä¿ç•™æ‰€æœ‰è®¡ç®—ç‰¹å¾ï¼ˆä¸è¿‡æ»¤ï¼‰
"""

import sys
import pandas as pd
import polars as pl
from pathlib import Path
from typing import Set, List, Optional, Union
import argparse
import glob
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich import print as rprint

# å¯¼å…¥ç‰¹å¾è®¡ç®—ç»„ä»¶
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆåŒ…æ¨¡å¼ï¼‰
    from .feature_engineering import (
        calc_realtime_features, calculate_enhanced_realtime_features,
        calculate_order_book_pressure, calc_realtime_features_polars,
        calculate_order_book_pressure_polars
    )
    from ..utils import console as utils_console
except ImportError:
    # ç›´æ¥å¯¼å…¥ï¼ˆè„šæœ¬æ¨¡å¼ï¼‰
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent))
    sys.path.append(str(Path(__file__).parent.parent))
    
    from feature_engineering import (
        calc_realtime_features, calculate_enhanced_realtime_features,
        calculate_order_book_pressure, calc_realtime_features_polars,
        calculate_order_book_pressure_polars
    )
    from utils import console as utils_console

# Use imported console or create a new one if import failed
try:
    console = utils_console
except NameError:
    console = Console()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç‰¹å¾åˆ—å®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ä¸»é”®åˆ—ï¼ˆç”¨äºæ•°æ®åˆå¹¶å’Œæ ‡è¯†ï¼‰
KEY_COLUMNS = [
    "è‡ªç„¶æ—¥", 
    "ticker", 
    "äº¤æ˜“æ‰€å§”æ‰˜å·"
]

# æ—¶é—´åˆ—ï¼ˆä¸ç”¨äºè®­ç»ƒï¼Œä½†ç”¨äºæ’åºå’Œçª—å£è®¡ç®—ï¼‰
# æ³¨æ„ï¼šäº‹ä»¶_datetime å±äºæœªæ¥ä¿¡æ¯ï¼Œç§»è‡³æ³„éœ²é£é™©åˆ—
TIME_COLUMNS = [
    "å§”æ‰˜_datetime"
]

# åŸå§‹æ•°æ®åˆ—ï¼ˆä»å§”æ‰˜äº‹ä»¶æµä¸­ç›´æ¥è·å–ï¼Œéƒ¨åˆ†å¯ç”¨äºç‰¹å¾å·¥ç¨‹ï¼‰
# æ³¨æ„ï¼šåˆ é™¤æœ‰æ³„éœ²é£é™©çš„åˆ—
RAW_DATA_COLUMNS = [
    "å§”æ‰˜ä»·æ ¼", "å§”æ‰˜æ•°é‡", "æ–¹å‘_å§”æ‰˜", "å§”æ‰˜ç±»å‹",
    # ç§»é™¤æ³„éœ²é£é™©åˆ—: "äº‹ä»¶ç±»å‹", "å­˜æ´»æ—¶é—´_ms", "æˆäº¤ä»·æ ¼", "æˆäº¤æ•°é‡", "äº‹ä»¶_datetime"
    "ç”³ä¹°ä»·1", "ç”³å–ä»·1", "ç”³ä¹°é‡1", "ç”³å–é‡1", "å‰æ”¶ç›˜",
    "bid1", "ask1", "bid_vol1", "ask_vol1", "prev_close",
    "bid2", "ask2", "bid_vol2", "ask_vol2",
    "bid3", "ask3", "bid_vol3", "ask_vol3",
    "bid4", "ask4", "bid_vol4", "ask_vol4",
    "bid5", "ask5", "bid_vol5", "ask_vol5"
]

# åŸºç¡€ç‰¹å¾åˆ—ï¼ˆç”±ç‰¹å¾å·¥ç¨‹ç”Ÿæˆï¼ŒåŸºäºfeature_engineering.pyä¸­å®é™…ç”Ÿæˆçš„ç‰¹å¾ï¼‰
BASE_FEATURE_COLUMNS = [
    # ä»·æ ¼ç›¸å…³ç‰¹å¾
    "mid_price", "spread", "pct_spread", "delta_mid", "price_dev_prevclose_bps",
    # æ•°é‡å’Œæ–¹å‘ç‰¹å¾
    "log_qty", "is_buy", 
    # æ—¶é—´ç‰¹å¾
    "time_sin", "time_cos", "in_auction", "seconds_since_market_open",
    # æ»šåŠ¨çª—å£ç‰¹å¾ï¼ˆåªä¿ç•™è®¢å•ç»Ÿè®¡ï¼Œç§»é™¤åŸºäºæœªæ¥ä¿¡æ¯çš„æ’¤å•/æˆäº¤ç»Ÿè®¡ï¼‰
    "orders_100ms", "orders_1s",
    # è®¢å•ç°¿ç‰¹å¾
    "book_imbalance", "price_aggressiveness", "cluster_score",
    # ä»·æ ¼ä½ç½®ç‰¹å¾ï¼ˆæ–°å¢çš„å¯è§‚æµ‹ç‰¹å¾ï¼‰
    "at_bid", "at_ask", "inside_spread",
    # å¯¹æ•°å˜æ¢ç‰¹å¾
    "log_order_price", "log_bid1", "log_ask1", "log_bid_vol", "log_ask_vol", "log_order_amount"
]

# æ‰©å±•ç‰¹å¾åˆ—ï¼ˆç”±æ‰©å±•ç‰¹å¾å·¥ç¨‹ç”Ÿæˆï¼ŒåŸºäºcalculate_enhanced_realtime_featureså®é™…ç”Ÿæˆçš„ç‰¹å¾ï¼‰
ENHANCED_FEATURE_COLUMNS = [
    # ç”Ÿå­˜æ—¶é—´ç‰¹å¾
    "z_survival",
    # ä»·æ ¼åŠ¨é‡ç‰¹å¾  
    "price_momentum_100ms", "spread_change",
    # è®¢å•å¯†åº¦ç‰¹å¾
    "order_density"
]

# æ‰€æœ‰ç‰¹å¾åˆ—ï¼ˆå¯ç”¨äºæ¨¡å‹è®­ç»ƒï¼‰
ALL_FEATURE_COLUMNS = BASE_FEATURE_COLUMNS + ENHANCED_FEATURE_COLUMNS

# ä¸èƒ½ç”¨äºè®­ç»ƒçš„åˆ—ï¼ˆæ•°æ®æ³„éœ²é£é™©ï¼‰
LEAKAGE_RISK_COLUMNS = [
    "å­˜æ´»æ—¶é—´_ms",     # æœªæ¥ä¿¡æ¯ï¼šå§”æ‰˜çš„æœ€ç»ˆå­˜æ´»æ—¶é—´
    "äº‹ä»¶_datetime",   # æœªæ¥ä¿¡æ¯ï¼šå§”æ‰˜ç»“æŸæ—¶é—´
    "æˆäº¤ä»·æ ¼",        # æœªæ¥ä¿¡æ¯ï¼šå®é™…æˆäº¤ä»·æ ¼
    "æˆäº¤æ•°é‡",        # æœªæ¥ä¿¡æ¯ï¼šå®é™…æˆäº¤æ•°é‡
    "äº‹ä»¶ç±»å‹",        # æœªæ¥ä¿¡æ¯ï¼šå§”æ‰˜æœ€ç»ˆäº‹ä»¶ç±»å‹ï¼ˆæˆäº¤/æ’¤å•ï¼‰
    # ä¸­é—´è®¡ç®—äº§ç”Ÿçš„æ³„éœ²åˆ—
    "is_cancel_event", # åŸºäºäº‹ä»¶ç±»å‹è®¡ç®—ï¼Œå±äºæœªæ¥ä¿¡æ¯
    "is_trade_event",  # åŸºäºäº‹ä»¶ç±»å‹è®¡ç®—ï¼Œå±äºæœªæ¥ä¿¡æ¯
    # å…¶ä»–å¯èƒ½çš„æ³„éœ²åˆ—
    "finish_time", "final_survival_time_ms", "life_ms",
    "exec_qty", "fill_ratio", "canceled", "total_events"
]

# è¾“å‡ºæ—¶éœ€è¦ä¿å­˜çš„åˆ—ï¼ˆç‰¹å¾æ–‡ä»¶ï¼‰
FEATURE_OUTPUT_COLUMNS = KEY_COLUMNS + TIME_COLUMNS + RAW_DATA_COLUMNS + ALL_FEATURE_COLUMNS

def get_feature_columns(include_enhanced: bool = True) -> List[str]:
    """
    è·å–ç‰¹å¾åˆ—åˆ—è¡¨
    
    Args:
        include_enhanced: æ˜¯å¦åŒ…å«æ‰©å±•ç‰¹å¾
        
    Returns:
        ç‰¹å¾åˆ—åˆ—è¡¨
    """
    if include_enhanced:
        return BASE_FEATURE_COLUMNS + ENHANCED_FEATURE_COLUMNS
    else:
        return BASE_FEATURE_COLUMNS.copy()

def get_training_feature_columns(include_enhanced: bool = True) -> List[str]:
    """
    è·å–å¯ç”¨äºè®­ç»ƒçš„ç‰¹å¾åˆ—åˆ—è¡¨ï¼ˆæ’é™¤æ³„éœ²é£é™©åˆ—ï¼‰
    
    Args:
        include_enhanced: æ˜¯å¦åŒ…å«æ‰©å±•ç‰¹å¾
        
    Returns:
        å®‰å…¨çš„ç‰¹å¾åˆ—åˆ—è¡¨
    """
    features = get_feature_columns(include_enhanced)
    # æ’é™¤æ³„éœ²é£é™©åˆ—
    safe_features = [col for col in features if col not in LEAKAGE_RISK_COLUMNS]
    return safe_features

def get_key_columns() -> List[str]:
    """è·å–ä¸»é”®åˆ—åˆ—è¡¨"""
    return KEY_COLUMNS.copy()

def get_time_columns() -> List[str]:
    """è·å–æ—¶é—´åˆ—åˆ—è¡¨"""
    return TIME_COLUMNS.copy()

def get_raw_data_columns() -> List[str]:
    """è·å–åŸå§‹æ•°æ®åˆ—åˆ—è¡¨"""
    return RAW_DATA_COLUMNS.copy()

def get_leakage_risk_columns() -> List[str]:
    """è·å–æœ‰æ•°æ®æ³„éœ²é£é™©çš„åˆ—åˆ—è¡¨"""
    return LEAKAGE_RISK_COLUMNS.copy()

def get_feature_output_columns(include_enhanced: bool = True) -> List[str]:
    """
    è·å–ç‰¹å¾æ–‡ä»¶è¾“å‡ºåˆ—åˆ—è¡¨ï¼ˆæ’é™¤æ³„éœ²é£é™©åˆ—ï¼‰
    
    Args:
        include_enhanced: æ˜¯å¦åŒ…å«æ‰©å±•ç‰¹å¾
        
    Returns:
        å®‰å…¨çš„è¾“å‡ºåˆ—åˆ—è¡¨
    """
    columns = KEY_COLUMNS + TIME_COLUMNS + RAW_DATA_COLUMNS
    columns.extend(get_feature_columns(include_enhanced))
    
    # æ’é™¤æ³„éœ²é£é™©åˆ—
    safe_columns = [col for col in columns if col not in LEAKAGE_RISK_COLUMNS]
    return safe_columns

class FeatureGenerator:
    """ç‰¹å¾ç”Ÿæˆå™¨"""
    
    def __init__(self, backend: str = "polars", extended_features: bool = True):
        """
        åˆå§‹åŒ–ç‰¹å¾ç”Ÿæˆå™¨
        
        Args:
            backend: è®¡ç®—åç«¯ ("polars" æˆ– "pandas")
            extended_features: æ˜¯å¦è®¡ç®—æ‰©å±•ç‰¹å¾
        """
        self.backend = backend
        self.extended_features = extended_features
        self.console = Console()
        
        # å®šä¹‰ç‰¹å¾è®¡ç®—æµæ°´çº¿
        self.feature_pipeline = self._build_feature_pipeline()
    
    def _build_feature_pipeline(self) -> List[dict]:
        """æ„å»ºç‰¹å¾è®¡ç®—æµæ°´çº¿"""
        if self.backend == "polars":
            pipeline = [
                {
                    "name": "åŸºç¡€å®æ—¶ç‰¹å¾",
                    "function": "calc_realtime_features_polars",
                    "description": "ç›˜å£å¿«ç…§ã€ä»·æ ¼ç‰¹å¾ã€æ»šåŠ¨çª—å£ç»Ÿè®¡"
                },
                {
                    "name": "è®¢å•ç°¿å‹åŠ›ç‰¹å¾", 
                    "function": "calculate_order_book_pressure_polars",
                    "description": "book_imbalanceã€price_aggressivenessç­‰"
                }
            ]
            
            # Polarsåç«¯ç›®å‰æ‰©å±•ç‰¹å¾éœ€è¦è½¬æ¢åˆ°pandasè®¡ç®—
            if self.extended_features:
                pipeline.append({
                    "name": "æ‰©å±•å®æ—¶ç‰¹å¾",
                    "function": "calculate_enhanced_realtime_features_polars", 
                    "description": "z_survivalã€ä»·æ ¼åŠ¨é‡ã€è®¢å•å¯†åº¦ç­‰ (è½¬æ¢åˆ°pandasè®¡ç®—)"
                })
        else:
            pipeline = [
                {
                    "name": "åŸºç¡€å®æ—¶ç‰¹å¾",
                    "function": "calc_realtime_features",
                    "description": "ç›˜å£å¿«ç…§ã€ä»·æ ¼ç‰¹å¾ã€æ»šåŠ¨çª—å£ç»Ÿè®¡"
                }
            ]
            
            if self.extended_features:
                pipeline.append({
                    "name": "æ‰©å±•å®æ—¶ç‰¹å¾",
                    "function": "calculate_enhanced_realtime_features", 
                    "description": "z_survivalã€ä»·æ ¼åŠ¨é‡ã€è®¢å•å¯†åº¦ç­‰"
                })
            
            pipeline.append({
                "name": "è®¢å•ç°¿å‹åŠ›ç‰¹å¾",
                "function": "calculate_order_book_pressure",
                "description": "book_imbalanceã€price_aggressivenessç­‰"
            })
        
        return pipeline
    
    def generate_features_for_data(self, 
                                  data: Union[pd.DataFrame, pl.DataFrame, pl.LazyFrame],
                                  tickers: Optional[Set[str]] = None,
                                  show_progress: bool = True) -> Union[pd.DataFrame, pl.DataFrame]:
        """
        ä¸ºç»™å®šæ•°æ®ç”Ÿæˆç‰¹å¾
        
        Args:
            data: è¾“å…¥çš„å§”æ‰˜äº‹ä»¶æµæ•°æ®
            tickers: è‚¡ç¥¨ä»£ç ç­›é€‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            å¸¦ç‰¹å¾çš„DataFrame
        """
        
        if show_progress:
            progress = Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                console=self.console,
                transient=True
            )
            progress.start()
        
        try:
            # æ•°æ®é¢„å¤„ç†
            if show_progress:
                prep_task = progress.add_task("é¢„å¤„ç†æ•°æ®...", total=100)
            
            if self.backend == "polars":
                df_processed = self._preprocess_polars(data, tickers)
            else:
                df_processed = self._preprocess_pandas(data, tickers)
            
            if show_progress:
                progress.update(prep_task, advance=100)
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if self.backend == "polars":
                if isinstance(df_processed, pl.LazyFrame):
                    data_size = df_processed.collect().height
                else:
                    data_size = df_processed.height
            else:
                data_size = len(df_processed)
            
            if data_size == 0:
                self.console.print("[yellow]è­¦å‘Š: é¢„å¤„ç†åæ•°æ®ä¸ºç©º[/yellow]")
                return df_processed
            
            # ç‰¹å¾è®¡ç®—æµæ°´çº¿
            for i, step in enumerate(self.feature_pipeline):
                if show_progress:
                    feat_task = progress.add_task(f"è®¡ç®—{step['name']}...", total=100)
                
                try:
                    df_processed = self._execute_feature_step(df_processed, step)
                    
                    if show_progress:
                        progress.update(feat_task, advance=100)
                        
                except Exception as e:
                    self.console.print(f"[red]é”™è¯¯: {step['name']}è®¡ç®—å¤±è´¥: {e}[/red]")
                    if show_progress:
                        progress.update(feat_task, advance=100)
                    continue
            
            return df_processed
            
        finally:
            if show_progress:
                progress.stop()
    
    def _preprocess_polars(self, data: Union[pl.DataFrame, pl.LazyFrame], tickers: Optional[Set[str]]) -> pl.LazyFrame:
        """Polarsæ•°æ®é¢„å¤„ç†"""
        if isinstance(data, pl.DataFrame):
            df = data.lazy()
        else:
            df = data
        
        # è‚¡ç¥¨ç­›é€‰
        if tickers:
            df = df.filter(pl.col("ticker").is_in(list(tickers)))
        
        # ç¡®ä¿æ—¶é—´åˆ—ç±»å‹æ­£ç¡®
        try:
            # å®‰å…¨çš„æ—¶é—´è½¬æ¢
            df = df.with_columns([
                pl.col("å§”æ‰˜_datetime").str.strptime(pl.Datetime("ns"), format="%Y-%m-%d %H:%M:%S%.f", strict=False),
                pl.col("äº‹ä»¶_datetime").str.strptime(pl.Datetime("ns"), format="%Y-%m-%d %H:%M:%S%.f", strict=False)
            ])
            
            # æŒ‰å§”æ‰˜æ—¶é—´æ’åºï¼ˆé‡è¦ï¼šè§£å†³rollingæ“ä½œçš„æ’åºé—®é¢˜ï¼‰
            df = df.sort("å§”æ‰˜_datetime")
            
        except Exception as e:
            self.console.print(f"[yellow]è­¦å‘Š: æ—¶é—´åˆ—å¤„ç†å¤±è´¥: {e}[/yellow]")
            # å¦‚æœæ—¶é—´è½¬æ¢å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ ¼å¼
            try:
                df = df.with_columns([
                    pl.col("å§”æ‰˜_datetime").cast(pl.Datetime("ns")),
                    pl.col("äº‹ä»¶_datetime").cast(pl.Datetime("ns"))
                ])
                df = df.sort("å§”æ‰˜_datetime")
            except Exception as e2:
                self.console.print(f"[red]é”™è¯¯: æ— æ³•è½¬æ¢æ—¶é—´åˆ—: {e2}[/red]")
        
        return df
    
    def _preprocess_pandas(self, data: pd.DataFrame, tickers: Optional[Set[str]]) -> pd.DataFrame:
        """Pandasæ•°æ®é¢„å¤„ç†"""
        df = data.copy()
        
        # è‚¡ç¥¨ç­›é€‰
        if tickers:
            df = df[df["ticker"].isin(tickers)]
        
        # ç¡®ä¿æ—¶é—´åˆ—ç±»å‹æ­£ç¡®
        try:
            df["å§”æ‰˜_datetime"] = pd.to_datetime(df["å§”æ‰˜_datetime"], errors='coerce')
            df["äº‹ä»¶_datetime"] = pd.to_datetime(df["äº‹ä»¶_datetime"], errors='coerce')
            
            # åˆ é™¤æ—¶é—´è½¬æ¢å¤±è´¥çš„è¡Œ
            df = df.dropna(subset=["å§”æ‰˜_datetime", "äº‹ä»¶_datetime"])
            
            # æŒ‰å§”æ‰˜æ—¶é—´æ’åº
            df = df.sort_values("å§”æ‰˜_datetime")
            
        except Exception as e:
            self.console.print(f"[yellow]è­¦å‘Š: æ—¶é—´åˆ—å¤„ç†å¤±è´¥: {e}[/yellow]")
        
        return df
    
    def _execute_feature_step(self, data: Union[pd.DataFrame, pl.LazyFrame], step: dict) -> Union[pd.DataFrame, pl.LazyFrame]:
        """æ‰§è¡Œå•ä¸ªç‰¹å¾è®¡ç®—æ­¥éª¤"""
        func_name = step["function"]
        
        try:
            if self.backend == "polars":
                if func_name == "calc_realtime_features_polars":
                    return calc_realtime_features_polars(data)
                elif func_name == "calculate_order_book_pressure_polars":
                    # è·å–å½“å‰schema
                    if isinstance(data, pl.LazyFrame):
                        schema = data.collect_schema()
                    else:
                        schema = data.schema
                    return calculate_order_book_pressure_polars(data, dict(schema))
                elif func_name == "calculate_enhanced_realtime_features_polars":
                    # Polarsæ‰©å±•ç‰¹å¾ï¼šéœ€è¦è½¬æ¢åˆ°pandasè®¡ç®—ç„¶åè½¬å›polars
                    self.console.print("[dim]  è½¬æ¢åˆ°pandasè®¡ç®—æ‰©å±•ç‰¹å¾...[/dim]")
                    if isinstance(data, pl.LazyFrame):
                        pd_data = data.collect().to_pandas()
                    else:
                        pd_data = data.to_pandas()
                    
                    # è®¡ç®—æ‰©å±•ç‰¹å¾
                    enhanced_data = calculate_enhanced_realtime_features(pd_data)
                    
                    # è½¬å›polars
                    result = pl.from_pandas(enhanced_data)
                    self.console.print("[dim]  æ‰©å±•ç‰¹å¾è®¡ç®—å®Œæˆ[/dim]")
                    return result.lazy()
            else:
                if func_name == "calc_realtime_features":
                    return calc_realtime_features(data)
                elif func_name == "calculate_enhanced_realtime_features":
                    return calculate_enhanced_realtime_features(data)
                elif func_name == "calculate_order_book_pressure":
                    return calculate_order_book_pressure(data)
            
            return data
            
        except Exception as e:
            self.console.print(f"[red]è­¦å‘Š: {step['name']}è®¡ç®—å¤±è´¥: {e}[/red]")
            return data
    
    def process_single_file(self, 
                           input_path: Path,
                           output_path: Path,
                           tickers: Optional[Set[str]] = None) -> bool:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶
        
        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            tickers: è‚¡ç¥¨ä»£ç ç­›é€‰
            
        Returns:
            æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        try:
            self.console.print(f"[cyan]ğŸ“ å¤„ç†æ–‡ä»¶: {input_path.name}[/cyan]")
            
            # åŠ è½½æ•°æ®
            if self.backend == "polars":
                # å®šä¹‰schema_overridesæ¥å¤„ç†é—®é¢˜åˆ—
                schema_overrides = {
                    "å§”æ‰˜ç±»å‹": pl.Utf8,  # å¼ºåˆ¶ä¸ºå­—ç¬¦ä¸²ç±»å‹
                    "äº‹ä»¶ç±»å‹": pl.Utf8,
                    "æ–¹å‘_å§”æ‰˜": pl.Utf8,
                    "æ–¹å‘_äº‹ä»¶": pl.Utf8,
                    "æˆäº¤ä»£ç ": pl.Utf8,
                    "å§”æ‰˜_datetime": pl.Utf8,  # å…ˆè¯»ä¸ºå­—ç¬¦ä¸²ï¼Œåç»­è½¬æ¢
                    "äº‹ä»¶_datetime": pl.Utf8
                }
                
                df = pl.read_csv(
                    input_path, 
                    try_parse_dates=False,  # å…³é—­è‡ªåŠ¨æ—¥æœŸè§£æ
                    schema_overrides=schema_overrides,
                    infer_schema_length=10000,  # å¢åŠ ç±»å‹æ¨æ–­é•¿åº¦
                    ignore_errors=True  # å¿½ç•¥è§£æé”™è¯¯
                )
            else:
                df = pd.read_csv(input_path, dtype=str)  # å…ˆå…¨éƒ¨è¯»ä¸ºå­—ç¬¦ä¸²
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
            if self.backend == "polars":
                if df.height == 0:
                    self.console.print(f"[yellow]è·³è¿‡ç©ºæ–‡ä»¶: {input_path.name}[/yellow]")
                    return False
            else:
                if len(df) == 0:
                    self.console.print(f"[yellow]è·³è¿‡ç©ºæ–‡ä»¶: {input_path.name}[/yellow]")
                    return False
            
            # ç”Ÿæˆç‰¹å¾
            df_features = self.generate_features_for_data(df, tickers=tickers)
            
            # ä½¿ç”¨æ˜ç¡®å®šä¹‰çš„è¾“å‡ºåˆ—ï¼ˆé¿å…ä¿å­˜ä¸éœ€è¦çš„åˆ—ï¼‰
            if self.backend == "polars":
                if isinstance(df_features, pl.LazyFrame):
                    df_features = df_features.collect()
                
                # è·å–å®é™…å­˜åœ¨çš„è¾“å‡ºåˆ—
                available_cols = df_features.columns
                final_cols = [col for col in get_feature_output_columns(self.extended_features) 
                             if col in available_cols]
                
                # ç¡®ä¿å…³é”®åˆ—å­˜åœ¨
                missing_key_cols = [col for col in KEY_COLUMNS if col not in available_cols]
                if missing_key_cols:
                    self.console.print(f"[yellow]è­¦å‘Š: ç¼ºå°‘å…³é”®åˆ— {missing_key_cols}[/yellow]")
                
                df_final = df_features.select(final_cols)
            else:
                # Pandaså¤„ç†
                available_cols = df_features.columns.tolist()
                final_cols = [col for col in get_feature_output_columns(self.extended_features) 
                             if col in available_cols]
                
                # ç¡®ä¿å…³é”®åˆ—å­˜åœ¨
                missing_key_cols = [col for col in KEY_COLUMNS if col not in available_cols]
                if missing_key_cols:
                    self.console.print(f"[yellow]è­¦å‘Š: ç¼ºå°‘å…³é”®åˆ— {missing_key_cols}[/yellow]")
                
                df_final = df_features[final_cols]
            
            # ä¿å­˜ç»“æœ
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            if self.backend == "polars":
                df_final.write_parquet(output_path)
            else:
                df_final.to_parquet(output_path, index=False)
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if self.backend == "polars":
                feature_count = len([c for c in df_features.columns if c not in ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·']])
                sample_count = df_features.height
            else:
                feature_count = len([c for c in df_features.columns if c not in ['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·']])
                sample_count = len(df_features)
            
            self.console.print(f"[green]âœ“ å·²ä¿å­˜: {output_path.name}[/green]")
            self.console.print(f"  ğŸ“Š ç‰¹å¾æ•°: {feature_count}, æ ·æœ¬æ•°: {sample_count:,}")
            
            return True
            
        except Exception as e:
            self.console.print(f"[red]âŒ å¤„ç†å¤±è´¥ {input_path.name}: {e}[/red]")
            return False
    
    def get_summary(self) -> dict:
        """è·å–ç‰¹å¾ç”Ÿæˆå™¨æ‘˜è¦ä¿¡æ¯"""
        return {
            "backend": self.backend,
            "extended_features": self.extended_features,
            "pipeline_steps": len(self.feature_pipeline),
            "pipeline_details": self.feature_pipeline
        }

def process_event_stream_directory(event_stream_dir: Path,
                                  output_dir: Path,
                                  backend: str = "polars",
                                  extended_features: bool = True,
                                  tickers: Optional[Set[str]] = None,
                                  dates: Optional[List[str]] = None) -> dict:
    """
    æ‰¹é‡å¤„ç†event_streamç›®å½•ä¸‹çš„æ‰€æœ‰æ—¥æœŸæ–‡ä»¶
    
    Args:
        event_stream_dir: event_streamæ ¹ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        backend: è®¡ç®—åç«¯
        extended_features: æ˜¯å¦è®¡ç®—æ‰©å±•ç‰¹å¾
        tickers: è‚¡ç¥¨ä»£ç ç­›é€‰
        dates: æŒ‡å®šå¤„ç†æ—¥æœŸ
        
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    
    # åˆ›å»ºç‰¹å¾ç”Ÿæˆå™¨
    generator = FeatureGenerator(backend=backend, extended_features=extended_features)
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    summary = generator.get_summary()
    console.print(f"\n[bold green]ğŸ¯ ç‰¹å¾ç”Ÿæˆæ¨¡å—[/bold green]")
    console.print(f"[dim]è¾“å…¥ç›®å½•: {event_stream_dir}[/dim]")
    console.print(f"[dim]è¾“å‡ºç›®å½•: {output_dir}[/dim]")
    console.print(f"[dim]è®¡ç®—åç«¯: {summary['backend']}[/dim]")
    console.print(f"[dim]æ‰©å±•ç‰¹å¾: {summary['extended_features']}[/dim]")
    console.print(f"[dim]æµæ°´çº¿æ­¥éª¤: {summary['pipeline_steps']}[/dim]")
    console.print(f"[dim]è‚¡ç¥¨ç­›é€‰: {list(tickers) if tickers else 'å…¨éƒ¨'}[/dim]")
    
    for step in summary['pipeline_details']:
        console.print(f"[dim]  â€¢ {step['name']}: {step['description']}[/dim]")
    
    # æŸ¥æ‰¾æ‰€æœ‰å§”æ‰˜äº‹ä»¶æµæ–‡ä»¶
    pattern = event_stream_dir / "*" / "å§”æ‰˜äº‹ä»¶æµ.csv"
    csv_files = list(event_stream_dir.glob("*/å§”æ‰˜äº‹ä»¶æµ.csv"))
    
    if not csv_files:
        console.print(f"[red]é”™è¯¯: æœªæ‰¾åˆ°å§”æ‰˜äº‹ä»¶æµæ–‡ä»¶ {pattern}[/red]")
        return {"total": 0, "success": 0, "failed": 0}
    
    # æ—¥æœŸç­›é€‰
    if dates:
        target_dates = set(dates)
        csv_files = [f for f in csv_files if f.parent.name in target_dates]
        missing_dates = target_dates - {f.parent.name for f in csv_files}
        if missing_dates:
            console.print(f"[yellow]è­¦å‘Š: æœªæ‰¾åˆ°æ—¥æœŸ {missing_dates}[/yellow]")
    
    if not csv_files:
        console.print(f"[red]é”™è¯¯: ç­›é€‰åæ— å¯å¤„ç†æ–‡ä»¶[/red]")
        return {"total": 0, "success": 0, "failed": 0}
    
    console.print(f"[dim]å‘ç° {len(csv_files)} ä¸ªæ–‡ä»¶å¾…å¤„ç†[/dim]\n")
    
    # æ‰¹é‡å¤„ç†
    results = {"total": len(csv_files), "success": 0, "failed": 0, "processed_files": []}
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeElapsedColumn(),
        console=console
    ) as progress:
        task = progress.add_task("å¤„ç†æ–‡ä»¶", total=len(csv_files))
        
        for csv_file in sorted(csv_files):
            date_str = csv_file.parent.name
            output_file = output_dir / f"X_{date_str}.parquet"
            
            progress.update(task, description=f"å¤„ç† {date_str}")
            
            if generator.process_single_file(csv_file, output_file, tickers):
                results["success"] += 1
                results["processed_files"].append(date_str)
            else:
                results["failed"] += 1
            
            progress.advance(task)
    
    return results

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="ç‰¹å¾ç”Ÿæˆæ¨¡å—")
    parser.add_argument("--input_dir", required=True, help="event_streamæ ¹ç›®å½•")
    parser.add_argument("--output_dir", required=True, help="ç‰¹å¾è¾“å‡ºç›®å½•")
    parser.add_argument("--backend", choices=["pandas", "polars"], default="polars", help="è®¡ç®—åç«¯")
    parser.add_argument("--extended", action="store_true", help="è®¡ç®—æ‰©å±•ç‰¹å¾")
    parser.add_argument("--tickers", nargs="*", help="è‚¡ç¥¨ä»£ç ç­›é€‰")
    parser.add_argument("--dates", nargs="*", help="æŒ‡å®šå¤„ç†æ—¥æœŸï¼Œæ ¼å¼ YYYYMMDD")
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥ç›®å½•
    input_dir = Path(args.input_dir)
    if not input_dir.exists():
        console.print(f"[red]é”™è¯¯: è¾“å…¥ç›®å½•ä¸å­˜åœ¨ {input_dir}[/red]")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å¤„ç†å‚æ•°
    tickers_set = set(args.tickers) if args.tickers else None
    
    try:
        # æ‰§è¡Œæ‰¹é‡å¤„ç†
        results = process_event_stream_directory(
            event_stream_dir=input_dir,
            output_dir=output_dir,
            backend=args.backend,
            extended_features=args.extended,
            tickers=tickers_set,
            dates=args.dates
        )
        
        # æ˜¾ç¤ºç»“æœ
        console.print(f"\n[bold cyan]ğŸ“Š å¤„ç†å®Œæˆ[/bold cyan]")
        console.print(f"æ€»æ–‡ä»¶æ•°: {results['total']}")
        console.print(f"æˆåŠŸ: {results['success']}")
        console.print(f"å¤±è´¥: {results['failed']}")
        
        if results['success'] > 0:
            console.print(f"[green]âœ… ç‰¹å¾æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}[/green]")
            console.print(f"å¤„ç†æ—¥æœŸ: {sorted(results['processed_files'])}")
        
        if results['failed'] > 0:
            console.print(f"[red]âš ï¸ {results['failed']} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥[/red]")
        
    except Exception as e:
        console.print(f"[red]âŒ å¤„ç†å‡ºé”™: {e}[/red]")
        sys.exit(1)

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹:

# å¤„ç†æ‰€æœ‰æ—¥æœŸæ–‡ä»¶
python scripts/data_process/features/feature_generator.py \
    --input_dir "/home/ma-user/code/fenglang/Spoofing Detect/data/event_stream" \
    --output_dir "/home/ma-user/code/fenglang/Spoofing Detect/data/features" \
    --backend polars \
    --extended

# å¤„ç†æŒ‡å®šè‚¡ç¥¨å’Œæ—¥æœŸ
python scripts/data_process/features/feature_generator.py \
    --input_dir "/path/to/event_stream" \
    --output_dir "/path/to/features" \
    --tickers 000001.SZ 000002.SZ \
    --dates 20230301 20230302 \
    --backend polars
""" 