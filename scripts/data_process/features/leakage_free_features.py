#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
leakage_free_features.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ä¸¥æ ¼é˜²æ•°æ®æ³„éœ²çš„ç‰¹å¾å·¥ç¨‹æ¨¡å—
â€¢ æ˜ç¡®åŒºåˆ†å§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ä¿¡æ¯ vs æœªæ¥ä¿¡æ¯
â€¢ æä¾›æ—¶é—´çª—å£æ§åˆ¶çš„ç‰¹å¾è®¡ç®—
â€¢ æ ‡è®°æ½œåœ¨æ³„éœ²ç‰¹å¾
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Set
from rich.console import Console

console = Console()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ•°æ®æ³„éœ²è¯†åˆ« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def identify_leakage_columns(df: pd.DataFrame) -> Dict[str, List[str]]:
    """è¯†åˆ«å¹¶åˆ†ç±»æ•°æ®ä¸­çš„æ³„éœ²é£é™©ç‰¹å¾"""
    
    # æ˜æ˜¾çš„æœªæ¥ä¿¡æ¯ï¼ˆç»å¯¹ä¸èƒ½ç”¨ï¼‰
    future_info = [
        "å­˜æ´»æ—¶é—´_ms",           # å§”æ‰˜çš„æœ€ç»ˆå­˜æ´»æ—¶é—´
        "äº‹ä»¶_datetime",         # æˆäº¤/æ’¤å•å‘ç”Ÿæ—¶é—´
        "æˆäº¤ä»·æ ¼", "æˆäº¤æ•°é‡",     # æœªæ¥æˆäº¤ä¿¡æ¯
    ]
    
    # å§”æ‰˜ç»“æœä¿¡æ¯ï¼ˆæ³„éœ²ï¼‰
    order_outcome = [
        "äº‹ä»¶ç±»å‹",              # æœ€ç»ˆæ˜¯æˆäº¤è¿˜æ˜¯æ’¤å•
        "is_cancel_event",       # æ˜¯å¦æ’¤å•
        "is_trade_event",        # æ˜¯å¦æˆäº¤
    ]
    
    # åŸºäºæœªæ¥äº‹ä»¶çš„èšåˆç‰¹å¾ï¼ˆæ³„éœ²ï¼‰
    future_aggregates = []
    for col in df.columns:
        if any(keyword in col.lower() for keyword in [
            "total_", "final_", "num_trades", "num_cancels", 
            "fill_ratio", "fully_filled"
        ]):
            future_aggregates.append(col)
    
    # å§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ä¿¡æ¯ï¼ˆå®‰å…¨ï¼‰
    observable_at_order = [
        # åŸºç¡€å§”æ‰˜ä¿¡æ¯
        "å§”æ‰˜ä»·æ ¼", "å§”æ‰˜æ•°é‡", "å§”æ‰˜_datetime", "æ–¹å‘_å§”æ‰˜", "is_buy",
        
        # è¡Œæƒ…ä¿¡æ¯ï¼ˆå§”æ‰˜æ—¶åˆ»çš„å¸‚åœºçŠ¶æ€ï¼‰
        "ç”³ä¹°ä»·1", "ç”³å–ä»·1", "ç”³ä¹°é‡1", "ç”³å–é‡1", "å‰æ”¶ç›˜", 
        "bid1", "ask1", "bid_vol1", "ask_vol1", "prev_close",
        "mid_price", "spread", "delta_mid", "pct_spread",
        
        # æ—¶é—´ç‰¹å¾
        "time_sin", "time_cos", "in_auction", "seconds_since_market_open",
        
        # ä»·æ ¼åˆ†æ
        "price_dev_prevclose_bps", "price_aggressiveness", "cluster_score",
        
        # å¯¹æ•°å˜æ¢
        "log_qty", "log_order_price", "log_bid1", "log_ask1", 
        "log_bid_vol", "log_ask_vol", "log_order_amount",
    ]
    
    # å†å²ç»Ÿè®¡ç‰¹å¾ï¼ˆéœ€è¦æ£€æŸ¥æ—¶é—´çª—å£ï¼‰
    historical_stats = []
    for col in df.columns:
        if any(keyword in col for keyword in [
            "orders_", "cancels_", "trades_", "cancel_ratio_"
        ]):
            historical_stats.append(col)
    
    return {
        "future_info": [col for col in future_info if col in df.columns],
        "order_outcome": [col for col in order_outcome if col in df.columns], 
        "future_aggregates": future_aggregates,
        "observable_at_order": [col for col in observable_at_order if col in df.columns],
        "historical_stats": historical_stats
    }

def create_feature_metadata(df: pd.DataFrame) -> pd.DataFrame:
    """ä¸ºæ¯ä¸ªç‰¹å¾åˆ›å»ºå…ƒæ•°æ®ï¼Œæ ‡è®°æ³„éœ²é£é™©"""
    leakage_info = identify_leakage_columns(df)
    
    feature_metadata = []
    for col in df.columns:
        risk_level = "UNKNOWN"
        category = "å…¶ä»–"
        description = ""
        
        if col in leakage_info["future_info"]:
            risk_level = "HIGH_LEAKAGE"
            category = "æœªæ¥ä¿¡æ¯"
            description = "åŒ…å«å§”æ‰˜æ—¶åˆ»æœªçŸ¥çš„æœªæ¥ä¿¡æ¯"
        elif col in leakage_info["order_outcome"]:
            risk_level = "HIGH_LEAKAGE"
            category = "è®¢å•ç»“æœ"
            description = "åŒ…å«è®¢å•çš„æœ€ç»ˆç»“æœä¿¡æ¯"
        elif col in leakage_info["future_aggregates"]:
            risk_level = "HIGH_LEAKAGE"
            category = "æœªæ¥èšåˆ"
            description = "åŸºäºæœªæ¥äº‹ä»¶çš„èšåˆç»Ÿè®¡"
        elif col in leakage_info["observable_at_order"]:
            risk_level = "SAFE"
            category = "å¯è§‚æµ‹ä¿¡æ¯"
            description = "å§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹çš„ä¿¡æ¯"
        elif col in leakage_info["historical_stats"]:
            risk_level = "NEED_CHECK"
            category = "å†å²ç»Ÿè®¡"
            description = "éœ€æ£€æŸ¥æ—¶é—´çª—å£çš„å†å²ç»Ÿè®¡ç‰¹å¾"
        
        feature_metadata.append({
            "feature": col,
            "risk_level": risk_level,
            "category": category,
            "description": description
        })
    
    return pd.DataFrame(feature_metadata)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å®‰å…¨ç‰¹å¾æ„é€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_safe_market_features(df: pd.DataFrame) -> pd.DataFrame:
    """åˆ›å»ºåŸºäºå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ä¿¡æ¯çš„å®‰å…¨ç‰¹å¾"""
    df_safe = df.copy()
    
    console.print("[green]ğŸ›¡ï¸ åˆ›å»ºå®‰å…¨çš„å¸‚åœºç‰¹å¾...[/green]")
    
    # 1. åŸºç¡€ä»·æ ¼ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å·²çŸ¥ï¼‰
    if all(col in df.columns for col in ["ç”³ä¹°ä»·1", "ç”³å–ä»·1"]):
        df_safe["mid_price"] = (df_safe["ç”³ä¹°ä»·1"] + df_safe["ç”³å–ä»·1"]) / 2
        df_safe["spread"] = df_safe["ç”³å–ä»·1"] - df_safe["ç”³ä¹°ä»·1"]
        df_safe["relative_spread"] = df_safe["spread"] / df_safe["mid_price"]
    
    # 2. ä»·æ ¼åç¦»ç‰¹å¾
    if "å§”æ‰˜ä»·æ ¼" in df.columns and "mid_price" in df_safe.columns:
        df_safe["delta_mid"] = df_safe["å§”æ‰˜ä»·æ ¼"] - df_safe["mid_price"]
        df_safe["price_distance_pct"] = abs(df_safe["delta_mid"]) / df_safe["mid_price"] * 100
    
    # 3. è®¢å•ç°¿å‹åŠ›ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
    if all(col in df.columns for col in ["ç”³ä¹°é‡1", "ç”³å–é‡1"]):
        total_vol = df_safe["ç”³ä¹°é‡1"] + df_safe["ç”³å–é‡1"]
        df_safe["book_imbalance"] = (df_safe["ç”³ä¹°é‡1"] - df_safe["ç”³å–é‡1"]) / (total_vol + 1e-8)
    
    # 4. æ—¶é—´ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å·²çŸ¥ï¼‰
    if "å§”æ‰˜_datetime" in df.columns:
        df_safe["å§”æ‰˜_datetime"] = pd.to_datetime(df_safe["å§”æ‰˜_datetime"])
        
        # å¼€å¸‚æ—¶é—´ç›¸å¯¹ä½ç½®
        market_open = df_safe["å§”æ‰˜_datetime"].dt.normalize() + pd.Timedelta("09:30:00")
        seconds_since_open = (df_safe["å§”æ‰˜_datetime"] - market_open).dt.total_seconds()
        df_safe["seconds_since_market_open"] = seconds_since_open
        
        # å‘¨æœŸç‰¹å¾
        day_seconds = 4.5 * 3600
        df_safe["time_sin"] = np.sin(2 * np.pi * seconds_since_open / day_seconds)
        df_safe["time_cos"] = np.cos(2 * np.pi * seconds_since_open / day_seconds)
        
        # é›†åˆç«ä»·æ ‡è®°
        time_obj = df_safe["å§”æ‰˜_datetime"].dt.time
        morning_auction = (time_obj >= pd.Timestamp("09:15").time()) & (time_obj < pd.Timestamp("09:30").time())
        closing_auction = (time_obj >= pd.Timestamp("14:57").time()) & (time_obj <= pd.Timestamp("15:00").time())
        df_safe["in_auction"] = (morning_auction | closing_auction).astype(int)
    
    # 5. å¯¹æ•°å˜æ¢ï¼ˆå¤„ç†å°ºåº¦é—®é¢˜ï¼‰
    for col, new_col in [
        ("å§”æ‰˜æ•°é‡", "log_qty"),
        ("å§”æ‰˜ä»·æ ¼", "log_order_price"),
        ("ç”³ä¹°ä»·1", "log_bid1"),
        ("ç”³å–ä»·1", "log_ask1"),
        ("ç”³ä¹°é‡1", "log_bid_vol"),
        ("ç”³å–é‡1", "log_ask_vol")
    ]:
        if col in df.columns:
            df_safe[new_col] = np.log1p(pd.to_numeric(df_safe[col], errors='coerce').fillna(0))
    
    return df_safe

def create_safe_historical_features(df: pd.DataFrame, time_col: str = "å§”æ‰˜_datetime") -> pd.DataFrame:
    """åˆ›å»ºåŸºäºå†å²ä¿¡æ¯çš„å®‰å…¨ç‰¹å¾ï¼ˆä¸¥æ ¼æ—¶é—´çª—å£æ§åˆ¶ï¼‰"""
    df_hist = df.copy()
    df_hist[time_col] = pd.to_datetime(df_hist[time_col])
    df_hist = df_hist.sort_values([time_col])
    
    console.print("[green]ğŸ“Š åˆ›å»ºå†å²ç»Ÿè®¡ç‰¹å¾ï¼ˆä¸¥æ ¼æ—¶é—´æ§åˆ¶ï¼‰...[/green]")
    
    # ä»…ç»Ÿè®¡å½“å‰å§”æ‰˜æ—¶åˆ»ä¹‹å‰çš„å†å²äº‹ä»¶
    for window, suffix in [("100ms", "_100ms"), ("1s", "_1s"), ("5s", "_5s")]:
        # è®¢å•å¯†åº¦ï¼šè¿‡å»æ—¶é—´çª—å£å†…çš„è®¢å•æ•°
        df_hist[f"orders{suffix}"] = (
            df_hist.rolling(window, on=time_col, closed='left')[time_col]
            .count().fillna(0)
        )
    
    # æ³¨æ„ï¼šæ’¤å•å’Œæˆäº¤ç»Ÿè®¡éœ€è¦ç‰¹åˆ«å°å¿ƒï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½åŒ…å«å½“å‰è®¢å•çš„ç»“æœ
    # è¿™é‡Œæˆ‘ä»¬åªç»Ÿè®¡å†å²ä¸Šå…¶ä»–è®¢å•çš„æ’¤å•/æˆäº¤æƒ…å†µ
    if "äº‹ä»¶ç±»å‹" in df.columns:
        console.print("[yellow]âš ï¸ è­¦å‘Šï¼šäº‹ä»¶ç±»å‹åŒ…å«å½“å‰è®¢å•ç»“æœï¼Œéœ€è°¨æ…ä½¿ç”¨[/yellow]")
        
        # åˆ›å»ºå†å²æ’¤å•æ ‡è®°ï¼ˆæ’é™¤å½“å‰è®¢å•ï¼‰
        df_hist["past_cancel_flag"] = (df_hist["äº‹ä»¶ç±»å‹"] == "æ’¤å•").astype(int)
        df_hist["past_trade_flag"] = (df_hist["äº‹ä»¶ç±»å‹"] == "æˆäº¤").astype(int)
        
        # å†å²æ’¤å•ç‡ï¼ˆä»…åŸºäºè¿‡å»æ•°æ®ï¼‰
        for window, suffix in [("1s", "_1s"), ("5s", "_5s")]:
            past_orders = df_hist.rolling(window, on=time_col, closed='left')[time_col].count()
            past_cancels = df_hist.rolling(window, on=time_col, closed='left')["past_cancel_flag"].sum()
            df_hist[f"cancel_ratio{suffix}"] = (past_cancels / (past_orders + 1e-8)).fillna(0)
    
    return df_hist

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ç‰¹å¾éªŒè¯å’Œæ¸…ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_id_columns() -> List[str]:
    """è¿”å›æ•°æ®æ ‡è¯†åˆ—ï¼ˆä¸ç”¨äºè®­ç»ƒï¼‰"""
    return [
        # ä¸»é”®æ ‡è¯†
        "è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·", "å§”æ‰˜ç¼–å·",
        
        # æ—¶é—´æˆ³ï¼ˆåŸå§‹æ ¼å¼ï¼Œä¸æ˜¯ç‰¹å¾ï¼‰
        "æ—¶é—´", "æ—¶é—´_å§”æ‰˜", "æ—¶é—´_äº‹ä»¶", 
        "è‡ªç„¶æ—¥_å§”æ‰˜", "è‡ªç„¶æ—¥_äº‹ä»¶",
        "å§”æ‰˜_datetime", "äº‹ä»¶_datetime", "quote_dt",
        "ts_ns", "event_ts_ns",
        
        # ä»£ç æ ‡è¯†
        "ä¸‡å¾—ä»£ç ", "äº¤æ˜“æ‰€ä»£ç ", "æˆäº¤ä»£ç ",
        
        # å…¶ä»–æ ‡è¯†ä¿¡æ¯
        "å§”æ‰˜ç±»å‹", "æ–¹å‘_å§”æ‰˜", "æ–¹å‘_äº‹ä»¶"
    ]

def get_safe_feature_list() -> List[str]:
    """è¿”å›ç¡®è®¤å®‰å…¨çš„ç‰¹å¾åˆ—è¡¨ï¼ˆçº¯ç‰¹å¾ï¼Œä¸åŒ…å«IDï¼‰"""
    return [        
        # å§”æ‰˜åŸºç¡€ä¿¡æ¯ï¼ˆæ•°å€¼ç‰¹å¾ï¼‰
        "å§”æ‰˜ä»·æ ¼", "å§”æ‰˜æ•°é‡", "is_buy", "å§”æ‰˜é‡‘é¢",
        
        # è¡Œæƒ…ä¿¡æ¯ï¼ˆå§”æ‰˜æ—¶åˆ»çš„å¸‚åœºçŠ¶æ€ï¼‰
        "ç”³ä¹°ä»·1", "ç”³å–ä»·1", "ç”³ä¹°é‡1", "ç”³å–é‡1", "å‰æ”¶ç›˜",
        "bid1", "ask1", "bid_vol1", "ask_vol1", "prev_close",
        
        # æ´¾ç”Ÿä»·æ ¼ç‰¹å¾
        "mid_price", "spread", "relative_spread", "delta_mid", "price_distance_pct",
        "pct_spread", "price_vs_mid", "price_vs_mid_bps",
        "book_imbalance", "price_dev_prevclose_bps", "price_aggressiveness", "cluster_score",
        
        # æ—¶é—´ç‰¹å¾ï¼ˆå·¥ç¨‹åŒ–çš„ï¼Œä¸æ˜¯åŸå§‹æ—¶é—´æˆ³ï¼‰
        "time_sin", "time_cos", "in_auction", "seconds_since_market_open",
        
        # å¯¹æ•°å˜æ¢ç‰¹å¾
        "log_qty", "log_order_price", "log_bid1", "log_ask1", 
        "log_bid_vol", "log_ask_vol", "log_order_amount",
        
        # å†å²ç»Ÿè®¡ç‰¹å¾ï¼ˆåŸºäºæ—¶é—´çª—å£ï¼‰
        "orders_100ms", "orders_1s", "orders_5s",
        "cancels_100ms", "cancels_1s", "cancels_5s", "trades_1s",
        "cancel_ratio_100ms", "cancel_ratio_1s",
        
        # æŠ€æœ¯æŒ‡æ ‡
        "price_volatility", "price_momentum", "spread_volatility", "order_imbalance",
        
        # ç»Ÿè®¡ç‰¹å¾
        "hist_order_size_mean", "hist_spread_mean"
    ]

def get_leakage_feature_list() -> List[str]:
    """è¿”å›ç¡®è®¤æ³„éœ²çš„ç‰¹å¾åˆ—è¡¨ï¼ˆè®­ç»ƒæ—¶å¿…é¡»æ’é™¤ï¼‰"""
    return [
        # æ˜æ˜¾æœªæ¥ä¿¡æ¯
        "å­˜æ´»æ—¶é—´_ms", "final_survival_time_ms", "life_ms",
        "äº‹ä»¶_datetime", "finish_time",
        
        # æˆäº¤ç»“æœä¿¡æ¯
        "æˆäº¤ä»·æ ¼", "æˆäº¤æ•°é‡", "exec_qty", "fill_ratio",
        "äº‹ä»¶ç±»å‹", "is_cancel_event", "is_trade_event",
        
        # è®¢å•æœ€ç»ˆçŠ¶æ€
        "is_fully_filled", "canceled", "total_events",
        "num_trades", "num_cancels", "total_traded_qty",
        
        # æ ‡ç­¾ç›¸å…³
        "flag_R1", "flag_R2", "y_label",
        "enhanced_spoofing_liberal", "enhanced_spoofing_moderate", "enhanced_spoofing_strict"
    ]

def validate_features_safety(df: pd.DataFrame, feature_cols: List[str]) -> Dict[str, List[str]]:
    """éªŒè¯ç‰¹å¾çš„å®‰å…¨æ€§"""
    safe_features = get_safe_feature_list()
    leakage_features = get_leakage_feature_list()
    id_columns = get_id_columns()
    
    validated_safe = [col for col in feature_cols if col in safe_features]
    detected_leakage = [col for col in feature_cols if col in leakage_features]
    detected_ids = [col for col in feature_cols if col in id_columns]
    uncertain = [col for col in feature_cols if col not in safe_features and col not in leakage_features and col not in id_columns]
    
    return {
        "safe": validated_safe,
        "leakage": detected_leakage,
        "ids": detected_ids,
        "uncertain": uncertain
    }

def clean_features_for_training(df: pd.DataFrame, target_col: str = "y_label") -> pd.DataFrame:
    """æ¸…ç†æ•°æ®ï¼Œç§»é™¤æ³„éœ²ç‰¹å¾ï¼Œåªä¿ç•™å®‰å…¨ç‰¹å¾"""
    
    console.print("[bold cyan]ğŸ” æ¸…ç†ç‰¹å¾æ•°æ®ä»¥é˜²æ­¢æ³„éœ²...[/bold cyan]")
    
    # è·å–æ‰€æœ‰åˆ—
    all_cols = df.columns.tolist()
    
    # éªŒè¯ç‰¹å¾å®‰å…¨æ€§
    validation = validate_features_safety(df, all_cols)
    
    console.print(f"[green]âœ… å®‰å…¨ç‰¹å¾: {len(validation['safe'])} ä¸ª[/green]")
    console.print(f"[red]âŒ æ³„éœ²ç‰¹å¾: {len(validation['leakage'])} ä¸ª[/red]")
    console.print(f"[blue]ğŸ†” IDåˆ—: {len(validation['ids'])} ä¸ª[/blue]")
    console.print(f"[yellow]âš ï¸ ä¸ç¡®å®šç‰¹å¾: {len(validation['uncertain'])} ä¸ª[/yellow]")
    
    if validation['leakage']:
        console.print(f"[red]ç§»é™¤æ³„éœ²ç‰¹å¾: {validation['leakage'][:10]}{'...' if len(validation['leakage']) > 10 else ''}[/red]")
    
    if validation['ids']:
        console.print(f"[blue]ç§»é™¤IDåˆ—: {validation['ids'][:10]}{'...' if len(validation['ids']) > 10 else ''}[/blue]")
    
    if validation['uncertain']:
        console.print(f"[yellow]ä¸ç¡®å®šç‰¹å¾éœ€äººå·¥æ£€æŸ¥: {validation['uncertain'][:5]}{'...' if len(validation['uncertain']) > 5 else ''}[/yellow]")
    
    # ä¿ç•™å®‰å…¨ç‰¹å¾ + ç›®æ ‡å˜é‡
    keep_cols = validation['safe'].copy()
    if target_col in all_cols:
        keep_cols.append(target_col)
    
    # å¯¹äºä¸ç¡®å®šçš„ç‰¹å¾ï¼Œè¿›è¡Œæ›´ä¸¥æ ¼çš„ç­›é€‰
    uncertain_safe = []
    for col in validation['uncertain']:
        dtype = df[col].dtype
        # åªä¿ç•™æ•°å€¼å‹ç‰¹å¾ï¼Œæ’é™¤å­—ç¬¦ä¸²å’Œæ—¶é—´åˆ—
        if dtype in ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'bool']:
            uncertain_safe.append(col)
        elif 'datetime' not in str(dtype) and dtype != 'object':
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹
            try:
                pd.to_numeric(df[col], errors='raise')
                uncertain_safe.append(col)
            except:
                console.print(f"[yellow]è·³è¿‡éæ•°å€¼ç‰¹å¾: {col} (dtype: {dtype})[/yellow]")
        else:
            console.print(f"[yellow]è·³è¿‡éæ•°å€¼ç‰¹å¾: {col} (dtype: {dtype})[/yellow]")
    
    keep_cols.extend(uncertain_safe)
    
    # ç¡®ä¿åˆ—åœ¨DataFrameä¸­å­˜åœ¨
    keep_cols = [col for col in keep_cols if col in all_cols]
    
    cleaned_df = df[keep_cols].copy()
    
    # æœ€ç»ˆæ•°æ®ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
    for col in cleaned_df.columns:
        if col == target_col:
            continue
        dtype = cleaned_df[col].dtype
        if dtype == 'object':
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹
            try:
                cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce').fillna(0)
            except:
                console.print(f"[red]æ— æ³•è½¬æ¢ {col} ä¸ºæ•°å€¼å‹ï¼Œç§»é™¤æ­¤åˆ—[/red]")
                cleaned_df = cleaned_df.drop(columns=[col])
        elif 'datetime' in str(dtype):
            console.print(f"[red]ç§»é™¤æ—¶é—´åˆ—: {col}[/red]")
            cleaned_df = cleaned_df.drop(columns=[col])
    
    console.print(f"[green]ğŸ“Š æ¸…ç†å®Œæˆ: {len(cleaned_df.columns)} ä¸ªç‰¹å¾ä¿ç•™ï¼ˆæœ€ç»ˆæ•°å€¼æ£€æŸ¥é€šè¿‡ï¼‰[/green]")
    
    return cleaned_df 