#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
run_etl_from_event.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. è¾“å…¥  : event_stream/<YYYYMMDD>/å§”æ‰˜äº‹ä»¶æµ.csv
2. è¾“å‡º  : features_select/X_<YYYYMMDD>.parquet
           labels_select/labels_<YYYYMMDD>.parquet
3. è°ƒç”¨  :
      python run_etl_from_event.py \
          --root /obs/.../event_stream \
          --tickers 000989.SZ 300233.SZ \
          --r1_ms 50 --r2_ms 1000 --r2_mult 4.0 \
          --backend polars  # æ–°å¢ï¼šé€‰æ‹©è®¡ç®—åç«¯
"""

import pandas as pd, numpy as np
import polars as pl
from pathlib import Path
import argparse, re, sys
from rich.console import Console
from rich.progress import (
    Progress,
    SpinnerColumn,
    TextColumn,
    BarColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)
from rich import print as rprint
from concurrent.futures import ProcessPoolExecutor
import os

# Configure console
console = Console()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STRICT = True  # True = ç¼ºåˆ—ç«‹å³æŠ¥é”™ï¼›False = è‡ªåŠ¨å¡« NaN

# ä»…ä¿ç•™åœ¨å§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹çš„ç‰¹å¾ï¼Œé˜²æ­¢æ•°æ®æ³„éœ²
REALTIME_FEATURES = [
    "bid1",
    "ask1",
    "prev_close",
    "mid_price",
    "spread",
    "delta_mid",
    "pct_spread",
    "price_dev_prevclose",
    "is_buy",
    "log_qty",
    "orders_100ms",
    "cancels_5s",
    "time_sin",
    "time_cos",
    "in_auction",
]

# è®¾ç½® Polars çº¿ç¨‹æ•°
os.environ["POLARS_MAX_THREADS"] = str(os.cpu_count())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ util â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def require_cols(df: pd.DataFrame, cols: list[str]):
    """ä¸¥æ ¼æ¨¡å¼ï¼šä¿è¯å¿…é¡»åˆ—å…¨éƒ¨å­˜åœ¨ï¼Œå¦åˆ™æŠ›é”™"""
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise ValueError(f"ç¼ºå°‘å¿…è¦è¡Œæƒ…åˆ—ï¼š{missing}")


def calc_realtime_features(df: pd.DataFrame) -> pd.DataFrame:
    """å®æ—¶å¯è§‚æµ‹ç‰¹å¾åˆ—"""
    # ---------- 0. å¿…è¦åˆ—æ ¡éªŒ ----------
    needed = ["ç”³ä¹°ä»·1", "ç”³å–ä»·1", "å‰æ”¶ç›˜"]
    if STRICT:
        require_cols(df, needed)

    # ---------- 1. è¡Œæƒ…åŸå§‹åˆ— ----------
    df["bid1"] = df.get("ç”³ä¹°ä»·1")
    df["ask1"] = df.get("ç”³å–ä»·1")
    df["prev_close"] = df.get("å‰æ”¶ç›˜")

    # ---------- 2. ä»·æ ¼æ´¾ç”Ÿ ----------
    df["is_buy"] = (df["æ–¹å‘_å§”æ‰˜"] == "ä¹°").astype(int)
    df["mid_price"] = (df["bid1"] + df["ask1"]) / 2
    df["spread"] = df["ask1"] - df["bid1"]
    df["delta_mid"] = df["å§”æ‰˜ä»·æ ¼"] - df["mid_price"]
    df["pct_spread"] = (
        (df["delta_mid"].abs() / df["spread"]).replace([np.inf, -np.inf], 0).fillna(0)
    )
    df["log_qty"] = np.log1p(df["å§”æ‰˜æ•°é‡"])

    df.sort_values("å§”æ‰˜_datetime", inplace=True)
    # è®¢å•æ•°ç»Ÿè®¡ï¼šä½¿ç”¨ count() è€Œä¸æ˜¯ sum()
    df["orders_100ms"] = (
        df.rolling("100ms", on="å§”æ‰˜_datetime")["å§”æ‰˜_datetime"]
        .count()
        .shift(1)
        .fillna(0)
    )

    # æ’¤å•ç»Ÿè®¡ï¼šå…ˆè½¬æ¢ä¸º 0/1 å† sum
    df["is_cancel"] = (df["äº‹ä»¶ç±»å‹"] == "æ’¤å•").astype(int)
    df["cancels_5s"] = (
        df.rolling("5s", on="å§”æ‰˜_datetime")["is_cancel"].sum().shift(1).fillna(0)
    )

    sec = (
        df["å§”æ‰˜_datetime"]
        - df["å§”æ‰˜_datetime"].dt.normalize()
        - pd.Timedelta("09:30:00")
    ).dt.total_seconds()
    df["time_sin"] = np.sin(2 * np.pi * sec / (4.5 * 3600))
    df["time_cos"] = np.cos(2 * np.pi * sec / (4.5 * 3600))

    df["in_auction"] = (
        (
            df["å§”æ‰˜_datetime"].dt.time.between(
                pd.to_datetime("09:15").time(), pd.to_datetime("09:30").time(), "left"
            )
        )
        | (
            df["å§”æ‰˜_datetime"].dt.time.between(
                pd.to_datetime("14:57").time(), pd.to_datetime("15:00").time(), "both"
            )
        )
    ).astype(int)
    df["price_dev_prevclose"] = (
        (df["å§”æ‰˜ä»·æ ¼"] - df["prev_close"]) / df["prev_close"]
    ).fillna(0)

    return df


def apply_label_rules(df: pd.DataFrame, r1_ms: int, r2_ms: int, r2_mult: float):
    """
    ä¿®æ­£ç‰ˆæ ‡ç­¾è§„åˆ™ï¼š
    - R1: å¿«é€Ÿæ’¤å•ï¼ˆå­˜æ´»æ—¶é—´ < r1_ms ä¸”äº‹ä»¶ç±»å‹ä¸ºæ’¤å•ï¼‰
    - R2: åç¦»å¸‚åœºä¸­å¿ƒä»·ï¼ˆå­˜æ´»æ—¶é—´ < r2_ms ä¸”ä»·æ ¼åç¦» >= r2_mult * spreadï¼‰
    """
    # R1: å¿«é€Ÿæ’¤å•ï¼ˆäº‹ä»¶ç±»å‹ä¸ºæ’¤å•ä¸”å­˜æ´»æ—¶é—´å¾ˆçŸ­ï¼‰
    df["flag_R1"] = (df["å­˜æ´»æ—¶é—´_ms"] < r1_ms) & (df["äº‹ä»¶ç±»å‹"] == "æ’¤å•")

    # R2: ä»·æ ¼åç¦»ï¼ˆå­˜æ´»æ—¶é—´çŸ­ä¸”ä»·æ ¼åç¦»å¤§ï¼‰
    # NaN spread åœ¨ R2 é‡Œåº”è§†ä¸º"ä¸æ»¡è¶³" â†’ ç”¨ +âˆ å ä½
    safe_spread = df["spread"].fillna(np.inf)
    df["flag_R2"] = (df["å­˜æ´»æ—¶é—´_ms"] < r2_ms) & (
        df["delta_mid"].abs() >= r2_mult * safe_spread
    )
    df["y_label"] = (df["flag_R1"] & df["flag_R2"]).astype(int)

    return df


def remove_leakage_features(df: pd.DataFrame) -> pd.DataFrame:
    """ç§»é™¤åŒ…å«æœªæ¥ä¿¡æ¯çš„ç‰¹å¾åˆ—"""
    leakage_features = [
        "å­˜æ´»æ—¶é—´_ms",
        "final_survival_time_ms",
        "total_traded_qty",
        "num_trades",
        "is_fully_filled",
        "total_events",
        "num_cancels",
        "is_cancel",
    ]
    return df[[c for c in df.columns if c not in leakage_features]]


def calc_realtime_features_polars(df: pl.LazyFrame) -> pl.LazyFrame:
    """ä½¿ç”¨ Polars Lazy API è®¡ç®—å®æ—¶å¯è§‚æµ‹ç‰¹å¾"""
    # ç¡®ä¿æ—¶é—´åˆ—æ˜¯ ns ç²¾åº¦ï¼Œç”¨äºçª—å£å¯¹é½
    df = df.with_columns(
        [
            pl.col("å§”æ‰˜_datetime").cast(pl.Datetime("ns")).alias("ts_ns"),
            pl.col("äº‹ä»¶_datetime").cast(pl.Datetime("ns")).alias("event_ts_ns"),
        ]
    )

    # ç¬¬ä¸€æ­¥ï¼šåŸºç¡€åˆ—é‡å‘½å
    df = df.with_columns(
        [
            # è¡Œæƒ…åŸå§‹åˆ—
            pl.col("ç”³ä¹°ä»·1").alias("bid1"),
            pl.col("ç”³å–ä»·1").alias("ask1"),
            pl.col("å‰æ”¶ç›˜").alias("prev_close"),
            # åŸºç¡€æ´¾ç”Ÿ
            (pl.col("æ–¹å‘_å§”æ‰˜") == "ä¹°").cast(pl.Int8).alias("is_buy"),
            (pl.col("äº‹ä»¶ç±»å‹") == "æ’¤å•").cast(pl.Int8).alias("is_cancel"),
            pl.col("å§”æ‰˜æ•°é‡").log1p().alias("log_qty"),
        ]
    )

    # ç¬¬äºŒæ­¥ï¼šè®¡ç®—ä¸­é—´ä»·å’Œä»·å·®
    df = df.with_columns(
        [
            ((pl.col("bid1") + pl.col("ask1")) / 2).alias("mid_price"),
            (pl.col("ask1") - pl.col("bid1")).alias("spread"),
        ]
    )

    # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—ä¾èµ–äºä¸­é—´ä»·çš„ç‰¹å¾
    df = df.with_columns(
        [
            (pl.col("å§”æ‰˜ä»·æ ¼") - pl.col("mid_price")).alias("delta_mid"),
            ((pl.col("å§”æ‰˜ä»·æ ¼") - pl.col("prev_close")) / pl.col("prev_close"))
            .fill_null(0)
            .alias("price_dev_prevclose"),
        ]
    )

    # ç¬¬å››æ­¥ï¼šè®¡ç®—ä¾èµ–äºdelta_midçš„ç‰¹å¾
    df = df.with_columns(
        [
            (pl.col("delta_mid").abs() / pl.col("spread"))
            .fill_null(0)
            .replace([float("inf"), float("-inf")], 0)
            .alias("pct_spread"),
        ]
    )

    # ç¬¬äº”æ­¥ï¼šæ—¶é—´ç‰¹å¾ - ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•é¿å… map_elements
    df = df.with_columns(
        [
            # è®¡ç®—è‡ªå¸‚åœºå¼€å§‹çš„ç§’æ•°
            (
                (pl.col("å§”æ‰˜_datetime").dt.hour() - 9) * 3600
                + (pl.col("å§”æ‰˜_datetime").dt.minute() - 30) * 60
                + pl.col("å§”æ‰˜_datetime").dt.second()
            ).alias("seconds_since_market_open"),
            # é›†åˆç«ä»·æ—¶æ®µ
            (
                pl.col("å§”æ‰˜_datetime")
                .dt.time()
                .is_between(pl.time(9, 15), pl.time(9, 30), "left")
                | pl.col("å§”æ‰˜_datetime")
                .dt.time()
                .is_between(pl.time(14, 57), pl.time(15, 0), "both")
            )
            .cast(pl.Int8)
            .alias("in_auction"),
        ]
    )

    # è®¡ç®—ä¸‰è§’å‡½æ•°ç‰¹å¾
    df = df.with_columns(
        [
            (2 * 3.14159 * pl.col("seconds_since_market_open") / (4.5 * 3600))
            .sin()
            .alias("time_sin"),
            (2 * 3.14159 * pl.col("seconds_since_market_open") / (4.5 * 3600))
            .cos()
            .alias("time_cos"),
        ]
    )

    # æ»šåŠ¨çª—å£ç‰¹å¾ - ç®€åŒ–å¤„ç†ï¼Œé¿å… groupby_dynamic é—®é¢˜
    try:
        # å…ˆæ”¶é›†æ•°æ®ä»¥è¿›è¡Œæ»šåŠ¨çª—å£è®¡ç®—
        df_collected = df.collect()

        # æ£€æŸ¥æ˜¯å¦æœ‰ groupby_dynamic æ–¹æ³•
        if hasattr(df_collected, "groupby_dynamic"):
            # 1. 100ms è®¢å•è®¡æ•°
            orders_100ms = (
                df_collected.with_columns(pl.lit(1).alias("is_order"))
                .groupby_dynamic(
                    index_column="ts_ns", every="100ms", period="100ms", closed="right"
                )
                .agg(pl.col("is_order").count().alias("orders_100ms"))
            )

            # 2. 5s æ’¤å•è®¡æ•°
            cancels_5s = df_collected.groupby_dynamic(
                index_column="ts_ns", every="1s", period="5s", closed="right"
            ).agg(pl.col("is_cancel").sum().alias("cancels_5s"))

            # åˆå¹¶æ»šåŠ¨çª—å£ç‰¹å¾
            df_result = (
                df_collected.lazy()
                .join_asof(
                    orders_100ms.lazy(),
                    on="ts_ns",
                    strategy="backward",
                    tolerance="100ms",
                )
                .join_asof(
                    cancels_5s.lazy(), on="ts_ns", strategy="backward", tolerance="5s"
                )
            )
        else:
            # å¦‚æœæ²¡æœ‰ groupby_dynamicï¼Œä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•
            df_result = df_collected.lazy().with_columns(
                [
                    pl.lit(0).alias("orders_100ms"),  # é»˜è®¤å€¼
                    pl.lit(0).alias("cancels_5s"),  # é»˜è®¤å€¼
                ]
            )

    except Exception as e:
        # å¦‚æœæ»šåŠ¨çª—å£è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
        console.print(
            f"[yellow]Warning: Rolling window calculation failed, using default values[/yellow]"
        )
        df_result = df.with_columns(
            [
                pl.lit(0).alias("orders_100ms"),
                pl.lit(0).alias("cancels_5s"),
            ]
        )

    return df_result


def process_one_day_polars(
    evt_csv: Path,
    feat_dir: Path,
    lbl_dir: Path,
    watch: set,
    r1_ms: int,
    r2_ms: int,
    r2_mult: float,
):
    """ä½¿ç”¨ Polars å¤„ç†å•æ—¥æ•°æ®"""
    date_str = evt_csv.parent.name

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeElapsedColumn(),
        TimeRemainingColumn(),
        console=console,
    ) as progress:

        # ä»»åŠ¡1: åŠ è½½æ•°æ®
        load_task = progress.add_task(
            f"[cyan]{date_str}[/cyan] Loading CSV...", total=100
        )

        # ä½¿ç”¨ Polars è¯»å– CSVï¼Œæ·»åŠ  schema è¦†ç›–å’Œ null å€¼å¤„ç†
        try:
            df = pl.scan_csv(
                evt_csv,
                try_parse_dates=True,
                infer_schema_length=20000,
                null_values=["", "NULL", "null", "U"],
                ignore_errors=True,
            )
        except Exception as e:
            # å¦‚æœ Polars è§£æå¤±è´¥ï¼Œå›é€€åˆ° pandas
            console.print(
                f"[yellow]Warning: Polars CSV parsing failed for {date_str}, falling back to pandas[/yellow]"
            )
            return process_one_day_pandas_fallback(
                evt_csv, feat_dir, lbl_dir, watch, r1_ms, r2_ms, r2_mult
            )

        progress.update(load_task, advance=100)

        if watch:
            df = df.filter(pl.col("ticker").is_in(list(watch)))

        # æ·»åŠ æ—¥æœŸåˆ—
        df = df.with_columns(pl.lit(date_str).alias("è‡ªç„¶æ—¥"))

        # ä»»åŠ¡2: è®¡ç®—ç‰¹å¾
        feat_task = progress.add_task(
            f"[green]{date_str}[/green] Computing features...", total=100
        )
        df = calc_realtime_features_polars(df)
        progress.update(feat_task, advance=50)

        # åº”ç”¨æ ‡ç­¾è§„åˆ™
        df = df.with_columns(
            [
                # R1: å¿«é€Ÿæ’¤å•
                ((pl.col("å­˜æ´»æ—¶é—´_ms") < r1_ms) & (pl.col("äº‹ä»¶ç±»å‹") == "æ’¤å•"))
                .cast(pl.Int8)
                .alias("flag_R1"),
                # R2: ä»·æ ¼åç¦»
                (
                    (pl.col("å­˜æ´»æ—¶é—´_ms") < r2_ms)
                    & (
                        pl.col("delta_mid").abs()
                        >= r2_mult * pl.col("spread").fill_null(float("inf"))
                    )
                )
                .cast(pl.Int8)
                .alias("flag_R2"),
            ]
        )

        # è®¡ç®—æœ€ç»ˆæ ‡ç­¾
        df = df.with_columns(
            (pl.col("flag_R1") & pl.col("flag_R2")).cast(pl.Int8).alias("y_label")
        )
        progress.update(feat_task, advance=50)

        # ä»»åŠ¡3: æŒ‰å§”æ‰˜èšåˆ
        agg_task = progress.add_task(
            f"[yellow]{date_str}[/yellow] Aggregating orders...", total=100
        )

        # æ”¶é›†æ•°æ®å¹¶æŒ‰å§”æ‰˜åˆ†ç»„èšåˆ
        df_collected = df.collect()

        # ä½¿ç”¨æ­£ç¡®çš„ Polars è¯­æ³•
        try:
            df_orders = df_collected.group_by(["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"]).agg(
                [
                    pl.first(
                        "bid1",
                        "ask1",
                        "prev_close",
                        "mid_price",
                        "spread",
                        "delta_mid",
                        "pct_spread",
                        "orders_100ms",
                        "cancels_5s",
                        "log_qty",
                        "time_sin",
                        "time_cos",
                        "in_auction",
                        "price_dev_prevclose",
                        "is_buy",
                    ),
                    pl.col("y_label").max().alias("y_label"),
                ]
            )
        except AttributeError:
            # å¦‚æœ group_by ä¸å­˜åœ¨ï¼Œå°è¯• groupby
            try:
                df_orders = df_collected.groupby(
                    ["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"]
                ).agg(
                    [
                        pl.first(
                            "bid1",
                            "ask1",
                            "prev_close",
                            "mid_price",
                            "spread",
                            "delta_mid",
                            "pct_spread",
                            "orders_100ms",
                            "cancels_5s",
                            "log_qty",
                            "time_sin",
                            "time_cos",
                            "in_auction",
                            "price_dev_prevclose",
                            "is_buy",
                        ),
                        pl.col("y_label").max().alias("y_label"),
                    ]
                )
            except Exception as e:
                # å¦‚æœ Polars åˆ†ç»„æ“ä½œå¤±è´¥ï¼Œå›é€€åˆ° pandas
                console.print(
                    f"[yellow]Warning: Polars groupby failed for {date_str}, falling back to pandas[/yellow]"
                )
                return process_one_day_pandas_fallback(
                    evt_csv, feat_dir, lbl_dir, watch, r1_ms, r2_ms, r2_mult
                )

        progress.update(agg_task, advance=100)

        # ä»»åŠ¡4: ä¿å­˜æ•°æ®
        save_task = progress.add_task(
            f"[magenta]{date_str}[/magenta] Saving files...", total=100
        )

        # æ£€æŸ¥ç»“æœ
        if df_orders.is_empty():
            return 0, 0

        df_orders = remove_leakage_features(df_orders)

        feat_cols = ["è‡ªç„¶æ—¥", "äº¤æ˜“æ‰€å§”æ‰˜å·", "ticker", *REALTIME_FEATURES]

        feat_dir.mkdir(exist_ok=True)
        lbl_dir.mkdir(exist_ok=True)
        progress.update(save_task, advance=30)

        df_orders.select(feat_cols).write_parquet(
            feat_dir / f"X_{date_str}.parquet", compression="snappy"
        )
        progress.update(save_task, advance=35)

        df_orders.select(["è‡ªç„¶æ—¥", "äº¤æ˜“æ‰€å§”æ‰˜å·", "ticker", "y_label"]).write_parquet(
            lbl_dir / f"labels_{date_str}.parquet", compression="snappy"
        )
        progress.update(save_task, advance=35)

        return len(df_orders), int(df_orders["y_label"].sum())


def process_one_day_pandas_fallback(
    evt_csv: Path,
    feat_dir: Path,
    lbl_dir: Path,
    watch: set,
    r1_ms: int,
    r2_ms: int,
    r2_mult: float,
):
    """Pandas å›é€€å¤„ç†å‡½æ•°"""
    date_str = evt_csv.parent.name

    # è®¾ç½® low_memory=False é¿å…æ··åˆç±»å‹è­¦å‘Š
    df = pd.read_csv(
        evt_csv, parse_dates=["å§”æ‰˜_datetime", "äº‹ä»¶_datetime"], low_memory=False
    )

    if watch:
        df = df[df["ticker"].isin(watch)]
    if df.empty:
        return 0, 0

    # æ·»åŠ æ—¥æœŸåˆ—ï¼ˆä»æ–‡ä»¶è·¯å¾„æå–ï¼‰
    df["è‡ªç„¶æ—¥"] = date_str

    # è®¡ç®—ç‰¹å¾
    df = calc_realtime_features(df)
    df = apply_label_rules(df, r1_ms, r2_ms, r2_mult)

    # æŒ‰å§”æ‰˜èšåˆæ•°æ®
    order_groups = df.groupby(["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"])

    order_features = []

    for (date, ticker, order_id), group in order_groups:
        # å–å§”æ‰˜æ—¶åˆ»çš„ç‰¹å¾ï¼ˆç¬¬ä¸€è¡Œï¼Œå› ä¸ºæŒ‰å§”æ‰˜æ—¶é—´æ’åºï¼‰
        first_row = group.iloc[0].copy()

        # èšåˆæ ‡ç­¾ï¼šå¦‚æœä»»ä½•ä¸€ä¸ªäº‹ä»¶è§¦å‘äº†æ ‡ç­¾ï¼Œåˆ™æ•´ä¸ªå§”æ‰˜è¢«æ ‡è®°ä¸ºæ­£ä¾‹
        aggregated_label = group["y_label"].max()

        first_row["y_label"] = aggregated_label

        order_features.append(first_row)

    # è½¬æ¢ä¸ºDataFrame
    df_orders = pd.DataFrame(order_features)
    df_orders = remove_leakage_features(df_orders)

    if df_orders.empty:
        return 0, 0

    feat_cols = ["è‡ªç„¶æ—¥", "äº¤æ˜“æ‰€å§”æ‰˜å·", "ticker", *REALTIME_FEATURES]

    feat_dir.mkdir(exist_ok=True)
    lbl_dir.mkdir(exist_ok=True)

    df_orders[feat_cols].to_parquet(feat_dir / f"X_{date_str}.parquet", index=False)
    df_orders[["è‡ªç„¶æ—¥", "äº¤æ˜“æ‰€å§”æ‰˜å·", "ticker", "y_label"]].to_parquet(
        lbl_dir / f"labels_{date_str}.parquet", index=False
    )

    return len(df_orders), int(df_orders["y_label"].sum())


def process_one_day(
    evt_csv: Path,
    feat_dir: Path,
    lbl_dir: Path,
    watch: set,
    r1_ms: int,
    r2_ms: int,
    r2_mult: float,
    backend: str = "pandas",
):
    """æ ¹æ®åç«¯é€‰æ‹©å¤„ç†å‡½æ•°"""
    if backend == "polars":
        return process_one_day_polars(
            evt_csv, feat_dir, lbl_dir, watch, r1_ms, r2_ms, r2_mult
        )
    else:
        # åŸæœ‰çš„ Pandas å¤„ç†é€»è¾‘
        date_str = evt_csv.parent.name

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
            console=console,
        ) as progress:

            # ä»»åŠ¡1: åŠ è½½æ•°æ®
            load_task = progress.add_task(
                f"[cyan]{date_str}[/cyan] Loading CSV...", total=100
            )

            # è®¾ç½® low_memory=False é¿å…æ··åˆç±»å‹è­¦å‘Š
            df = pd.read_csv(
                evt_csv,
                parse_dates=["å§”æ‰˜_datetime", "äº‹ä»¶_datetime"],
                low_memory=False,
            )
            progress.update(load_task, advance=100)

            if watch:
                df = df[df["ticker"].isin(watch)]
            if df.empty:
                return 0, 0

            # æ·»åŠ æ—¥æœŸåˆ—ï¼ˆä»æ–‡ä»¶è·¯å¾„æå–ï¼‰
            df["è‡ªç„¶æ—¥"] = date_str  # æ·»åŠ äº¤æ˜“æ—¥æœŸåˆ—

            # ä»»åŠ¡2: è®¡ç®—åŸºç¡€ç‰¹å¾
            feat_task = progress.add_task(
                f"[green]{date_str}[/green] Computing features...", total=100
            )
            df = calc_realtime_features(df)
            progress.update(feat_task, advance=50)

            df = apply_label_rules(df, r1_ms, r2_ms, r2_mult)
            progress.update(feat_task, advance=50)

            # ä»»åŠ¡3: æŒ‰å§”æ‰˜èšåˆæ•°æ®
            # æŒ‰å§”æ‰˜åˆ†ç»„èšåˆ
            order_groups = df.groupby(["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"])
            total_orders = len(order_groups)

            agg_task = progress.add_task(
                f"[yellow]{date_str}[/yellow] Aggregating {total_orders:,} orders...",
                total=total_orders,
            )

            order_features = []

            for i, ((date, ticker, order_id), group) in enumerate(order_groups):
                # å–å§”æ‰˜æ—¶åˆ»çš„ç‰¹å¾ï¼ˆç¬¬ä¸€è¡Œï¼Œå› ä¸ºæŒ‰å§”æ‰˜æ—¶é—´æ’åºï¼‰
                first_row = group.iloc[0].copy()

                # èšåˆæ ‡ç­¾ï¼šå¦‚æœä»»ä½•ä¸€ä¸ªäº‹ä»¶è§¦å‘äº†æ ‡ç­¾ï¼Œåˆ™æ•´ä¸ªå§”æ‰˜è¢«æ ‡è®°ä¸ºæ­£ä¾‹
                aggregated_label = group[
                    "y_label"
                ].max()  # ä»»ä½•äº‹ä»¶è§¦å‘ -> æ•´ä¸ªå§”æ‰˜ä¸ºæ­£ä¾‹

                # æ›´æ–°æ ‡ç­¾
                first_row["y_label"] = aggregated_label

                order_features.append(first_row)

                # æ›´æ–°è¿›åº¦ï¼ˆæ¯100ä¸ªå§”æ‰˜æ›´æ–°ä¸€æ¬¡ä»¥æé«˜æ€§èƒ½ï¼‰
                if i % 100 == 0 or i == total_orders - 1:
                    progress.update(agg_task, completed=i + 1)

            # ä»»åŠ¡4: ä¿å­˜æ•°æ®
            save_task = progress.add_task(
                f"[magenta]{date_str}[/magenta] Saving files...", total=100
            )

            # è½¬æ¢ä¸ºDataFrame
            df_orders = pd.DataFrame(order_features)
            df_orders = remove_leakage_features(df_orders)

            if df_orders.empty:
                return 0, 0

            feat_cols = ["è‡ªç„¶æ—¥", "äº¤æ˜“æ‰€å§”æ‰˜å·", "ticker", *REALTIME_FEATURES]

            feat_dir.mkdir(exist_ok=True)
            lbl_dir.mkdir(exist_ok=True)
            progress.update(save_task, advance=30)

            df_orders[feat_cols].to_parquet(
                feat_dir / f"X_{date_str}.parquet", index=False
            )
            progress.update(save_task, advance=35)

            df_orders[
                ["è‡ªç„¶æ—¥", "äº¤æ˜“æ‰€å§”æ‰˜å·", "ticker", "y_label"]
            ].to_parquet(  # æ·»åŠ æ—¥æœŸåˆ°æ ‡ç­¾æ–‡ä»¶
                lbl_dir / f"labels_{date_str}.parquet", index=False
            )
            progress.update(save_task, advance=35)

            return len(df_orders), int(df_orders["y_label"].sum())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--root", required=True, help="event_stream æ ¹ç›®å½•")
    parser.add_argument("--tickers", nargs="*", default=None, help="è‚¡ç¥¨ç™½åå•")
    parser.add_argument("--r1_ms", type=int, default=50)
    parser.add_argument("--r2_ms", type=int, default=1000)
    parser.add_argument("--r2_mult", type=float, default=4.0)
    parser.add_argument(
        "--backend",
        choices=["pandas", "polars"],
        default="polars",
        help="é€‰æ‹©è®¡ç®—åç«¯ï¼špandas æˆ– polars",
    )
    parser.add_argument(
        "--max_workers",
        type=int,
        default=None,
        help="è¿›ç¨‹æ± æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°ï¼ˆä»… polars åç«¯æœ‰æ•ˆï¼‰",
    )
    args = parser.parse_args()

    evt_root = Path(args.root)
    parent = evt_root.parent
    feat_dir = parent / "features_select"
    lbl_dir = parent / "labels_select"
    watch = set(args.tickers) if args.tickers else set()

    date_dirs = [
        p for p in evt_root.iterdir() if p.is_dir() and re.fullmatch(r"\d{8}", p.name)
    ]
    if not date_dirs:
        sys.exit("[ERR] no date folders under event_stream")

    # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
    console.print(f"\n[bold green]ğŸš€ Starting ETL Pipeline[/bold green]")
    console.print(f"[dim]Root: {evt_root}[/dim]")
    console.print(f"[dim]Backend: {args.backend}[/dim]")
    console.print(f"[dim]Tickers: {list(watch) if watch else 'All'}[/dim]")
    console.print(f"[dim]Found {len(date_dirs)} date directories[/dim]\n")

    total_rows = 0
    total_positives = 0
    successful_days = 0

    if args.backend == "polars" and args.max_workers:
        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†å¤šä¸ªæ—¥æœŸ
        with ProcessPoolExecutor(max_workers=args.max_workers) as executor:
            futures = []
            for d in sorted(date_dirs):
                evt_csv = d / "å§”æ‰˜äº‹ä»¶æµ.csv"
                if not evt_csv.exists():
                    rprint(f"[red]Ã— {d.name}: missing csv[/red]")
                    continue

                futures.append(
                    executor.submit(
                        process_one_day,
                        evt_csv,
                        feat_dir,
                        lbl_dir,
                        watch,
                        args.r1_ms,
                        args.r2_ms,
                        args.r2_mult,
                        args.backend,
                    )
                )

            for d, future in zip(sorted(date_dirs), futures):
                try:
                    rows, pos = future.result()
                    if rows:
                        rprint(
                            f"[green]âœ“ {d.name}: orders={rows:,}  positive={pos}[/green]"
                        )
                        total_rows += rows
                        total_positives += pos
                        successful_days += 1
                    else:
                        rprint(f"[yellow]Ã— {d.name}: no watch-list rows[/yellow]")
                except Exception as e:
                    rprint(
                        f"[red]Ã— {d.name}: ERROR - {str(e).replace('[', '\\[').replace(']', '\\]')}[/red]"
                    )
    else:
        # ä¸²è¡Œå¤„ç†
        for d in sorted(date_dirs):
            evt_csv = d / "å§”æ‰˜äº‹ä»¶æµ.csv"
            if not evt_csv.exists():
                rprint(f"[red]Ã— {d.name}: missing csv[/red]")
                continue

            try:
                rows, pos = process_one_day(
                    evt_csv,
                    feat_dir,
                    lbl_dir,
                    watch,
                    args.r1_ms,
                    args.r2_ms,
                    args.r2_mult,
                    args.backend,
                )
                if rows:
                    rprint(
                        f"[green]âœ“ {d.name}: orders={rows:,}  positive={pos}[/green]"
                    )
                    total_rows += rows
                    total_positives += pos
                    successful_days += 1
                else:
                    rprint(f"[yellow]Ã— {d.name}: no watch-list rows[/yellow]")
            except Exception as e:
                rprint(
                    f"[red]Ã— {d.name}: ERROR - {str(e).replace('[', '\\[').replace(']', '\\]')}[/red]"
                )
                continue

    # æ˜¾ç¤ºæ€»ç»“
    console.print(f"\n[bold cyan]ğŸ“Š ETL Pipeline Summary[/bold cyan]")
    console.print(f"  Backend: {args.backend}")
    console.print(f"  Processed days: {successful_days}/{len(date_dirs)}")
    console.print(f"  Total orders: {total_rows:,}")
    console.print(f"  Positive labels: {total_positives:,}")
    if total_rows > 0:
        positive_rate = total_positives / total_rows * 100
        console.print(f"  Positive rate: {positive_rate:.4f}%")
    console.print(f"[bold green]âœ… ETL Pipeline completed![/bold green]\n")


if __name__ == "__main__":
    main()

"""
# ä½¿ç”¨ Polars åç«¯ï¼ˆæ¨èï¼‰
python scripts/data_process/run_etl_from_event.py \
    --root "/obs/users/fenglang/general/Spoofing Detect/data/event_stream" \
    --tickers 000989.SZ 300233.SZ \
    --r1_ms 50 --r2_ms 1000 --r2_mult 1.5 \
    --backend polars \
    --max_workers 10  # å¯é€‰ï¼šè¿›ç¨‹çº§å¹¶è¡Œ

# ä½¿ç”¨ Pandas åç«¯ï¼ˆå…¼å®¹ï¼‰
python scripts/data_process/run_etl_from_event.py \
    --root "/obs/users/fenglang/general/Spoofing Detect/data/event_stream" \
    --tickers 000989.SZ 300233.SZ \
    --r1_ms 50 --r2_ms 1000 --r2_mult 4.0 \
    --backend pandas
"""
