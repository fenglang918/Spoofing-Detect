#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ•°æ®æ³„éœ²è¯Šæ–­å·¥å…·
================
åˆ†æå½“å‰æ•°æ®é›†ä¸­çš„æ½œåœ¨æ•°æ®æ³„éœ²é—®é¢˜ï¼Œå¹¶æä¾›ä¿®å¤å»ºè®®
"""

import argparse
import pandas as pd
import numpy as np
import glob
import os
from pathlib import Path
import seaborn as sns
import matplotlib.pyplot as plt
from rich.console import Console
from rich.table import Table
from rich import print as rprint

console = Console()

def load_data(data_root):
    """åŠ è½½ç‰¹å¾å’Œæ ‡ç­¾æ•°æ®"""
    feat_pats = [os.path.join(data_root, "features_select", "X_*.parquet")]
    lab_pats = [os.path.join(data_root, "labels_select", "labels_*.parquet")]
    
    feat_files = []
    for pat in feat_pats:
        feat_files.extend(sorted(glob.glob(pat)))
    
    lab_files = []
    for pat in lab_pats:
        lab_files.extend(sorted(glob.glob(pat)))
        
    if not feat_files or not lab_files:
        raise FileNotFoundError("æ‰¾ä¸åˆ°ç‰¹å¾æˆ–æ ‡ç­¾æ–‡ä»¶")
    
    console.print(f"å‘ç° {len(feat_files)} ä¸ªç‰¹å¾æ–‡ä»¶å’Œ {len(lab_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶")
    
    df_feat = pd.concat([pd.read_parquet(f) for f in feat_files], ignore_index=True)
    df_lab = pd.concat([pd.read_parquet(f) for f in lab_files], ignore_index=True)
    
    # åˆå¹¶æ•°æ®
    df = df_feat.merge(df_lab, on=["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"], how="inner")
    
    return df

def identify_leakage_features(df):
    """è¯†åˆ«å¯èƒ½åŒ…å«æ•°æ®æ³„éœ²çš„ç‰¹å¾"""
    
    leakage_features = {
        "ç›´æ¥æœªæ¥ä¿¡æ¯": [
            "å­˜æ´»æ—¶é—´_ms", "final_survival_time_ms"
        ],
        "æˆäº¤ç›¸å…³ï¼ˆå§”æ‰˜æ—¶åˆ»æœªçŸ¥ï¼‰": [
            "total_traded_qty", "num_trades", "is_fully_filled"
        ],
        "èšåˆç»Ÿè®¡ï¼ˆåŒ…å«æœªæ¥äº‹ä»¶ï¼‰": [
            "total_events", "num_cancels"
        ],
        "æ’¤å•æ ‡å¿—ï¼ˆå§”æ‰˜æ—¶åˆ»æœªçŸ¥ï¼‰": [
            "is_cancel"
        ],
        "å…¶ä»–å¯ç–‘ç‰¹å¾": [
            "cancellation_flag", "fill_ratio", "execution_time"
        ]
    }
    
    safe_features = [
        # è¡Œæƒ…ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "bid1", "ask1", "prev_close", "mid_price", "spread",
        
        # ä»·æ ¼ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "delta_mid", "pct_spread", "price_dev_prevclose",
        
        # è®¢å•ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "is_buy", "log_qty", "å§”æ‰˜ä»·æ ¼", "å§”æ‰˜æ•°é‡",
        
        # å†å²ç»Ÿè®¡ç‰¹å¾ï¼ˆåªä½¿ç”¨è¿‡å»çš„ä¿¡æ¯ï¼‰
        "orders_100ms", "cancels_5s", 
        
        # æ—¶é—´ç‰¹å¾
        "time_sin", "time_cos", "in_auction"
    ]
    
    return leakage_features, safe_features

def analyze_feature_target_correlation(df, feature, target="y_label"):
    """åˆ†æç‰¹å¾ä¸æ ‡ç­¾çš„ç›¸å…³æ€§ï¼ˆç”¨äºæ£€æµ‹æ³„éœ²ï¼‰"""
    if feature not in df.columns:
        return None
    
    # è®¡ç®—ç›¸å…³æ€§
    correlation = df[feature].corr(df[target])
    
    # è®¡ç®—äº’ä¿¡æ¯ï¼ˆå¯¹äºåˆ†ç±»å˜é‡ï¼‰
    try:
        from sklearn.feature_selection import mutual_info_classif
        mi = mutual_info_classif(df[[feature]].fillna(0), df[target], random_state=42)[0]
    except:
        mi = None
    
    return {
        'correlation': correlation,
        'mutual_info': mi,
        'unique_values': df[feature].nunique(),
        'missing_rate': df[feature].isnull().mean()
    }

def temporal_analysis(df):
    """æ—¶é—´åºåˆ—åˆ†æ"""
    console.print("\nğŸ•’ æ—¶é—´åºåˆ—åˆ†æ")
    
    # æŒ‰æ—¥æœŸç»Ÿè®¡
    date_stats = df.groupby('è‡ªç„¶æ—¥').agg({
        'y_label': ['count', 'sum', 'mean']
    }).round(4)
    
    date_stats.columns = ['æ€»æ ·æœ¬æ•°', 'æ­£æ ·æœ¬æ•°', 'æ­£æ ·æœ¬ç‡']
    
    table = Table(title="æŒ‰æ—¥æœŸç»Ÿè®¡")
    table.add_column("æ—¥æœŸ", style="cyan")
    table.add_column("æ€»æ ·æœ¬æ•°", style="magenta")
    table.add_column("æ­£æ ·æœ¬æ•°", style="yellow")
    table.add_column("æ­£æ ·æœ¬ç‡", style="green")
    
    for date, row in date_stats.iterrows():
        table.add_row(
            str(date),
            f"{int(row['æ€»æ ·æœ¬æ•°']):,}",
            f"{int(row['æ­£æ ·æœ¬æ•°']):,}",
            f"{row['æ­£æ ·æœ¬ç‡']:.4f}"
        )
    
    console.print(table)
    
    return date_stats

def feature_importance_analysis(df, target="y_label"):
    """ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆåŸºäºç®€å•ç›¸å…³æ€§ï¼‰"""
    console.print("\nğŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†æ")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    feature_cols = [col for col in numeric_cols if col not in ["è‡ªç„¶æ—¥", "y_label"]]
    
    correlations = []
    for col in feature_cols:
        corr = analyze_feature_target_correlation(df, col, target)
        if corr:
            correlations.append({
                'feature': col,
                'correlation': abs(corr['correlation']),
                'mutual_info': corr['mutual_info'] or 0,
                'unique_values': corr['unique_values'],
                'missing_rate': corr['missing_rate']
            })
    
    # æ’åºç‰¹å¾
    corr_df = pd.DataFrame(correlations).sort_values('correlation', ascending=False)
    
    # æ˜¾ç¤ºtopç‰¹å¾
    table = Table(title="ç‰¹å¾-æ ‡ç­¾ç›¸å…³æ€§ Top 15")
    table.add_column("ç‰¹å¾", style="cyan")
    table.add_column("ç›¸å…³æ€§", style="magenta")
    table.add_column("å”¯ä¸€å€¼æ•°", style="yellow")
    table.add_column("ç¼ºå¤±ç‡", style="green")
    
    for _, row in corr_df.head(15).iterrows():
        table.add_row(
            row['feature'],
            f"{row['correlation']:.4f}",
            f"{int(row['unique_values']):,}",
            f"{row['missing_rate']:.4f}"
        )
    
    console.print(table)
    
    return corr_df

def detect_perfect_predictors(df, target="y_label", threshold=0.95):
    """æ£€æµ‹å®Œç¾é¢„æµ‹ç‰¹å¾ï¼ˆå¯èƒ½çš„æ•°æ®æ³„éœ²ï¼‰"""
    console.print(f"\nğŸš¨ æ£€æµ‹å®Œç¾é¢„æµ‹ç‰¹å¾ï¼ˆç›¸å…³æ€§ > {threshold}ï¼‰")
    
    leakage_features, _ = identify_leakage_features(df)
    all_leakage = []
    for category, features in leakage_features.items():
        all_leakage.extend(features)
    
    perfect_predictors = []
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        if col == target:
            continue
        
        corr = df[col].corr(df[target])
        if abs(corr) > threshold:
            is_known_leakage = col in all_leakage
            perfect_predictors.append({
                'feature': col,
                'correlation': corr,
                'is_known_leakage': is_known_leakage
            })
    
    if perfect_predictors:
        table = Table(title="ğŸš¨ å®Œç¾é¢„æµ‹ç‰¹å¾ï¼ˆç–‘ä¼¼æ•°æ®æ³„éœ²ï¼‰")
        table.add_column("ç‰¹å¾", style="cyan")
        table.add_column("ç›¸å…³æ€§", style="red")
        table.add_column("å·²çŸ¥æ³„éœ²", style="yellow")
        
        for pred in perfect_predictors:
            table.add_row(
                pred['feature'],
                f"{pred['correlation']:.6f}",
                "âœ…" if pred['is_known_leakage'] else "â“"
            )
        
        console.print(table)
    else:
        console.print("âœ… æœªå‘ç°å®Œç¾é¢„æµ‹ç‰¹å¾")
    
    return perfect_predictors

def analyze_survival_time_distribution(df):
    """åˆ†æå­˜æ´»æ—¶é—´åˆ†å¸ƒ"""
    if "å­˜æ´»æ—¶é—´_ms" not in df.columns and "final_survival_time_ms" not in df.columns:
        console.print("âš ï¸ æœªæ‰¾åˆ°å­˜æ´»æ—¶é—´ç‰¹å¾")
        return
    
    console.print("\nâ±ï¸ å­˜æ´»æ—¶é—´åˆ†æ")
    
    survival_col = "å­˜æ´»æ—¶é—´_ms" if "å­˜æ´»æ—¶é—´_ms" in df.columns else "final_survival_time_ms"
    
    # æŒ‰æ ‡ç­¾åˆ†ç»„ç»Ÿè®¡å­˜æ´»æ—¶é—´
    survival_stats = df.groupby('y_label')[survival_col].describe()
    
    console.print("å­˜æ´»æ—¶é—´ç»Ÿè®¡ï¼ˆæŒ‰æ ‡ç­¾åˆ†ç»„ï¼‰:")
    console.print(survival_stats)
    
    # è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„æ ‡ç­¾åˆ†å¸ƒ
    thresholds = [50, 100, 500, 1000, 2000, 5000]
    
    table = Table(title="å­˜æ´»æ—¶é—´é˜ˆå€¼åˆ†æ")
    table.add_column("é˜ˆå€¼(ms)", style="cyan")
    table.add_column("< é˜ˆå€¼æ ·æœ¬æ•°", style="magenta")
    table.add_column("< é˜ˆå€¼æ­£æ ·æœ¬ç‡", style="yellow")
    table.add_column(">= é˜ˆå€¼æ ·æœ¬æ•°", style="green")
    table.add_column(">= é˜ˆå€¼æ­£æ ·æœ¬ç‡", style="blue")
    
    for threshold in thresholds:
        below_threshold = df[df[survival_col] < threshold]
        above_threshold = df[df[survival_col] >= threshold]
        
        below_pos_rate = below_threshold['y_label'].mean() if len(below_threshold) > 0 else 0
        above_pos_rate = above_threshold['y_label'].mean() if len(above_threshold) > 0 else 0
        
        table.add_row(
            str(threshold),
            f"{len(below_threshold):,}",
            f"{below_pos_rate:.4f}",
            f"{len(above_threshold):,}",
            f"{above_pos_rate:.4f}"
        )
    
    console.print(table)

def generate_recommendations(df):
    """ç”Ÿæˆä¿®å¤å»ºè®®"""
    console.print("\nğŸ’¡ ä¿®å¤å»ºè®®")
    
    leakage_features, safe_features = identify_leakage_features(df)
    
    # æ£€æŸ¥å½“å‰æ•°æ®é›†ä¸­å­˜åœ¨çš„æ³„éœ²ç‰¹å¾
    existing_leakage = []
    for category, features in leakage_features.items():
        existing = [f for f in features if f in df.columns]
        if existing:
            existing_leakage.extend(existing)
    
    if existing_leakage:
        console.print("ğŸš« [bold red]å‘ç°ä»¥ä¸‹æ•°æ®æ³„éœ²ç‰¹å¾ï¼Œå¿…é¡»ç§»é™¤ï¼š[/bold red]")
        for feat in existing_leakage:
            console.print(f"   â€¢ {feat}")
    
    # æ£€æŸ¥å®‰å…¨ç‰¹å¾
    existing_safe = [f for f in safe_features if f in df.columns]
    console.print(f"\nâœ… [bold green]å¯ä»¥å®‰å…¨ä½¿ç”¨çš„ç‰¹å¾ ({len(existing_safe)} ä¸ª)ï¼š[/bold green]")
    for feat in existing_safe:
        console.print(f"   â€¢ {feat}")
    
    missing_safe = [f for f in safe_features if f not in df.columns]
    if missing_safe:
        console.print(f"\nâš ï¸ [bold yellow]å»ºè®®æ·»åŠ çš„å®‰å…¨ç‰¹å¾ï¼š[/bold yellow]")
        for feat in missing_safe:
            console.print(f"   â€¢ {feat}")
    
    console.print("\nğŸ“‹ [bold cyan]å…·ä½“ä¿®å¤æ­¥éª¤ï¼š[/bold cyan]")
    console.print("1. é‡æ–°è¿è¡Œç‰¹å¾å·¥ç¨‹ï¼Œç§»é™¤æ‰€æœ‰åŒ…å«æœªæ¥ä¿¡æ¯çš„ç‰¹å¾")
    console.print("2. ç¡®ä¿æ ‡ç­¾æ„é€ åªåŸºäºå§”æ‰˜åçš„çœŸå®ç»“æœï¼ˆä¸æ˜¯é¢„æµ‹ç»“æœï¼‰")
    console.print("3. å®æ–½ä¸¥æ ¼çš„æ—¶é—´åºåˆ—éªŒè¯ï¼ˆè®­ç»ƒé›†æ—¥æœŸ < éªŒè¯é›†æ—¥æœŸï¼‰")
    console.print("4. æ·»åŠ æ­£åˆ™åŒ–å’Œæ—©åœï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ")
    console.print("5. ä½¿ç”¨æˆ‘æä¾›çš„ä¿®å¤ç‰ˆè®­ç»ƒè„šæœ¬: scripts/train/train_baseline_fixed.py")

def main():
    parser = argparse.ArgumentParser(description="æ•°æ®æ³„éœ²è¯Šæ–­å·¥å…·")
    parser.add_argument("--data_root", required=True, 
                      help="æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--output_dir", default="./leakage_analysis", 
                      help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(exist_ok=True)
    
    console.print("ğŸ” [bold blue]å¼€å§‹æ•°æ®æ³„éœ²è¯Šæ–­...[/bold blue]")
    
    # åŠ è½½æ•°æ®
    try:
        df = load_data(args.data_root)
        console.print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼š{df.shape}")
    except Exception as e:
        console.print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼š{e}")
        return
    
    # åŸºæœ¬ç»Ÿè®¡
    console.print(f"\nğŸ“ˆ æ•°æ®æ¦‚è§ˆï¼š")
    console.print(f"   â€¢ æ€»æ ·æœ¬æ•°ï¼š{len(df):,}")
    console.print(f"   â€¢ ç‰¹å¾æ•°ï¼š{len(df.columns) - 4}")  # å‡å»IDåˆ—
    console.print(f"   â€¢ æ­£æ ·æœ¬æ•°ï¼š{df['y_label'].sum():,}")
    console.print(f"   â€¢ æ­£æ ·æœ¬ç‡ï¼š{df['y_label'].mean():.6f}")
    
    # æ—¶é—´åºåˆ—åˆ†æ
    date_stats = temporal_analysis(df)
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    corr_df = feature_importance_analysis(df)
    
    # æ£€æµ‹å®Œç¾é¢„æµ‹ç‰¹å¾
    perfect_predictors = detect_perfect_predictors(df)
    
    # å­˜æ´»æ—¶é—´åˆ†æ
    analyze_survival_time_distribution(df)
    
    # ç”Ÿæˆå»ºè®®
    generate_recommendations(df)
    
    # ä¿å­˜åˆ†æç»“æœ
    try:
        corr_df.to_csv(Path(args.output_dir) / "feature_correlations.csv", index=False)
        date_stats.to_csv(Path(args.output_dir) / "temporal_stats.csv")
        console.print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°ï¼š{args.output_dir}")
    except Exception as e:
        console.print(f"âš ï¸ ä¿å­˜å¤±è´¥ï¼š{e}")
    
    console.print("\nğŸ¯ [bold green]è¯Šæ–­å®Œæˆï¼è¯·æŸ¥çœ‹ä¸Šè¿°å»ºè®®å¹¶ä½¿ç”¨ä¿®å¤ç‰ˆè®­ç»ƒè„šæœ¬ã€‚[/bold green]")

if __name__ == "__main__":
    main()

"""
# ä½¿ç”¨ç¤ºä¾‹
python scripts/analyze_data_leakage.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --output_dir "./leakage_analysis"
""" 