#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Training with Enhanced Labels (Correct Implementation)
----------------------------------------------------
æ­£ç¡®å®ç°ï¼š
â€¢ æ ‡ç­¾å¯ä»¥ä½¿ç”¨ä»»ä½•ä¿¡æ¯ï¼ˆåŒ…æ‹¬æœªæ¥ä¿¡æ¯ï¼‰æ¥å®šä¹‰è™šå‡æŠ¥å•
â€¢ ç‰¹å¾åªä½¿ç”¨å§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ä¿¡æ¯ï¼Œç§»é™¤æ•°æ®æ³„éœ²
â€¢ å¯¹æ¯”åŸå§‹æ ‡ç­¾ vs å¢å¼ºæ ‡ç­¾çš„æ•ˆæœ
"""

import argparse, glob, os, time, warnings
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import average_precision_score, classification_report, confusion_matrix, roc_auc_score
from pathlib import Path
import json

warnings.filterwarnings('ignore')

def get_safe_features():
    """è·å–å®‰å…¨çš„ï¼ˆæ— æ•°æ®æ³„éœ²ï¼‰ç‰¹å¾åˆ—è¡¨"""
    return [
        # è¡Œæƒ…ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "bid1", "ask1", "prev_close", "mid_price", "spread",
        
        # ä»·æ ¼ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "delta_mid", "pct_spread", "price_dev_prevclose",
        
        # è®¢å•ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "is_buy", "log_qty",
        
        # å†å²ç»Ÿè®¡ç‰¹å¾ï¼ˆåªä½¿ç”¨è¿‡å»çš„ä¿¡æ¯ï¼‰
        "orders_100ms", "cancels_5s", 
        
        # æ—¶é—´ç‰¹å¾
        "time_sin", "time_cos", "in_auction",
        
        # å¢å¼ºç‰¹å¾ï¼ˆåŸºäºå§”æ‰˜æ—¶åˆ»çš„ä¿¡æ¯ï¼‰
        "book_imbalance", "price_aggressiveness"
    ]

def get_leakage_features():
    """è¯†åˆ«åŒ…å«æ•°æ®æ³„éœ²çš„ç‰¹å¾ï¼ˆéœ€è¦ç§»é™¤ï¼‰"""
    return [
        # æ˜æ˜¾çš„æœªæ¥ä¿¡æ¯ç‰¹å¾
        "final_survival_time_ms",  # æœ€ç»ˆå­˜æ´»æ—¶é—´
        "total_events",           # æ€»äº‹ä»¶æ•°
        "total_traded_qty",       # æ€»æˆäº¤é‡
        "num_trades",            # æˆäº¤æ¬¡æ•°
        "num_cancels",           # æ’¤å•æ¬¡æ•°
        "is_fully_filled",       # æ˜¯å¦å®Œå…¨æˆäº¤
        
        # å¯èƒ½åŒ…å«æœªæ¥ä¿¡æ¯çš„èšåˆç‰¹å¾
        "å­˜æ´»æ—¶é—´_ms",           # å­˜æ´»æ—¶é—´ï¼ˆåœ¨ç‰¹å¾ä¸­æ˜¯æ³„éœ²ï¼‰
        "layering_score"         # å¦‚æœåŸºäºæœªæ¥ä¿¡æ¯è®¡ç®—
    ]

def analyze_data_quality(df):
    """åˆ†ææ•°æ®è´¨é‡å’Œæ ‡ç­¾åˆ†å¸ƒ"""
    print("\nğŸ“Š Data Quality Analysis:")
    print(f"Total samples: {len(df):,}")
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    label_cols = [col for col in df.columns if any(x in col for x in ['spoofing', 'y_label', 'quick_', 'price_', 'fake_', 'layering_', 'active_'])]
    
    print(f"\nğŸ·ï¸ Label Distribution:")
    for col in sorted(label_cols):
        if col in df.columns:
            pos_count = df[col].sum()
            pos_rate = pos_count / len(df) * 100
            print(f"  {col:<30}: {pos_count:>8,} ({pos_rate:>6.3f}%)")
    
    # ç‰¹å¾æ³„éœ²æ£€æŸ¥
    safe_features = get_safe_features()
    leakage_features = get_leakage_features()
    
    available_safe = [f for f in safe_features if f in df.columns]
    found_leakage = [f for f in leakage_features if f in df.columns]
    
    print(f"\nâœ… Safe features available: {len(available_safe)}")
    print(f"âš ï¸ Leakage features found: {len(found_leakage)}")
    
    if found_leakage:
        print(f"  Removing: {found_leakage}")
    
    return available_safe, found_leakage

def comprehensive_evaluation(y_true, y_pred_proba, label_name=""):
    """ç»¼åˆè¯„ä¼°å‡½æ•°"""
    metrics = {}
    
    # åŸºç¡€æŒ‡æ ‡
    pr_auc = average_precision_score(y_true, y_pred_proba)
    roc_auc = roc_auc_score(y_true, y_pred_proba)
    
    metrics['PR-AUC'] = pr_auc
    metrics['ROC-AUC'] = roc_auc
    
    # Precision at K
    for k in [0.001, 0.005, 0.01, 0.05]:
        k_int = max(1, int(len(y_true) * k))
        top_k_idx = y_pred_proba.argsort()[::-1][:k_int]
        prec_k = y_true.iloc[top_k_idx].mean() if hasattr(y_true, 'iloc') else y_true[top_k_idx].mean()
        metrics[f'Precision@{k*100:.1f}%'] = prec_k
    
    # æ‰“å°ç»“æœ
    print(f"\nğŸ“Š {label_name} Results:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.6f}")
    
    return metrics

def train_model(X_train, y_train, X_valid, y_valid, label_name, balance_ratio=20):
    """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
    print(f"\nğŸš€ Training with {label_name}...")
    
    # æ£€æŸ¥æ­£æ ·æœ¬
    pos_count = y_train.sum()
    if pos_count == 0:
        print(f"  âš ï¸ No positive samples for {label_name}")
        return None, None
    
    print(f"  Positive samples: {pos_count:,} ({pos_count/len(y_train)*100:.3f}%)")
    
    # æ•°æ®å¹³è¡¡
    pos_indices = y_train[y_train == 1].index
    neg_indices = y_train[y_train == 0].index
    
    if len(neg_indices) > 0:
        target_neg_size = min(len(pos_indices) * balance_ratio, len(neg_indices))
        selected_neg_indices = np.random.choice(neg_indices, target_neg_size, replace=False)
        
        selected_indices = np.concatenate([pos_indices, selected_neg_indices])
        X_train_balanced = X_train.loc[selected_indices]
        y_train_balanced = y_train.loc[selected_indices]
        
        print(f"  Balanced: {y_train_balanced.value_counts().to_dict()}")
    else:
        X_train_balanced, y_train_balanced = X_train, y_train
    
    # æ¨¡å‹è®­ç»ƒ
    model = lgb.LGBMClassifier(
        objective='binary',
        metric='average_precision',
        learning_rate=0.05,
        num_leaves=31,
        max_depth=6,
        n_estimators=1000,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=10,
        reg_lambda=10,
        random_state=42,
        verbose=-1
    )
    
    # è®­ç»ƒ
    try:
        model.fit(
            X_train_balanced, y_train_balanced,
            eval_set=[(X_valid, y_valid)],
            early_stopping_rounds=100,
            verbose=False
        )
    except TypeError:
        from lightgbm import early_stopping
        model.fit(
            X_train_balanced, y_train_balanced,
            eval_set=[(X_valid, y_valid)],
            callbacks=[early_stopping(100)]
        )
    
    # é¢„æµ‹å’Œè¯„ä¼°
    y_pred_proba = model.predict_proba(X_valid)[:, 1]
    metrics = comprehensive_evaluation(y_valid, y_pred_proba, label_name)
    
    return model, metrics

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_root", required=True, help="æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--train_regex", default="202503|202504", help="è®­ç»ƒæ•°æ®æ—¥æœŸæ­£åˆ™")
    parser.add_argument("--valid_regex", default="202505", help="éªŒè¯æ•°æ®æ—¥æœŸæ­£åˆ™")
    parser.add_argument("--results_dir", help="ç»“æœä¿å­˜ç›®å½•")
    
    args = parser.parse_args()
    
    print("ğŸš€ Training with Enhanced Labels (Correct Implementation)")
    print("=" * 70)
    
    data_root = Path(args.data_root)
    
    # è®¾ç½®ç»“æœç›®å½•
    if args.results_dir:
        results_dir = Path(args.results_dir)
    else:
        results_dir = data_root / "enhanced_label_results_correct"
    results_dir.mkdir(exist_ok=True)
    
    print(f"ğŸ“ Data root: {data_root}")
    print(f"ğŸ“ Results dir: {results_dir}")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“¥ Loading data...")
    
    # ç‰¹å¾æ•°æ®
    feat_files = list((data_root / "features_select").glob("X_*.parquet"))
    if not feat_files:
        print("âŒ No feature files found")
        return
    df_features = pd.concat([pd.read_parquet(f) for f in feat_files], ignore_index=True)
    print(f"Features shape: {df_features.shape}")
    
    # æ ‡ç­¾æ•°æ®
    label_files = list((data_root / "labels_select").glob("labels_*.parquet"))
    if not label_files:
        print("âŒ No label files found")
        return
    df_labels = pd.concat([pd.read_parquet(f) for f in label_files], ignore_index=True)
    print(f"Labels shape: {df_labels.shape}")
    
    # åˆå¹¶æ•°æ®
    df = df_features.merge(df_labels, on=['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·'], how='inner')
    print(f"Merged shape: {df.shape}")
    
    # æ•°æ®è´¨é‡åˆ†æ
    safe_features, leakage_features = analyze_data_quality(df)
    
    # ç§»é™¤æ³„éœ²ç‰¹å¾
    df_clean = df.drop(columns=leakage_features, errors='ignore')
    print(f"\nAfter removing leakage features: {df_clean.shape}")
    
    # æ•°æ®åˆ‡åˆ†
    train_mask = df_clean["è‡ªç„¶æ—¥"].astype(str).str.contains(args.train_regex)
    valid_mask = df_clean["è‡ªç„¶æ—¥"].astype(str).str.contains(args.valid_regex)
    
    df_train = df_clean[train_mask].copy()
    df_valid = df_clean[valid_mask].copy()
    
    print(f"\nğŸ“… Data split:")
    print(f"Training: {len(df_train):,} samples")
    print(f"Validation: {len(df_valid):,} samples")
    
    # å‡†å¤‡ç‰¹å¾
    feature_cols = [f for f in safe_features if f in df_clean.columns]
    print(f"\nğŸ“Š Using {len(feature_cols)} safe features:")
    for i, feat in enumerate(feature_cols, 1):
        print(f"  {i:2d}. {feat}")
    
    X_train = df_train[feature_cols].fillna(0)
    X_valid = df_valid[feature_cols].fillna(0)
    
    # è®­ç»ƒä¸åŒæ ‡ç­¾ç­–ç•¥
    results = {}
    
    # 1. åŸå§‹æ ‡ç­¾
    if 'y_label' in df_train.columns:
        model, metrics = train_model(
            X_train, df_train['y_label'], 
            X_valid, df_valid['y_label'],
            "Original Labels"
        )
        if metrics:
            results['original'] = metrics
    
    # 2. å¢å¼ºæ ‡ç­¾ - å„ç§ç­–ç•¥
    enhanced_labels = {
        'composite_spoofing': 'Enhanced Composite',
        'conservative_spoofing': 'Enhanced Conservative', 
        'quick_cancel_impact': 'Quick Cancel Impact',
        'price_manipulation': 'Price Manipulation',
        'fake_liquidity': 'Fake Liquidity',
        'layering_cancel': 'Layering Cancel',
        'active_hours_spoofing': 'Active Hours'
    }
    
    for label_col, label_name in enhanced_labels.items():
        if label_col in df_train.columns:
            model, metrics = train_model(
                X_train, df_train[label_col],
                X_valid, df_valid[label_col], 
                label_name
            )
            if metrics:
                results[label_col] = metrics
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    if results:
        print("\nğŸ“Š Enhanced Labels Performance Comparison")
        print("=" * 80)
        
        comparison_df = pd.DataFrame(results).T
        
        # ä¸»è¦æŒ‡æ ‡
        key_metrics = ['PR-AUC', 'ROC-AUC', 'Precision@0.1%', 'Precision@1.0%']
        print(f"\nğŸ¯ Key Metrics:")
        print(comparison_df[key_metrics].round(6))
        
        # æœ€ä½³ç­–ç•¥
        best_pr_auc = comparison_df['PR-AUC'].idxmax()
        best_prec = comparison_df['Precision@0.1%'].idxmax()
        
        print(f"\nğŸ† Best Strategies:")
        print(f"  Best PR-AUC: {best_pr_auc} ({comparison_df.loc[best_pr_auc, 'PR-AUC']:.6f})")
        print(f"  Best Precision@0.1%: {best_prec} ({comparison_df.loc[best_prec, 'Precision@0.1%']:.6f})")
        
        # æ”¹è¿›åˆ†æ
        if 'original' in results and 'composite_spoofing' in results:
            orig_pr = results['original']['PR-AUC'] 
            enh_pr = results['composite_spoofing']['PR-AUC']
            improvement = (enh_pr - orig_pr) / orig_pr * 100
            
            print(f"\nğŸ“ˆ Enhancement Analysis:")
            print(f"  Original PR-AUC: {orig_pr:.6f}")
            print(f"  Enhanced PR-AUC: {enh_pr:.6f}")
            print(f"  Improvement: {improvement:+.1f}%")
        
        # ä¿å­˜ç»“æœ
        comparison_df.to_csv(results_dir / "enhanced_labels_comparison.csv", float_format='%.8f')
        
        with open(results_dir / "enhanced_labels_results.json", 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ Results saved to: {results_dir}")
        
        # æ ‡ç­¾æ”¹è¿›æ€»ç»“
        print(f"\nğŸ’¡ Enhanced Labeling Summary:")
        print(f"  â€¢ æ ‡ç­¾å¯ä»¥ä½¿ç”¨æœªæ¥ä¿¡æ¯å®šä¹‰è™šå‡æŠ¥å• âœ…")
        print(f"  â€¢ ç‰¹å¾ä¸¥æ ¼é™åˆ¶ä¸ºå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ âœ…")
        print(f"  â€¢ é€šè¿‡å¤šç§è§„åˆ™æ‰©å¤§æ­£æ ·æœ¬é›†åˆ âœ…")
        print(f"  â€¢ æé«˜äº†æ¨¡å‹çš„å­¦ä¹ æ ·æœ¬æ•°é‡ âœ…")
        
    else:
        print("âŒ No valid results obtained")

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# è®­ç»ƒå¹¶å¯¹æ¯”å¢å¼ºæ ‡ç­¾æ•ˆæœ
python scripts/train/train_enhanced_labels_correct.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --train_regex "202503|202504" \
  --valid_regex "202505"

# æŒ‡å®šç»“æœä¿å­˜ç›®å½•
python scripts/train/train_enhanced_labels_correct.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --results_dir "/home/ma-user/code/fenglang/Spoofing Detect/enhanced_results"
""" 