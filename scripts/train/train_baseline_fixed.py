#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LightGBM Training Pipeline - DATA LEAKAGE FIXED
-------------------------------------------------
â€¢ ç§»é™¤æ•°æ®æ³„éœ²ï¼šåªä½¿ç”¨å§”æ‰˜æ—¶åˆ»çš„å®æ—¶å¯è§‚æµ‹ç‰¹å¾
â€¢ ä¸¥æ ¼æ—¶é—´åºåˆ—éªŒè¯ï¼šè®­ç»ƒé›†åœ¨éªŒè¯é›†ä¹‹å‰
â€¢ é‡æ–°å¹³è¡¡æ•°æ®é›†ä»¥è·å¾—æ›´å¥½çš„æ³›åŒ–æ€§èƒ½
"""

import argparse, glob, os, re, sys, time, warnings
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import average_precision_score, classification_report, confusion_matrix
from sklearn.model_selection import StratifiedKFold

# Suppress warnings
warnings.filterwarnings('ignore')

# ---------- Utils ----------------------------------------------------------
def load_parquets(patterns):
    files = []
    for pat in patterns:
        files.extend(sorted(glob.glob(pat)))
    if not files:
        raise FileNotFoundError(f"No file matched {patterns}")
    return pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)

def split_by_date(df, regex):
    mask = df["è‡ªç„¶æ—¥"].astype(str).str.contains(regex)
    return df[mask], df[~mask]

def precision_at_k(y_true, y_score, k_ratio=0.001):
    k = max(1, int(len(y_true) * k_ratio))
    top_k_idx = y_score.argsort()[::-1][:k]
    return y_true.iloc[top_k_idx].mean()

def get_realtime_features():
    """åªè¿”å›åœ¨å§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹çš„ç‰¹å¾ï¼ˆæ— æœªæ¥ä¿¡æ¯ï¼‰"""
    return [
        # è¡Œæƒ…ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "bid1", "ask1", "prev_close", "mid_price", "spread",
        
        # ä»·æ ¼ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "delta_mid", "pct_spread", "price_dev_prevclose",
        
        # è®¢å•ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "is_buy", "log_qty",
        
        # å†å²ç»Ÿè®¡ç‰¹å¾ï¼ˆåªä½¿ç”¨è¿‡å»çš„ä¿¡æ¯ï¼‰
        "orders_100ms", "cancels_5s", 
        
        # æ—¶é—´ç‰¹å¾
        "time_sin", "time_cos", "in_auction"
    ]

def remove_leakage_features(df):
    """ç§»é™¤åŒ…å«æœªæ¥ä¿¡æ¯çš„ç‰¹å¾"""
    leakage_features = [
        # ç›´æ¥çš„æœªæ¥ä¿¡æ¯
        "å­˜æ´»æ—¶é—´_ms", "final_survival_time_ms",
        
        # æˆäº¤ç›¸å…³ï¼ˆå§”æ‰˜æ—¶åˆ»æœªçŸ¥ï¼‰
        "total_traded_qty", "num_trades", "is_fully_filled",
        
        # èšåˆç»Ÿè®¡ï¼ˆåŒ…å«æœªæ¥äº‹ä»¶ï¼‰
        "total_events", "num_cancels",
        
        # æ’¤å•æ ‡å¿—ï¼ˆå§”æ‰˜æ—¶åˆ»æœªçŸ¥æ˜¯å¦ä¼šæ’¤å•ï¼‰
        "is_cancel"
    ]
    
    available_features = [col for col in df.columns if col not in leakage_features]
    print(f"ç§»é™¤äº† {len(leakage_features)} ä¸ªæ³„éœ²ç‰¹å¾")
    print(f"ä¿ç•™äº† {len(available_features)} ä¸ªç‰¹å¾")
    
    return df[available_features]

def validate_temporal_order(df_train, df_valid):
    """éªŒè¯æ—¶é—´åºåˆ—é¡ºåºï¼šè®­ç»ƒé›†åº”åœ¨éªŒè¯é›†ä¹‹å‰"""
    train_max_date = df_train["è‡ªç„¶æ—¥"].max()
    valid_min_date = df_valid["è‡ªç„¶æ—¥"].min()
    
    if train_max_date >= valid_min_date:
        print(f"âš ï¸ è­¦å‘Šï¼šæ—¶é—´æ³„éœ²æ£€æµ‹ï¼è®­ç»ƒé›†æœ€å¤§æ—¥æœŸ ({train_max_date}) >= éªŒè¯é›†æœ€å°æ—¥æœŸ ({valid_min_date})")
        return False
    else:
        print(f"âœ… æ—¶é—´åºåˆ—éªŒè¯é€šè¿‡ï¼šè®­ç»ƒé›† (â‰¤{train_max_date}) â†’ éªŒè¯é›† (â‰¥{valid_min_date})")
        return True

def balance_dataset(df, target_col="y_label", method="undersample", max_ratio=10):
    """å¹³è¡¡æ•°æ®é›†ä»¥æ”¹å–„æ¨¡å‹æ³›åŒ–"""
    pos = df[df[target_col] == 1]
    neg = df[df[target_col] == 0]
    
    print(f"åŸå§‹æ•°æ®åˆ†å¸ƒï¼šæ­£æ ·æœ¬ {len(pos):,}, è´Ÿæ ·æœ¬ {len(neg):,}")
    
    if len(neg) == 0 or len(pos) == 0:
        print("âš ï¸ æ•°æ®é›†ä¸­ç¼ºå°‘æ­£æ ·æœ¬æˆ–è´Ÿæ ·æœ¬")
        return df
    
    ratio = len(neg) / len(pos)
    print(f"åŸå§‹ä¸å¹³è¡¡æ¯”ä¾‹ï¼š{ratio:.1f}:1")
    
    if ratio > max_ratio and method == "undersample":
        # ä¸‹é‡‡æ ·è´Ÿæ ·æœ¬
        target_neg_size = int(len(pos) * max_ratio)  # ç¡®ä¿æ˜¯æ•´æ•°
        neg_sampled = neg.sample(n=min(target_neg_size, len(neg)), random_state=42)
        balanced_df = pd.concat([pos, neg_sampled], ignore_index=True)
        print(f"ä¸‹é‡‡æ ·åï¼šæ­£æ ·æœ¬ {len(pos):,}, è´Ÿæ ·æœ¬ {len(neg_sampled):,}")
    else:
        balanced_df = df
        print("ä¿æŒåŸå§‹åˆ†å¸ƒ")
    
    return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

# ---------- Main -----------------------------------------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_root", required=True, help="æ ¹ç›®å½•ï¼ŒåŒ…å« features_select/ labels_select/")
    parser.add_argument("--train_regex", default="202503|202504", help="è®­ç»ƒé›†æ—¥æœŸæ­£åˆ™")
    parser.add_argument("--valid_regex", default="202505", help="éªŒè¯é›†æ—¥æœŸæ­£åˆ™")
    parser.add_argument("--device", choices=["gpu", "cpu"], default="cpu")
    parser.add_argument("--balance_method", choices=["none", "undersample"], default="undersample")
    parser.add_argument("--max_imbalance_ratio", type=float, default=20.0)
    
    # LightGBMå‚æ•°
    parser.add_argument("--learning_rate", type=float, default=0.1)
    parser.add_argument("--num_leaves", type=int, default=31)
    parser.add_argument("--max_depth", type=int, default=6)
    parser.add_argument("--n_estimators", type=int, default=1000)
    parser.add_argument("--early_stop", type=int, default=100)
    parser.add_argument("--reg_lambda", type=float, default=10.0)
    parser.add_argument("--reg_alpha", type=float, default=10.0)
    
    args = parser.parse_args()

    t0 = time.time()
    print("ğŸ” Loading data...")
    
    # Load data
    feat_pats = [os.path.join(args.data_root, "features_select", "X_*.parquet")]
    lab_pats  = [os.path.join(args.data_root, "labels_select", "labels_*.parquet")]
    df_feat = load_parquets(feat_pats)
    df_lab  = load_parquets(lab_pats)
    
    print(f"ç‰¹å¾æ•°æ®å½¢çŠ¶ï¼š{df_feat.shape}")
    print(f"æ ‡ç­¾æ•°æ®å½¢çŠ¶ï¼š{df_lab.shape}")
    
    # Merge datasets
    df = df_feat.merge(df_lab, on=["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"], how="inner", validate="one_to_one")
    print(f"åˆå¹¶åæ•°æ®å½¢çŠ¶ï¼š{df.shape}")

    # Remove data leakage
    print("\nğŸš« ç§»é™¤æ•°æ®æ³„éœ²ç‰¹å¾...")
    df = remove_leakage_features(df)

    # Train / Valid split by date
    print("\nğŸ“… æŒ‰æ—¥æœŸåˆ‡åˆ†æ•°æ®...")
    df_train, df_remaining = split_by_date(df, args.train_regex)
    df_valid, df_drop = split_by_date(df_remaining, args.valid_regex)
    
    if df_valid.empty:
        sys.exit("âŒ æ‰¾ä¸åˆ°éªŒè¯é›†æ—¥æœŸï¼Œè¯·æ£€æŸ¥ --valid_regex")
    
    print(f"è®­ç»ƒé›†ï¼š{len(df_train):,} æ ·æœ¬")
    print(f"éªŒè¯é›†ï¼š{len(df_valid):,} æ ·æœ¬")
    
    # Validate temporal order
    print("\nâ° éªŒè¯æ—¶é—´åºåˆ—...")
    validate_temporal_order(df_train, df_valid)
    
    # Balance training data
    if args.balance_method != "none":
        print("\nâš–ï¸ å¹³è¡¡è®­ç»ƒæ•°æ®...")
        df_train = balance_dataset(df_train, method=args.balance_method, max_ratio=args.max_imbalance_ratio)
    
    # Prepare features
    id_cols = ["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·", "y_label"]
    feature_cols = [col for col in df_train.columns if col not in id_cols]
    realtime_features = get_realtime_features()
    
    # Filter to only use realtime observable features
    available_realtime = [f for f in realtime_features if f in feature_cols]
    if len(available_realtime) != len(realtime_features):
        missing = set(realtime_features) - set(available_realtime)
        print(f"âš ï¸ ç¼ºå°‘ç‰¹å¾ï¼š{missing}")
    
    feature_cols = available_realtime
    print(f"\nğŸ“Š ä½¿ç”¨ {len(feature_cols)} ä¸ªå®æ—¶å¯è§‚æµ‹ç‰¹å¾ï¼š")
    for i, feat in enumerate(feature_cols, 1):
        print(f"  {i:2d}. {feat}")
    
    X_tr = df_train[feature_cols]
    y_tr = df_train["y_label"]
    X_va = df_valid[feature_cols]
    y_va = df_valid["y_label"]
    
    # Check for missing values
    print(f"\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥ï¼š")
    print(f"è®­ç»ƒé›†ç¼ºå¤±å€¼ï¼š{X_tr.isnull().sum().sum()}")
    print(f"éªŒè¯é›†ç¼ºå¤±å€¼ï¼š{X_va.isnull().sum().sum()}")
    
    # Fill missing values if any
    if X_tr.isnull().sum().sum() > 0 or X_va.isnull().sum().sum() > 0:
        X_tr = X_tr.fillna(0)
        X_va = X_va.fillna(0)
        print("å¡«å……ç¼ºå¤±å€¼ä¸º0")

    # Print class distribution
    print(f"\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒï¼š")
    print(f"è®­ç»ƒé›† - æ­£æ ·æœ¬: {y_tr.sum():,} ({y_tr.mean():.4%})")
    print(f"éªŒè¯é›† - æ­£æ ·æœ¬: {y_va.sum():,} ({y_va.mean():.4%})")

    # LightGBM params with regularization
    print(f"\nğŸš€ è®­ç»ƒ LightGBM æ¨¡å‹...")
    params = dict(
        device_type=args.device,
        boosting_type="gbdt",
        objective="binary",
        metric="average_precision",
        learning_rate=args.learning_rate,
        num_leaves=args.num_leaves,
        max_depth=args.max_depth,
        n_estimators=args.n_estimators,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_lambda=args.reg_lambda,
        reg_alpha=args.reg_alpha,
        min_data_in_leaf=20,
        min_sum_hessian_in_leaf=1e-3,
        random_state=42,
        early_stopping_rounds=args.early_stop,
        verbose=-1
    )

    clf = lgb.LGBMClassifier(**params)
    clf.fit(
        X_tr, y_tr,
        eval_set=[(X_va, y_va)],
        eval_metric="average_precision",
    )

    # Predictions and metrics
    print(f"\nğŸ“Š æ¨¡å‹è¯„ä¼°ï¼š")
    y_prob = clf.predict_proba(X_va)[:, 1]
    pr_auc = average_precision_score(y_va, y_prob)
    prec_k = precision_at_k(y_va.reset_index(drop=True), pd.Series(y_prob), 0.001)

    print(f"è®­ç»ƒå®Œæˆæ—¶é—´ï¼š{time.time()-t0:.1f}s")
    print(f"æœ€ä½³è¿­ä»£ï¼š{clf.best_iteration_}")
    print(f"PR-AUCï¼š{pr_auc:.6f}")
    print(f"Precision@Top0.1%ï¼š{prec_k:.4f}")

    # Feature importance
    print(f"\nğŸ” ç‰¹å¾é‡è¦æ€§ Top 10ï¼š")
    feature_imp = pd.DataFrame({
        'feature': feature_cols,
        'importance': clf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for i, (feat, imp) in enumerate(feature_imp.head(10).values):
        print(f"  {i+1:2d}. {feat:<20} {imp:>8.0f}")

    # Binary classification metrics
    y_pred = (y_prob > 0.5).astype(int)
    print(f"\nğŸ“‹ åˆ†ç±»æŠ¥å‘Šï¼ˆé˜ˆå€¼=0.5ï¼‰ï¼š")
    print(classification_report(y_va, y_pred, target_names=['æ­£å¸¸', 'æ¬ºè¯ˆ']))

    # Save model
    model_path = os.path.join(args.data_root, "model_lgbm_fixed.txt")
    clf.booster_.save_model(model_path)
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜ï¼š{model_path}")
    
    # Save feature importance
    feature_imp.to_csv(os.path.join(args.data_root, "feature_importance.csv"), index=False)
    print(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜ï¼š{os.path.join(args.data_root, 'feature_importance.csv')}")

if __name__ == "__main__":
    main()

"""
# ä¿®å¤æ•°æ®æ³„éœ²åçš„è®­ç»ƒå‘½ä»¤
python scripts/train/train_baseline_fixed.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --train_regex "202503|202504" \
  --valid_regex "202505" \
  --device "cpu" \
  --balance_method "undersample" \
  --max_imbalance_ratio 20.0 \
  --learning_rate 0.1 \
  --num_leaves 31 \
  --max_depth 6 \
  --n_estimators 1000 \
  --early_stop 100 \
  --reg_lambda 10.0 \
  --reg_alpha 10.0
""" 