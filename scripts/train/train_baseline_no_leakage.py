#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
No-Leakage Training Pipeline for Spoofing Detection
--------------------------------------------------
ä¸¥æ ¼ç§»é™¤ä¿¡æ¯æ³„éœ²çš„è®­ç»ƒæµç¨‹ï¼š
â€¢ åªä½¿ç”¨å§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹çš„ç‰¹å¾
â€¢ ç§»é™¤æ‰€æœ‰æœªæ¥èšåˆç‰¹å¾
â€¢ ä½¿ç”¨åŸå§‹æ ‡ç­¾æˆ–ä¸¥æ ¼å®šä¹‰çš„æ—¶ç‚¹æ ‡ç­¾
â€¢ çœŸå®çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°
"""

import argparse, glob, os, time, warnings
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import average_precision_score, classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

def get_realtime_observable_features():
    """ä¸¥æ ¼å®šä¹‰ï¼šåªåœ¨å§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹çš„ç‰¹å¾"""
    return [
        # è¡Œæƒ…ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "bid1", "ask1", "prev_close", "mid_price", "spread",
        
        # ä»·æ ¼ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "delta_mid", "pct_spread", "price_dev_prevclose",
        
        # è®¢å•ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "is_buy", "log_qty",
        
        # å†å²ç»Ÿè®¡ç‰¹å¾ï¼ˆåªä½¿ç”¨è¿‡å»çš„ä¿¡æ¯ï¼Œæ—¶é—´çª—å£ç»“æŸæ—¶åˆ» < å§”æ‰˜æ—¶åˆ»ï¼‰
        "orders_100ms", "cancels_5s", 
        
        # æ—¶é—´ç‰¹å¾
        "time_sin", "time_cos", "in_auction",
        
        # å¢å¼ºç‰¹å¾ï¼ˆåŸºäºå§”æ‰˜æ—¶åˆ»çš„ä¿¡æ¯ï¼‰
        "book_imbalance",  # åŸºäºå§”æ‰˜æ—¶åˆ»çš„ç”³ä¹°ç”³å–é‡
        "price_aggressiveness",  # åŸºäºå§”æ‰˜ä»·æ ¼ä¸å½“æ—¶bid/askçš„å…³ç³»
        # æ³¨æ„ï¼šlayering_scoreå¦‚æœåŸºäºæœªæ¥ä¿¡æ¯åˆ™ä¸èƒ½ä½¿ç”¨
    ]

def get_leakage_features():
    """å®šä¹‰å¯èƒ½æ³„éœ²æœªæ¥ä¿¡æ¯çš„ç‰¹å¾"""
    return [
        # æ˜æ˜¾çš„æœªæ¥ä¿¡æ¯
        "final_survival_time_ms",  # æœ€ç»ˆå­˜æ´»æ—¶é—´
        "total_events",           # æ€»äº‹ä»¶æ•°
        "total_traded_qty",       # æ€»æˆäº¤é‡
        "num_trades",            # æˆäº¤æ¬¡æ•°
        "num_cancels",           # æ’¤å•æ¬¡æ•°
        "is_fully_filled",       # æ˜¯å¦å®Œå…¨æˆäº¤
        
        # å¯èƒ½åŒ…å«æœªæ¥ä¿¡æ¯çš„ç‰¹å¾
        "layering_score",        # å¦‚æœåŸºäºæœªæ¥è®¢å•æ¨¡å¼è®¡ç®—
        
        # åŸºäºæ•´ä¸ªè®¢å•ç”Ÿå‘½å‘¨æœŸçš„èšåˆç‰¹å¾
        "å­˜æ´»æ—¶é—´_ms",           # å­˜æ´»æ—¶é—´ï¼ˆäº‹ä»¶å‘ç”Ÿæ—¶æœªçŸ¥ï¼‰
    ]

def create_time_based_labels(df, r1_ms=50, r2_ms=1000, r2_mult=4.0):
    """åˆ›å»ºåŸºäºæ—¶ç‚¹çš„æ ‡ç­¾ï¼ˆé¿å…ä½¿ç”¨å­˜æ´»æ—¶é—´ï¼‰"""
    print("ğŸ·ï¸ Creating time-based labels without leakage...")
    
    # æŒ‰è®¢å•åˆ†ç»„å¤„ç†
    labels = []
    
    for (date, ticker, order_id), group in df.groupby(['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·']):
        group = group.sort_values('äº‹ä»¶_datetime')
        
        # è·å–ç¬¬ä¸€ä¸ªäº‹ä»¶ï¼ˆå§”æ‰˜æ—¶åˆ»ï¼‰çš„ä¿¡æ¯
        first_event = group.iloc[0]
        
        # æ£€æŸ¥åœ¨r1_msæ—¶é—´çª—å£å†…æ˜¯å¦æœ‰æ’¤å•
        r1_window_end = first_event['å§”æ‰˜_datetime'] + pd.Timedelta(milliseconds=r1_ms)
        r1_events = group[group['äº‹ä»¶_datetime'] <= r1_window_end]
        has_quick_cancel = (r1_events['äº‹ä»¶ç±»å‹'] == 'æ’¤å•').any()
        
        # æ£€æŸ¥ä»·æ ¼åç¦»æ¡ä»¶ï¼ˆåŸºäºå§”æ‰˜æ—¶åˆ»çš„ä¿¡æ¯ï¼‰
        if 'spread' in first_event and 'delta_mid' in first_event:
            spread_val = first_event['spread']
            delta_mid_val = first_event['delta_mid']
            price_condition = abs(delta_mid_val) >= r2_mult * spread_val if spread_val > 0 else False
        else:
            price_condition = False
        
        # ç»„åˆæ¡ä»¶
        is_spoofing = has_quick_cancel and price_condition
        
        # ä¸ºè¯¥è®¢å•çš„æ‰€æœ‰äº‹ä»¶åˆ†é…æ ‡ç­¾
        for idx in group.index:
            labels.append({
                'index': idx,
                'y_label_clean': int(is_spoofing)
            })
    
    label_df = pd.DataFrame(labels).set_index('index')
    return label_df

def analyze_data_leakage(df):
    """åˆ†ææ•°æ®æ³„éœ²é£é™©"""
    print("\nğŸ” Analyzing potential data leakage...")
    
    leakage_features = get_leakage_features()
    observable_features = get_realtime_observable_features()
    
    print(f"ğŸ“Š Total features in dataset: {len(df.columns)}")
    
    # æ£€æŸ¥æ³„éœ²ç‰¹å¾
    found_leakage = [f for f in leakage_features if f in df.columns]
    if found_leakage:
        print(f"âš ï¸ Found {len(found_leakage)} potential leakage features:")
        for feat in found_leakage:
            print(f"  - {feat}")
    
    # æ£€æŸ¥å¯ç”¨çš„å®‰å…¨ç‰¹å¾
    available_safe = [f for f in observable_features if f in df.columns]
    print(f"âœ… Found {len(available_safe)} safe observable features:")
    for feat in available_safe:
        print(f"  - {feat}")
    
    return found_leakage, available_safe

def comprehensive_evaluation(y_true, y_pred_proba):
    """ç»¼åˆè¯„ä¼°ï¼ˆæ— æ³„éœ²ç‰ˆæœ¬ï¼‰"""
    # åŸºç¡€æŒ‡æ ‡
    pr_auc = average_precision_score(y_true, y_pred_proba)
    roc_auc = roc_auc_score(y_true, y_pred_proba)
    
    metrics = {'PR-AUC': pr_auc, 'ROC-AUC': roc_auc}
    
    # Precision at K
    for k in [0.001, 0.005, 0.01, 0.05]:
        k_int = max(1, int(len(y_true) * k))
        top_k_idx = y_pred_proba.argsort()[::-1][:k_int]
        prec_k = y_true.iloc[top_k_idx].mean() if hasattr(y_true, 'iloc') else y_true[top_k_idx].mean()
        metrics[f'Precision@{k*100:.1f}%'] = prec_k
    
    return metrics

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_root", required=True)
    parser.add_argument("--train_regex", default="202503|202504")
    parser.add_argument("--valid_regex", default="202505")
    parser.add_argument("--use_original_labels", action="store_true", 
                       help="ä½¿ç”¨åŸå§‹y_labelè€Œä¸æ˜¯é‡æ–°ç”Ÿæˆçš„æ— æ³„éœ²æ ‡ç­¾")
    parser.add_argument("--r1_ms", type=int, default=50, help="å¿«é€Ÿæ’¤å•æ—¶é—´é˜ˆå€¼")
    parser.add_argument("--r2_ms", type=int, default=1000, help="ä»·æ ¼åç¦»æ—¶é—´é˜ˆå€¼")
    parser.add_argument("--r2_mult", type=float, default=4.0, help="ä»·æ ¼åç¦»å€æ•°")
    
    args = parser.parse_args()
    
    t0 = time.time()
    print("ğŸ” Loading data for no-leakage training...")
    
    # åŠ è½½ç‰¹å¾æ•°æ®
    feat_pats = [os.path.join(args.data_root, "features_select", "X_*.parquet")]
    files = []
    for pat in feat_pats:
        files.extend(sorted(glob.glob(pat)))
    
    if not files:
        print(f"âŒ No feature files found")
        return
    
    df_feat = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)
    print(f"Features data shape: {df_feat.shape}")
    
    # åŠ è½½æ ‡ç­¾æ•°æ®
    lab_pats = [os.path.join(args.data_root, "labels_select", "labels_*.parquet")]
    files = []
    for pat in lab_pats:
        files.extend(sorted(glob.glob(pat)))
    
    df_lab = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)
    print(f"Labels data shape: {df_lab.shape}")
    
    # åˆå¹¶æ•°æ®
    df = df_feat.merge(df_lab, on=["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"], how="inner")
    print(f"Merged data shape: {df.shape}")
    
    # åˆ†ææ•°æ®æ³„éœ²
    leakage_features, safe_features = analyze_data_leakage(df)
    
    # ç§»é™¤æ³„éœ²ç‰¹å¾
    print(f"\nğŸš« Removing {len(leakage_features)} leakage features...")
    df_clean = df.drop(columns=leakage_features, errors='ignore')
    
    # å¦‚æœä¸ä½¿ç”¨åŸå§‹æ ‡ç­¾ï¼Œç”Ÿæˆæ— æ³„éœ²æ ‡ç­¾
    if not args.use_original_labels:
        print("ğŸ·ï¸ Generating clean labels without survival time...")
        
        # éœ€è¦åŠ è½½åŸå§‹äº‹ä»¶æµæ•°æ®æ¥ç”Ÿæˆæ— æ³„éœ²æ ‡ç­¾
        event_pattern = os.path.join(args.data_root, "event_stream", "*", "å§”æ‰˜äº‹ä»¶æµ.csv")
        event_files = glob.glob(event_pattern)
        
        if event_files:
            print(f"Found {len(event_files)} event stream files")
            
            # è¯»å–å‡ ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
            sample_events = []
            for event_file in event_files[:3]:  # åªå¤„ç†å‰3ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
                date_str = os.path.basename(os.path.dirname(event_file))
                if any(date_str in regex for regex in [args.train_regex, args.valid_regex]):
                    print(f"Processing {date_str}...")
                    event_df = pd.read_csv(event_file, parse_dates=['å§”æ‰˜_datetime', 'äº‹ä»¶_datetime'])
                    sample_events.append(event_df)
            
            if sample_events:
                events_combined = pd.concat(sample_events, ignore_index=True)
                clean_labels = create_time_based_labels(events_combined, args.r1_ms, args.r2_ms, args.r2_mult)
                
                # åˆå¹¶æ¸…æ´æ ‡ç­¾
                df_clean = df_clean.merge(clean_labels, left_index=True, right_index=True, how='left')
                df_clean['y_label_clean'] = df_clean['y_label_clean'].fillna(0).astype(int)
                target_col = 'y_label_clean'
            else:
                print("âš ï¸ No event files found, using original y_label")
                target_col = 'y_label'
        else:
            print("âš ï¸ No event stream files found, using original y_label")
            target_col = 'y_label'
    else:
        target_col = 'y_label'
    
    print(f"Using target column: {target_col}")
    
    # å‡†å¤‡ç‰¹å¾
    id_cols = ["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·", "y_label"]
    if target_col != 'y_label':
        id_cols.append(target_col)
    
    feature_cols = [col for col in safe_features if col in df_clean.columns]
    print(f"\nğŸ“Š Using {len(feature_cols)} safe features:")
    for i, feat in enumerate(feature_cols, 1):
        print(f"  {i:2d}. {feat}")
    
    # æ•°æ®åˆ‡åˆ†
    train_mask = df_clean["è‡ªç„¶æ—¥"].astype(str).str.contains(args.train_regex)
    valid_mask = df_clean["è‡ªç„¶æ—¥"].astype(str).str.contains(args.valid_regex)
    
    df_train = df_clean[train_mask].copy()
    df_valid = df_clean[valid_mask].copy()
    
    print(f"\nğŸ“… Data split:")
    print(f"Training: {len(df_train):,} samples")
    print(f"Validation: {len(df_valid):,} samples")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X_tr = df_train[feature_cols].fillna(0)
    y_tr = df_train[target_col]
    X_va = df_valid[feature_cols].fillna(0)
    y_va = df_valid[target_col]
    
    print(f"\nâš–ï¸ Class distribution:")
    print(f"Train: {y_tr.value_counts().to_dict()}")
    print(f"Valid: {y_va.value_counts().to_dict()}")
    
    # ç®€å•ä¸‹é‡‡æ ·å¹³è¡¡æ•°æ®
    if y_tr.sum() > 0:
        pos_indices = y_tr[y_tr == 1].index
        neg_indices = y_tr[y_tr == 0].index
        
        # ä¿æŒ1:20çš„æ¯”ä¾‹
        target_neg_size = min(len(pos_indices) * 20, len(neg_indices))
        selected_neg_indices = np.random.choice(neg_indices, target_neg_size, replace=False)
        
        selected_indices = np.concatenate([pos_indices, selected_neg_indices])
        X_tr = X_tr.loc[selected_indices]
        y_tr = y_tr.loc[selected_indices]
        
        print(f"After balancing: {y_tr.value_counts().to_dict()}")
    
    # è®­ç»ƒæ¨¡å‹
    print(f"\nğŸš€ Training LightGBM model...")
    model = lgb.LGBMClassifier(
        objective='binary',
        metric='average_precision',
        learning_rate=0.05,
        num_leaves=31,
        max_depth=6,
        n_estimators=1000,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=10,
        reg_lambda=10,
        random_state=42,
        verbose=-1
    )
    
    # è®­ç»ƒ
    try:
        model.fit(
            X_tr, y_tr,
            eval_set=[(X_va, y_va)],
            early_stopping_rounds=100,
            verbose=False
        )
    except TypeError:
        from lightgbm import early_stopping
        model.fit(
            X_tr, y_tr,
            eval_set=[(X_va, y_va)],
            callbacks=[early_stopping(100)]
        )
    
    # è¯„ä¼°
    print("\nğŸ“Š Model Evaluation (No Leakage):")
    y_pred_proba = model.predict_proba(X_va)[:, 1]
    
    metrics = comprehensive_evaluation(y_va, y_pred_proba)
    for metric, value in metrics.items():
        print(f"{metric}: {value:.6f}")
    
    # ç‰¹å¾é‡è¦æ€§
    feature_imp = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nğŸ” Top 10 Feature Importance (No Leakage):")
    for i, (feat, imp) in enumerate(feature_imp.head(10).values):
        print(f"  {i+1:2d}. {feat:<25} {imp:>8.0f}")
    
    # ä¿å­˜ç»“æœ
    results = {
        'method': f"No-Leakage-Training-{target_col}",
        'n_features': len(feature_cols),
        'removed_leakage_features': len(leakage_features),
        'training_time': time.time() - t0,
        **metrics
    }
    
    import json
    results_file = os.path.join(args.data_root, "no_leakage_results.json")
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ Results saved to: {results_file}")
    print(f"â±ï¸ Total training time: {time.time()-t0:.1f}s")

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# ä½¿ç”¨åŸå§‹æ ‡ç­¾ï¼Œç§»é™¤æ³„éœ²ç‰¹å¾
python scripts/train/train_baseline_no_leakage.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --use_original_labels

# ç”Ÿæˆæ— æ³„éœ²æ ‡ç­¾å¹¶è®­ç»ƒ
python scripts/train/train_baseline_no_leakage.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --r1_ms 50 --r2_ms 1000 --r2_mult 4.0
""" 