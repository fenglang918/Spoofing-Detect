#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Enhanced LightGBM Training Pipeline for Spoofing Detection (Fixed)
------------------------------------------------------------------
ä¸»è¦ä¼˜åŒ–ï¼š
â€¢ å¢å¼ºç‰¹å¾å·¥ç¨‹ï¼šåŸºäºç°æœ‰åˆ—çš„æŠ€æœ¯æŒ‡æ ‡ã€ç»Ÿè®¡ç‰¹å¾ã€äº¤äº’ç‰¹å¾
â€¢ æ”¹è¿›æ•°æ®å¹³è¡¡ç­–ç•¥ï¼šä¸‹é‡‡æ · + åˆ†å±‚é‡‡æ ·
â€¢ æ¨¡å‹é›†æˆï¼šå¤šç§ç®—æ³•ç»„åˆ
â€¢ æ›´ç»†è‡´çš„è¶…å‚æ•°è°ƒä¼˜
â€¢ å¢å¼ºè¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–
â€¢ é€‚é…æ–°çš„æ•°æ®æ¶æ„ï¼ˆç¬¦åˆdata.mdè§„èŒƒï¼‰
"""

import argparse, glob, os, re, sys, time, warnings
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import average_precision_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import optuna
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

# ---------- Focal Loss Implementation --------------------------------
def focal_loss_objective(y_true, y_pred, alpha=0.25, gamma=2.0):
    """
    Focal Loss for LightGBM
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ¦‚ç‡ï¼ˆlogitsï¼‰
        alpha: å¹³è¡¡å› å­ï¼Œç”¨äºå¹³è¡¡æ­£è´Ÿæ ·æœ¬
        gamma: focusingå‚æ•°ï¼Œç”¨äºå‡å°‘æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡
    """
    # å°†logitsè½¬æ¢ä¸ºæ¦‚ç‡
    p = 1 / (1 + np.exp(-y_pred))
    
    # è®¡ç®—focal loss
    ce_loss = -y_true * np.log(p + 1e-8) - (1 - y_true) * np.log(1 - p + 1e-8)
    p_t = p * y_true + (1 - p) * (1 - y_true)
    alpha_t = alpha * y_true + (1 - alpha) * (1 - y_true)
    
    focal_weight = alpha_t * (1 - p_t) ** gamma
    focal_loss = focal_weight * ce_loss
    
    # è®¡ç®—æ¢¯åº¦å’Œæµ·å¡çŸ©é˜µ
    grad = focal_weight * (p - y_true)
    hess = focal_weight * p * (1 - p) * (gamma * (y_true - p) + 1)
    
    return grad, hess

def focal_loss_lgb(alpha=0.25, gamma=2.0):
    """è¿”å›LightGBMå¯ç”¨çš„focal lossç›®æ ‡å‡½æ•°"""
    def objective(y_true, y_pred):
        return focal_loss_objective(y_true, y_pred, alpha, gamma)
    return objective

# ---------- Enhanced Feature Engineering --------------------------------
def create_technical_indicators(df):
    """åˆ›å»ºæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ï¼ˆåŸºäºç°æœ‰åˆ—ï¼‰"""
    features = {}
    
    # ä»·æ ¼æŠ€æœ¯æŒ‡æ ‡
    features['price_volatility'] = df.groupby('ticker')['mid_price'].transform(
        lambda x: x.rolling(100, min_periods=1).std()
    )
    features['price_momentum'] = df.groupby('ticker')['mid_price'].transform(
        lambda x: x.pct_change(5)
    )
    features['relative_spread'] = df['spread'] / df['mid_price']
    features['spread_volatility'] = df.groupby('ticker')['spread'].transform(
        lambda x: x.rolling(50, min_periods=1).std()
    )
    
    # è®¢å•æµæŒ‡æ ‡
    features['order_imbalance'] = (df['bid1'] - df['ask1']) / (df['bid1'] + df['ask1'])
    
    # ä»·æ ¼çº§åˆ«ç‰¹å¾ï¼ˆåŸºäºç°æœ‰çš„ä»·æ ¼ä¿¡æ¯ï¼‰
    # å‡è®¾å§”æ‰˜ä»·æ ¼å¯ä»¥é€šè¿‡mid_priceå’Œdelta_midé‡æ„
    features['at_bid'] = (df['delta_mid'] <= -df['spread']/2).astype(int)
    features['at_ask'] = (df['delta_mid'] >= df['spread']/2).astype(int)
    features['between_quotes'] = ((df['delta_mid'] > -df['spread']/2) & 
                                 (df['delta_mid'] < df['spread']/2)).astype(int)
    
    return pd.DataFrame(features)

def create_statistical_features(df):
    """åˆ›å»ºç»Ÿè®¡ç‰¹å¾"""
    features = {}
    
    # åˆ†ä½æ•°ç‰¹å¾
    for col in ['log_qty', 'spread', 'delta_mid']:
        if col in df.columns:
            # è®¡ç®—åˆ†ä½æ•°rank
            features[f'{col}_rank'] = df.groupby('ticker')[col].transform(
                lambda x: x.rank(pct=True)
            )
            # ç›¸å¯¹äºå¸‚åœºå¹³å‡çš„æ ‡å‡†åŒ–
            features[f'{col}_zscore'] = df.groupby('ticker')[col].transform(
                lambda x: (x - x.mean()) / (x.std() + 1e-8)
            )
    
    # å†å²è¡Œä¸ºç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…æ—¶é—´åˆ—é—®é¢˜ï¼‰
    features['hist_order_size_mean'] = df.groupby('ticker')['log_qty'].transform(
        lambda x: x.expanding().mean().shift(1)
    )
    features['hist_spread_mean'] = df.groupby('ticker')['spread'].transform(
        lambda x: x.expanding().mean().shift(1)
    )
    
    return pd.DataFrame(features)

def create_interaction_features(df):
    """åˆ›å»ºäº¤äº’ç‰¹å¾"""
    features = {}
    
    # ç¡®ä¿relative_spreadå­˜åœ¨
    if 'relative_spread' not in df.columns:
        df['relative_spread'] = df['spread'] / df['mid_price']
    
    # å…³é”®äº¤äº’
    features['qty_spread_interaction'] = df['log_qty'] * df['relative_spread']
    features['time_qty_interaction'] = df['time_cos'] * df['log_qty']
    features['direction_spread_interaction'] = df['is_buy'] * df['pct_spread']
    
    # ä»·æ ¼-æ—¶é—´äº¤äº’
    features['price_time_interaction'] = df['price_dev_prevclose'] * df['time_sin']
    
    # è®¢å•æ´»åŠ¨äº¤äº’
    features['orders_cancels_ratio'] = df['orders_100ms'] / (df['cancels_5s'] + 1)
    features['activity_spread_interaction'] = df['orders_100ms'] * df['relative_spread']
    
    return pd.DataFrame(features)

def enhance_features(df):
    """å¢å¼ºç‰¹å¾å·¥ç¨‹"""
    print("ğŸ”§ Creating enhanced features...")
    print(f"Original columns: {df.columns.tolist()}")
    
    # è®¡ç®—åŸºç¡€è¡ç”Ÿç‰¹å¾
    if 'relative_spread' not in df.columns:
        df['relative_spread'] = df['spread'] / df['mid_price']
    
    # åˆ›å»ºæŠ€æœ¯æŒ‡æ ‡
    try:
        tech_features = create_technical_indicators(df)
        print(f"Created {len(tech_features.columns)} technical indicators")
    except Exception as e:
        print(f"Warning: Technical indicators failed: {e}")
        tech_features = pd.DataFrame()
    
    # åˆ›å»ºç»Ÿè®¡ç‰¹å¾
    try:
        stat_features = create_statistical_features(df)
        print(f"Created {len(stat_features.columns)} statistical features")
    except Exception as e:
        print(f"Warning: Statistical features failed: {e}")
        stat_features = pd.DataFrame()
    
    # åˆ›å»ºäº¤äº’ç‰¹å¾
    try:
        interaction_features = create_interaction_features(df)
        print(f"Created {len(interaction_features.columns)} interaction features")
    except Exception as e:
        print(f"Warning: Interaction features failed: {e}")
        interaction_features = pd.DataFrame()
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    feature_dfs = [df]
    if not tech_features.empty:
        # ç§»é™¤å·²å­˜åœ¨çš„åˆ—
        tech_features = tech_features.loc[:, ~tech_features.columns.isin(df.columns)]
        if not tech_features.empty:
            feature_dfs.append(tech_features)
    if not stat_features.empty:
        stat_features = stat_features.loc[:, ~stat_features.columns.isin(df.columns)]
        if not stat_features.empty:
            feature_dfs.append(stat_features)
    if not interaction_features.empty:
        # æ£€æŸ¥äº¤äº’ç‰¹å¾æ˜¯å¦é‡å¤
        existing_cols = df.columns.tolist()
        if len(feature_dfs) > 1:
            for feat_df in feature_dfs[1:]:
                existing_cols.extend(feat_df.columns.tolist())
        interaction_features = interaction_features.loc[:, ~interaction_features.columns.isin(existing_cols)]
        if not interaction_features.empty:
            feature_dfs.append(interaction_features)
    
    enhanced_df = pd.concat(feature_dfs, axis=1)
    
    # æœ€ç»ˆæ£€æŸ¥ï¼šç§»é™¤ä»»ä½•é‡å¤çš„åˆ—
    enhanced_df = enhanced_df.loc[:, ~enhanced_df.columns.duplicated()]
    
    # å¤„ç†æ— ç©·å€¼å’Œç¼ºå¤±å€¼
    enhanced_df = enhanced_df.replace([np.inf, -np.inf], np.nan)
    enhanced_df = enhanced_df.fillna(0)
    
    print(f"Enhanced dataframe shape: {enhanced_df.shape}")
    print(f"Final columns: {len(enhanced_df.columns)} (duplicates removed)")
    return enhanced_df

# ---------- Advanced Sampling Strategies --------------------------------
def advanced_sampling(X, y, method='undersample', sampling_strategy='auto'):
    """é«˜çº§é‡‡æ ·ç­–ç•¥ï¼ˆç§»é™¤SMOTEï¼Œæ•°æ®å¤ªä¸å¹³è¡¡ï¼‰"""
    print(f"ğŸ¯ Applying {method} sampling...")
    
    original_pos = y.sum()
    original_neg = len(y) - original_pos
    print(f"Original distribution: {original_pos:,} positive, {original_neg:,} negative")
    
    if method == 'undersample':
        # ä¸‹é‡‡æ ·è´Ÿæ ·æœ¬ï¼Œä¿æŒåˆç†æ¯”ä¾‹
        pos_indices = y[y == 1].index
        neg_indices = y[y == 0].index
        
        # ä¿æŒ1:10çš„æ¯”ä¾‹ï¼Œé¿å…è¿‡åº¦ä¸å¹³è¡¡
        target_neg_size = min(len(pos_indices) * 10, len(neg_indices))
        selected_neg_indices = np.random.choice(neg_indices, target_neg_size, replace=False)
        
        selected_indices = np.concatenate([pos_indices, selected_neg_indices])
        selected_X = X.loc[selected_indices]
        selected_y = y.loc[selected_indices]
        
        new_pos = selected_y.sum()
        new_neg = len(selected_y) - new_pos
        print(f"After {method}: {new_pos:,} positive, {new_neg:,} negative")
        return selected_X, selected_y
        
    elif method == 'stratified_undersample':
        # åˆ†å±‚ä¸‹é‡‡æ ·ï¼ŒæŒ‰è‚¡ç¥¨åˆ†åˆ«é‡‡æ ·
        pos_indices = y[y == 1].index
        neg_indices = y[y == 0].index
        
        # æ¯ä¸ªè‚¡ç¥¨ä¿æŒç›¸åŒçš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
        if 'ticker' in X.columns:
            selected_indices = []
            for ticker in X['ticker'].unique():
                ticker_mask = X['ticker'] == ticker
                ticker_pos = pos_indices[X.loc[pos_indices, 'ticker'] == ticker]
                ticker_neg = neg_indices[X.loc[neg_indices, 'ticker'] == ticker]
                
                if len(ticker_pos) > 0 and len(ticker_neg) > 0:
                    # æ¯ä¸ªè‚¡ç¥¨ä¿æŒ1:10æ¯”ä¾‹
                    target_neg = min(len(ticker_pos) * 10, len(ticker_neg))
                    selected_neg = np.random.choice(ticker_neg, target_neg, replace=False)
                    selected_indices.extend(ticker_pos.tolist())
                    selected_indices.extend(selected_neg.tolist())
            
            selected_X = X.loc[selected_indices]
            selected_y = y.loc[selected_indices]
        else:
            # å›é€€åˆ°æ™®é€šä¸‹é‡‡æ ·
            return advanced_sampling(X, y, method='undersample')
        
        new_pos = selected_y.sum()
        new_neg = len(selected_y) - new_pos
        print(f"After {method}: {new_pos:,} positive, {new_neg:,} negative")
        return selected_X, selected_y
    
    else:
        # ä¸é‡‡æ ·ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
        print("Using original data without sampling")
        return X, y

# ---------- Model Ensemble -----------------------------------------------
class EnsembleClassifier:
    def __init__(self, models=None):
        if models is None:
            self.models = {
                'lgb': lgb.LGBMClassifier(
                    objective='binary',
                    metric='average_precision',
                    boosting_type='gbdt',
                    n_estimators=1000,
                    learning_rate=0.05,
                    num_leaves=31,
                    max_depth=6,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_alpha=10,
                    reg_lambda=10,
                    random_state=42,
                    verbose=-1
                ),
                'xgb': XGBClassifier(
                    objective='binary:logistic',
                    eval_metric='aucpr',
                    n_estimators=500,  # å‡å°‘è½®æ•°é¿å…è¿‡æ‹Ÿåˆ
                    learning_rate=0.05,
                    max_depth=6,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_alpha=10,
                    reg_lambda=10,
                    random_state=42,
                    verbosity=0
                ),
                'rf': RandomForestClassifier(
                    n_estimators=300,  # å‡å°‘æ ‘çš„æ•°é‡
                    max_depth=8,
                    min_samples_split=20,
                    min_samples_leaf=10,
                    random_state=42,
                    n_jobs=-1
                )
            }
        else:
            self.models = models
        
        self.weights = None
        self.fitted_models = {}
    
    def fit(self, X, y, X_val=None, y_val=None):
        """è®­ç»ƒé›†æˆæ¨¡å‹"""
        print("ğŸš€ Training ensemble models...")
        
        for name, model in self.models.items():
            print(f"  Training {name}...")
            
            if name == 'lgb' and X_val is not None:
                # LightGBM with early stopping
                try:
                    model.fit(
                        X, y,
                        eval_set=[(X_val, y_val)],
                        early_stopping_rounds=100,
                        verbose=False
                    )
                except TypeError:
                    from lightgbm import early_stopping
                    model.fit(
                        X, y,
                        eval_set=[(X_val, y_val)],
                        callbacks=[early_stopping(100)]
                    )
            else:
                # XGBoostå’ŒRandomForestç›´æ¥è®­ç»ƒ
                model.fit(X, y)
            
            self.fitted_models[name] = model
        
        # å¦‚æœæœ‰éªŒè¯é›†ï¼Œè®¡ç®—æƒé‡
        if X_val is not None and y_val is not None:
            self._compute_weights(X_val, y_val)
        else:
            # ç­‰æƒé‡
            self.weights = {name: 1.0/len(self.models) for name in self.models.keys()}
    
    def _compute_weights(self, X_val, y_val):
        """åŸºäºéªŒè¯é›†æ€§èƒ½è®¡ç®—æƒé‡"""
        scores = {}
        for name, model in self.fitted_models.items():
            pred_proba = model.predict_proba(X_val)[:, 1]
            score = average_precision_score(y_val, pred_proba)
            scores[name] = score
            print(f"  {name} PR-AUC: {score:.4f}")
        
        # åŸºäºæ€§èƒ½è®¡ç®—æƒé‡
        total_score = sum(scores.values())
        if total_score > 0:
            self.weights = {name: score/total_score for name, score in scores.items()}
        else:
            self.weights = {name: 1.0/len(self.models) for name in self.models.keys()}
        print(f"  Weights: {self.weights}")
    
    def predict_proba(self, X):
        """é›†æˆé¢„æµ‹"""
        predictions = []
        for name, model in self.fitted_models.items():
            pred = model.predict_proba(X)[:, 1]
            predictions.append(pred * self.weights[name])
        
        ensemble_pred = np.sum(predictions, axis=0)
        return np.column_stack([1 - ensemble_pred, ensemble_pred])

# ---------- Hyperparameter Optimization ----------------------------------
def optimize_lgb_params(X_train, y_train, X_val, y_val, n_trials=50):
    """ä¼˜åŒ–LightGBMè¶…å‚æ•°"""
    def objective(trial):
        params = {
            'objective': 'binary',
            'metric': 'average_precision',
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'num_leaves': trial.suggest_int('num_leaves', 10, 300),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 0, 20),
            'reg_lambda': trial.suggest_float('reg_lambda', 0, 20),
            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),
            'random_state': 42,
            'verbose': -1
        }
        
        model = lgb.LGBMClassifier(**params, n_estimators=1000)
        
        # ä¿®å¤early_stoppingå‚æ•°å…¼å®¹æ€§
        try:
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                early_stopping_rounds=100,
                verbose=False
            )
        except TypeError:
            # å¤„ç†æ–°ç‰ˆæœ¬LightGBM
            from lightgbm import early_stopping
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                callbacks=[early_stopping(100)]
            )
        
        pred_proba = model.predict_proba(X_val)[:, 1]
        return average_precision_score(y_val, pred_proba)
    
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
    
    print(f"Best PR-AUC: {study.best_value:.6f}")
    return study.best_params

# ---------- Ticker Selection Functions -----------------------------------
def filter_tickers_by_criteria(df, method="random", max_tickers=None):
    """æ ¹æ®æ ‡å‡†ç­›é€‰è‚¡ç¥¨"""
    if method == "random":
        # éšæœºé€‰æ‹©
        available_tickers = df['ticker'].unique()
        if max_tickers and len(available_tickers) > max_tickers:
            selected_tickers = np.random.choice(available_tickers, max_tickers, replace=False)
            return selected_tickers.tolist()
        return available_tickers.tolist()
    
    elif method == "by_volume":
        # æŒ‰äº¤æ˜“é‡é€‰æ‹©ï¼ˆå¦‚æœæœ‰ç›¸å…³å­—æ®µï¼‰
        if 'log_qty' in df.columns:
            ticker_volumes = df.groupby('ticker')['log_qty'].sum().sort_values(ascending=False)
        else:
            print("âš ï¸ æ²¡æœ‰äº¤æ˜“é‡å­—æ®µï¼Œä½¿ç”¨æ ·æœ¬æ•°é‡ä½œä¸ºä»£ç†")
            ticker_volumes = df.groupby('ticker').size().sort_values(ascending=False)
        
        if max_tickers:
            return ticker_volumes.head(max_tickers).index.tolist()
        return ticker_volumes.index.tolist()
    
    elif method == "by_positive_samples":
        # æŒ‰æ­£æ ·æœ¬æ•°é‡é€‰æ‹©
        if 'y_label' in df.columns:
            ticker_positive = df[df['y_label'] == 1].groupby('ticker').size().sort_values(ascending=False)
            if max_tickers:
                return ticker_positive.head(max_tickers).index.tolist()
            return ticker_positive.index.tolist()
        else:
            print("âš ï¸ æ²¡æœ‰æ ‡ç­¾å­—æ®µï¼Œå›é€€åˆ°éšæœºé€‰æ‹©")
            return filter_tickers_by_criteria(df, "random", max_tickers)
    
    else:
        return df['ticker'].unique().tolist()

def apply_ticker_filtering(df, args):
    """åº”ç”¨è‚¡ç¥¨ç­›é€‰é€»è¾‘"""
    print(f"ğŸ” åŸå§‹è‚¡ç¥¨æ•°é‡: {df['ticker'].nunique()}")
    print(f"åŸå§‹æ•°æ®é‡: {len(df):,} æ¡")
    
    # 1. ä»æ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨
    if args.ticker_file:
        if os.path.exists(args.ticker_file):
            with open(args.ticker_file, 'r') as f:
                file_tickers = [line.strip() for line in f if line.strip()]
            print(f"ğŸ“‹ ä»æ–‡ä»¶è¯»å–åˆ° {len(file_tickers)} ä¸ªè‚¡ç¥¨ä»£ç ")
            df = df[df['ticker'].isin(file_tickers)]
        else:
            print(f"âš ï¸ è‚¡ç¥¨æ–‡ä»¶ä¸å­˜åœ¨: {args.ticker_file}")
    
    # 2. åº”ç”¨åŒ…å«åˆ—è¡¨
    if args.include_tickers:
        print(f"ğŸ“Œ ç­›é€‰æŒ‡å®šè‚¡ç¥¨: {args.include_tickers}")
        df = df[df['ticker'].isin(args.include_tickers)]
        missing_tickers = set(args.include_tickers) - set(df['ticker'].unique())
        if missing_tickers:
            print(f"âš ï¸ ä»¥ä¸‹è‚¡ç¥¨åœ¨æ•°æ®ä¸­æœªæ‰¾åˆ°: {missing_tickers}")
    
    # 3. åº”ç”¨æ’é™¤åˆ—è¡¨
    if args.exclude_tickers:
        print(f"ğŸš« æ’é™¤è‚¡ç¥¨: {args.exclude_tickers}")
        df = df[~df['ticker'].isin(args.exclude_tickers)]
    
    # 4. åº”ç”¨æ•°é‡é™åˆ¶å’Œé€‰æ‹©æ–¹æ³•
    if args.max_tickers:
        available_tickers = df['ticker'].unique()
        if len(available_tickers) > args.max_tickers:
            print(f"ğŸ¯ ä½¿ç”¨ {args.ticker_selection_method} æ–¹æ³•é€‰æ‹© {args.max_tickers} ä¸ªè‚¡ç¥¨")
            selected_tickers = filter_tickers_by_criteria(df, args.ticker_selection_method, args.max_tickers)
            df = df[df['ticker'].isin(selected_tickers)]
            print(f"âœ… é€‰ä¸­è‚¡ç¥¨: {selected_tickers}")
    
    print(f"ğŸ” ç­›é€‰åè‚¡ç¥¨æ•°é‡: {df['ticker'].nunique()}")
    print(f"ç­›é€‰åæ•°æ®é‡: {len(df):,} æ¡")
    
    # æ˜¾ç¤ºè‚¡ç¥¨åˆ†å¸ƒç»Ÿè®¡
    if df['ticker'].nunique() <= 20:
        ticker_stats = df.groupby('ticker').agg({
            'y_label': ['count', 'sum']  # æ€»æ ·æœ¬æ•°å’Œæ­£æ ·æœ¬æ•°
        }).round(3)
        ticker_stats.columns = ['æ ·æœ¬æ•°', 'æ­£æ ·æœ¬æ•°']
        ticker_stats['æ­£æ ·æœ¬ç‡%'] = (ticker_stats['æ­£æ ·æœ¬æ•°'] / ticker_stats['æ ·æœ¬æ•°'] * 100).round(3)
        print("\nğŸ“Š è‚¡ç¥¨åˆ†å¸ƒç»Ÿè®¡:")
        for ticker, stats in ticker_stats.iterrows():
            print(f"  {ticker}: {stats['æ ·æœ¬æ•°']:,} æ ·æœ¬, {stats['æ­£æ ·æœ¬æ•°']:,} æ­£æ ·æœ¬ ({stats['æ­£æ ·æœ¬ç‡%']:.3f}%)")
    else:
        # è‚¡ç¥¨å¤ªå¤šï¼Œåªæ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡
        ticker_stats = df.groupby('ticker')['y_label'].agg(['count', 'sum'])
        print(f"\nğŸ“Š è‚¡ç¥¨åˆ†å¸ƒæ±‡æ€»:")
        print(f"  å¹³å‡æ ·æœ¬æ•°: {ticker_stats['count'].mean():.0f}")
        print(f"  å¹³å‡æ­£æ ·æœ¬æ•°: {ticker_stats['sum'].mean():.1f}")
        print(f"  å¹³å‡æ­£æ ·æœ¬ç‡: {(ticker_stats['sum']/ticker_stats['count']).mean()*100:.3f}%")
    
    return df

# ---------- Enhanced Evaluation ------------------------------------------
def comprehensive_evaluation(y_true, y_pred_proba, y_pred_binary=None):
    """ç»¼åˆè¯„ä¼°"""
    if y_pred_binary is None:
        y_pred_binary = (y_pred_proba > 0.5).astype(int)
    
    # åŸºç¡€æŒ‡æ ‡
    pr_auc = average_precision_score(y_true, y_pred_proba)
    roc_auc = roc_auc_score(y_true, y_pred_proba)
    
    # è®¡ç®—æ ·æœ¬åˆ†å¸ƒ
    total_samples = len(y_true)
    positive_samples = y_true.sum()
    
    # Precision at K
    metrics = {'PR-AUC': pr_auc, 'ROC-AUC': roc_auc}
    
    print(f"\nğŸ“Š æ ·æœ¬åˆ†å¸ƒ:")
    print(f"æ€»æ ·æœ¬æ•°: {total_samples:,}")
    print(f"æ­£æ ·æœ¬æ•°: {positive_samples:,} ({positive_samples/total_samples*100:.3f}%)")
    print(f"\nğŸ“ˆ Precision@K% (å®é™…å€¼ / ç†è®ºæœ€å¤§å€¼ = è¾¾æˆç‡):")
    
    for k in [0.001, 0.005, 0.01, 0.05]:
        k_int = max(1, int(len(y_true) * k))
        top_k_idx = y_pred_proba.argsort()[::-1][:k_int]
        prec_k = y_true.iloc[top_k_idx].mean() if hasattr(y_true, 'iloc') else y_true[top_k_idx].mean()
        
        # è®¡ç®—ç†è®ºæœ€å¤§å€¼
        theoretical_max = min(positive_samples / k_int, 1.0)
        achievement_rate = prec_k / theoretical_max if theoretical_max > 0 else 0
        
        metrics[f'Precision@{k*100:.1f}%'] = prec_k
        metrics[f'Precision@{k*100:.1f}%_max'] = theoretical_max
        metrics[f'Precision@{k*100:.1f}%_achievement'] = achievement_rate
        
        print(f"  K={k*100:4.1f}%: {prec_k:.6f} / {theoretical_max:.6f} = {achievement_rate*100:5.1f}%")
    
    return metrics

# ---------- Main Function ------------------------------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_root", required=True)
    parser.add_argument("--train_regex", default="202503|202504")
    parser.add_argument("--valid_regex", default="202505")
    parser.add_argument("--sampling_method", choices=["none", "undersample", "stratified_undersample"], 
                       default="undersample", help="é‡‡æ ·æ–¹æ³•ï¼ˆç§»é™¤SMOTEï¼‰")
    parser.add_argument("--use_ensemble", action="store_true", help="ä½¿ç”¨æ¨¡å‹é›†æˆ")
    parser.add_argument("--optimize_params", action="store_true", help="ä¼˜åŒ–è¶…å‚æ•°")
    parser.add_argument("--n_trials", type=int, default=50, help="è¶…å‚æ•°ä¼˜åŒ–è¯•éªŒæ¬¡æ•°")
    parser.add_argument("--use_enhanced_labels", action="store_true", 
                       help="ä½¿ç”¨å¢å¼ºæ ‡ç­¾è€Œä¸æ˜¯åŸå§‹y_label")
    parser.add_argument("--label_type", choices=["composite_spoofing", "conservative_spoofing"], 
                       default="composite_spoofing",
                       help="ä½¿ç”¨å“ªç§å¢å¼ºæ ‡ç­¾ç±»å‹")
    parser.add_argument("--use_focal_loss", action="store_true", help="ä½¿ç”¨Focal Losså¤„ç†ä¸å¹³è¡¡æ•°æ®")
    parser.add_argument("--focal_alpha", type=float, default=0.25, help="Focal Loss alphaå‚æ•°")
    parser.add_argument("--focal_gamma", type=float, default=2.0, help="Focal Loss gammaå‚æ•°")
    parser.add_argument("--use_class_weight", action="store_true", help="ä½¿ç”¨ç±»åˆ«æƒé‡å¤„ç†ä¸å¹³è¡¡æ•°æ®")
    parser.add_argument("--class_weight_ratio", type=float, default=None, help="æ­£æ ·æœ¬æƒé‡æ¯”ä¾‹ï¼Œé»˜è®¤ä¸ºè´Ÿæ ·æœ¬æ•°/æ­£æ ·æœ¬æ•°")
    parser.add_argument("--eval_output_dir", type=str, default=None, 
                       help="è¯„ä¼°ç»“æœä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º results/evaluation_results")
    
    # è‚¡ç¥¨ç­›é€‰ç›¸å…³å‚æ•°
    parser.add_argument("--include_tickers", type=str, nargs="*", default=None,
                       help="æŒ‡å®šåŒ…å«çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œä¾‹å¦‚: --include_tickers 000001.SZ 000002.SZ")
    parser.add_argument("--exclude_tickers", type=str, nargs="*", default=None,
                       help="æŒ‡å®šæ’é™¤çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œä¾‹å¦‚: --exclude_tickers 000001.SZ 000002.SZ")
    parser.add_argument("--ticker_file", type=str, default=None,
                       help="ä»æ–‡ä»¶è¯»å–è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªè‚¡ç¥¨ä»£ç ")
    parser.add_argument("--max_tickers", type=int, default=None,
                       help="æœ€å¤§è‚¡ç¥¨æ•°é‡é™åˆ¶ï¼Œéšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„è‚¡ç¥¨")
    parser.add_argument("--ticker_selection_method", choices=["random", "by_volume", "by_positive_samples"], 
                       default="random", help="è‚¡ç¥¨é€‰æ‹©æ–¹æ³•ï¼šéšæœºã€æŒ‰äº¤æ˜“é‡ã€æŒ‰æ­£æ ·æœ¬æ•°")
    
    args = parser.parse_args()
    
    t0 = time.time()
    print("ğŸ” Loading and preparing data...")
    
    # Load data - é€‚é…æ–°çš„æ•°æ®æ¶æ„ï¼ˆç¬¦åˆdata.mdï¼‰
    # æ–°æ¶æ„ä½¿ç”¨ features/ å’Œ labels/ è€Œä¸æ˜¯ features_select/ å’Œ labels_select/
    feat_pats = [os.path.join(args.data_root, "features", "X_*.parquet")]
    lab_pats = [os.path.join(args.data_root, "labels", "labels_*.parquet")]
    
    # å¦‚æœæ–°è·¯å¾„ä¸å­˜åœ¨ï¼Œå›é€€åˆ°æ—§è·¯å¾„ä»¥ä¿æŒå…¼å®¹æ€§
    if not glob.glob(feat_pats[0]):
        print("âš ï¸ New architecture paths not found, trying legacy paths...")
        feat_pats = [os.path.join(args.data_root, "features_select", "X_*.parquet")]
        lab_pats = [os.path.join(args.data_root, "labels_select", "labels_*.parquet")]
    
    # åŠ è½½ç‰¹å¾æ•°æ®
    files = []
    for pat in feat_pats:
        files.extend(sorted(glob.glob(pat)))
    
    if not files:
        print(f"âŒ No feature files found. Tried:")
        print(f"  New: {args.data_root}/features/X_*.parquet")
        print(f"  Legacy: {args.data_root}/features_select/X_*.parquet")
        print("Please run the data processing pipeline first:")
        print("python scripts/data_process/complete_pipeline.py --base_data_dir <path> --output_root <path>")
        return
    
    print(f"ğŸ“Š Loading features from {len(files)} files...")
    df_feat = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)
    print(f"Features data shape: {df_feat.shape}")
    print(f"Feature columns: {df_feat.columns.tolist()}")
    
    # åŠ è½½æ ‡ç­¾æ•°æ®
    files = []
    for pat in lab_pats:
        files.extend(sorted(glob.glob(pat)))
    
    # å¦‚æœæ ‡å‡†ä½ç½®æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œå°è¯•labels_enhancedç›®å½•
    if not files:
        enhanced_lab_pat = os.path.join(args.data_root, "labels_enhanced", "labels_*.parquet")
        files.extend(sorted(glob.glob(enhanced_lab_pat)))
        if files:
            print(f"âœ… Found {len(files)} label files in labels_enhanced directory")
    
    if not files:
        print(f"âŒ No label files found. Tried:")
        print(f"  New: {args.data_root}/labels/labels_*.parquet")
        print(f"  Legacy: {args.data_root}/labels_select/labels_*.parquet")
        print(f"  Enhanced: {args.data_root}/labels_enhanced/labels_*.parquet")
        print("Please run the data processing pipeline first.")
        return
    
    print(f"ğŸ“Š Loading labels from {len(files)} files...")
    df_lab = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)
    print(f"Labels data shape: {df_lab.shape}")
    print(f"Label columns: {df_lab.columns.tolist()}")
    
    # æ£€æŸ¥å¢å¼ºæ ‡ç­¾æ˜¯å¦å¯ç”¨
    if args.use_enhanced_labels:
        if args.label_type not in df_lab.columns:
            print(f"âŒ Enhanced label '{args.label_type}' not found in labels.")
            print(f"Available label columns: {df_lab.columns.tolist()}")
            print("Please ensure ETL was run with --enhanced_labels flag.")
            return
        print(f"âœ… Using enhanced label: {args.label_type}")
    
    # åˆå¹¶æ•°æ®
    df = df_feat.merge(df_lab, on=["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·"], how="inner")
    print(f"Merged data shape: {df.shape}")
    print(f"Merged columns: {df.columns.tolist()}")
    
    # è®¾ç½®ç›®æ ‡å˜é‡
    if args.use_enhanced_labels:
        df['y_label'] = df[args.label_type]
        print(f"Using {args.label_type} as target variable")
        print(f"Target distribution: {df['y_label'].value_counts().to_dict()}")
    else:
        if 'y_label' not in df.columns:
            print("âŒ y_label column not found in data")
            return
        print("Using original y_label as target variable")
        print(f"Target distribution: {df['y_label'].value_counts().to_dict()}")
    
    # åº”ç”¨è‚¡ç¥¨ç­›é€‰
    df = apply_ticker_filtering(df, args)
    
    # Split data
    train_mask = df["è‡ªç„¶æ—¥"].astype(str).str.contains(args.train_regex)
    valid_mask = df["è‡ªç„¶æ—¥"].astype(str).str.contains(args.valid_regex)
    
    df_train = df[train_mask].copy()
    df_valid = df[valid_mask].copy()
    
    print(f"Training: {len(df_train):,}, Validation: {len(df_valid):,}")
    
    # Enhanced feature engineering
    df_train = enhance_features(df_train)
    df_valid = enhance_features(df_valid)
    
    # ä½¿ç”¨æ˜ç¡®å®šä¹‰çš„åˆ—ï¼Œé¿å…æ•°æ®æ³„éœ²
    print("ğŸ›¡ï¸ ä½¿ç”¨æ˜ç¡®å®šä¹‰çš„ç‰¹å¾åˆ—...")
    
    # å¯¼å…¥åˆ—å®šä¹‰æ¨¡å—
    try:
        import sys
        from pathlib import Path
        
        # å¯¼å…¥ç‰¹å¾åˆ—å®šä¹‰
        sys.path.append(str(Path(__file__).parent.parent / "data_process" / "features"))
        from feature_generator import get_training_feature_columns, get_key_columns, get_leakage_risk_columns
        
        # å¯¼å…¥æ ‡ç­¾åˆ—å®šä¹‰
        sys.path.append(str(Path(__file__).parent.parent / "data_process" / "labels"))
        from label_generator import get_training_target_columns, get_label_columns
        
        # è·å–å¯ç”¨äºè®­ç»ƒçš„ç‰¹å¾åˆ—
        feature_cols = get_training_feature_columns(include_enhanced=True)
        
        # è·å–å®é™…å­˜åœ¨çš„ç‰¹å¾åˆ—
        available_cols = df_train.columns.tolist()
        actual_feature_cols = [col for col in feature_cols if col in available_cols]
        
        # æ£€æŸ¥ç¼ºå¤±çš„é‡è¦ç‰¹å¾
        missing_features = [col for col in feature_cols if col not in available_cols]
        if missing_features:
            print(f"âš ï¸ ç¼ºå¤± {len(missing_features)} ä¸ªå®šä¹‰çš„ç‰¹å¾: {missing_features[:10]}{'...' if len(missing_features) > 10 else ''}")
        
        # æ£€æŸ¥å¯èƒ½çš„æ³„éœ²é£é™©åˆ—
        leakage_risk_cols = get_leakage_risk_columns()
        potential_leakage = [col for col in available_cols if col in leakage_risk_cols]
        if potential_leakage:
            print(f"ğŸš¨ æ£€æµ‹åˆ°æ½œåœ¨æ³„éœ²é£é™©åˆ—: {potential_leakage}")
            # ä»ç‰¹å¾åˆ—ä¸­ç§»é™¤
            actual_feature_cols = [col for col in actual_feature_cols if col not in potential_leakage]
        
        feature_cols = actual_feature_cols
        print(f"âœ… ä½¿ç”¨ {len(feature_cols)} ä¸ªå®‰å…¨ç‰¹å¾")
        
        # æ˜¾ç¤ºç‰¹å¾ç±»åˆ«ç»Ÿè®¡
        if len(feature_cols) > 0:
            base_features = [col for col in feature_cols if col in available_cols and 
                           any(base_col in col for base_col in ['mid_price', 'spread', 'log_qty', 'is_buy', 'time_', 'orders_', 'book_'])]
            enhanced_features = [col for col in feature_cols if col not in base_features]
            print(f"  åŸºç¡€ç‰¹å¾: {len(base_features)}, æ‰©å±•ç‰¹å¾: {len(enhanced_features)}")
        
    except ImportError as e:
        print(f"âš ï¸ æ— æ³•å¯¼å…¥åˆ—å®šä¹‰æ¨¡å—ï¼Œä½¿ç”¨åŸºç¡€æ¸…ç†: {e}")
        
        # åŸºç¡€æ¸…ç†ï¼šæ‰‹åŠ¨æ’é™¤å·²çŸ¥æ³„éœ²ç‰¹å¾
        leakage_cols = [
            "å­˜æ´»æ—¶é—´_ms", "äº‹ä»¶_datetime", "æˆäº¤ä»·æ ¼", "æˆäº¤æ•°é‡", "äº‹ä»¶ç±»å‹",
            "is_cancel_event", "is_trade_event",
            "flag_R1", "flag_R2", "enhanced_spoofing_liberal", 
            "enhanced_spoofing_moderate", "enhanced_spoofing_strict"
        ]
        
        id_cols = ["è‡ªç„¶æ—¥", "ticker", "äº¤æ˜“æ‰€å§”æ‰˜å·", "y_label"]
        exclude_cols = id_cols + leakage_cols
        feature_cols = [col for col in df_train.columns if col not in exclude_cols]
        
        # ç§»é™¤éæ•°å€¼åˆ—
        non_numeric_cols = []
        for col in feature_cols:
            dtype = df_train[col].dtype
            if dtype == 'object' or 'datetime' in str(dtype):
                non_numeric_cols.append(col)
        
        feature_cols = [col for col in feature_cols if col not in non_numeric_cols]
        
        if non_numeric_cols:
            print(f"âš ï¸ ç§»é™¤ {len(non_numeric_cols)} ä¸ªéæ•°å€¼åˆ—: {non_numeric_cols[:5]}{'...' if len(non_numeric_cols) > 5 else ''}")
        
        print(f"ä½¿ç”¨ {len(feature_cols)} ä¸ªç‰¹å¾ï¼ˆåŸºç¡€æ¸…ç†ï¼‰")
    
    X_tr = df_train[feature_cols]
    y_tr = df_train["y_label"]
    X_va = df_valid[feature_cols]
    y_va = df_valid["y_label"]
    
    # Handle missing values
    X_tr = X_tr.fillna(0)
    X_va = X_va.fillna(0)
    
    print(f"Class distribution - Train: {y_tr.value_counts().to_dict()}")
    print(f"Class distribution - Valid: {y_va.value_counts().to_dict()}")
    
    # Advanced sampling (ç§»é™¤SMOTEï¼Œä½¿ç”¨ä¸‹é‡‡æ ·)
    if args.sampling_method != "none":
        X_tr, y_tr = advanced_sampling(X_tr, y_tr, method=args.sampling_method)
    
    # Model training
    if args.use_ensemble:
        # Train ensemble
        model = EnsembleClassifier()
        model.fit(X_tr, y_tr, X_va, y_va)
    else:
        # Single model with optional hyperparameter optimization
        base_params = {
            'objective': 'binary',
            'metric': 'average_precision',
            'learning_rate': 0.03,
            'num_leaves': 31,
            'max_depth': 6,
            'n_estimators': 3000,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 10,
            'reg_lambda': 10,
            'random_state': 42,
            'verbose': -1
        }
        
        if args.use_focal_loss:
            print(f"ğŸ¯ ä½¿ç”¨Focal Loss (alpha={args.focal_alpha}, gamma={args.focal_gamma})")
            base_params.update({
                'objective': focal_loss_lgb(args.focal_alpha, args.focal_gamma),
                'metric': 'None'  # ä½¿ç”¨è‡ªå®šä¹‰objectiveæ—¶éœ€è¦è®¾ç½®ä¸ºNone
            })
            model = lgb.LGBMClassifier(**base_params)
        elif args.use_class_weight:
            # è®¡ç®—ç±»åˆ«æƒé‡
            neg_count = (y_tr == 0).sum()
            pos_count = (y_tr == 1).sum()
            
            if args.class_weight_ratio is not None:
                scale_pos_weight = args.class_weight_ratio
            else:
                scale_pos_weight = neg_count / pos_count
            
            print(f"ğŸ¯ ä½¿ç”¨ç±»åˆ«æƒé‡ (scale_pos_weight={scale_pos_weight:.2f})")
            print(f"   è´Ÿæ ·æœ¬: {neg_count:,}, æ­£æ ·æœ¬: {pos_count:,}, æ¯”ä¾‹: {neg_count/pos_count:.1f}:1")
            
            base_params['scale_pos_weight'] = scale_pos_weight
            model = lgb.LGBMClassifier(**base_params)
        elif args.optimize_params:
            best_params = optimize_lgb_params(X_tr, y_tr, X_va, y_va, args.n_trials)
            model = lgb.LGBMClassifier(**best_params, n_estimators=1000, random_state=42, verbose=-1)
        else:
            model = lgb.LGBMClassifier(**base_params)
        
        # ä¿®å¤early_stoppingå‚æ•°
        if args.use_focal_loss:
            # Focal Lossè®­ç»ƒï¼Œä½¿ç”¨å›ºå®šè½®æ•°ï¼Œä¸ä½¿ç”¨early stopping
            print("ğŸ“ Focal Lossæ¨¡å¼ï¼šä½¿ç”¨å›ºå®š500è½®è®­ç»ƒï¼ˆæ— early stoppingï¼‰")
            model.n_estimators = 500
            model.fit(X_tr, y_tr)
        else:
            try:
                model.fit(
                    X_tr, y_tr,
                    eval_set=[(X_va, y_va)],
                    early_stopping_rounds=100,
                    verbose=False
                )
            except TypeError:
                # å¤„ç†æ–°ç‰ˆæœ¬LightGBMçš„å…¼å®¹æ€§é—®é¢˜
                from lightgbm import early_stopping
                model.fit(
                    X_tr, y_tr,
                    eval_set=[(X_va, y_va)],
                    callbacks=[early_stopping(100)]
                )
    
    # Evaluation
    print("\nğŸ“Š Comprehensive Evaluation:")
    if args.use_focal_loss:
        # Focal Lossæ¨¡å¼ï¼špredictè¿”å›1Dæ•°ç»„ï¼Œç›´æ¥ä½¿ç”¨
        y_pred_proba = model.predict(X_va)
        # å°†logitsè½¬æ¢ä¸ºæ¦‚ç‡
        y_pred_proba = 1 / (1 + np.exp(-y_pred_proba))
    else:
        y_pred_proba = model.predict_proba(X_va)[:, 1]
    
    # æ•´ä½“è¯„ä¼°
    print("\nğŸŒ Overall Performance:")
    metrics = comprehensive_evaluation(y_va, y_pred_proba)
    for metric, value in metrics.items():
        print(f"{metric}: {value:.6f}")
    
    # åˆ†è‚¡ç¥¨è¯„ä¼°
    if 'ticker' in df_valid.columns and df_valid['ticker'].nunique() > 1:
        print("\nğŸ“ˆ Per-Ticker Evaluation:")
        ticker_metrics = {}
        
        for ticker in sorted(df_valid['ticker'].unique()):
            ticker_mask = df_valid['ticker'] == ticker
            y_ticker = y_va[ticker_mask]
            pred_ticker = y_pred_proba[ticker_mask]
            
            if len(y_ticker) > 0 and y_ticker.sum() > 0:  # ç¡®ä¿æœ‰æ­£æ ·æœ¬
                print(f"\nğŸ“Š {ticker}:")
                ticker_eval = comprehensive_evaluation(y_ticker, pred_ticker)
                ticker_metrics[ticker] = ticker_eval
                
                # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
                key_metrics = ['PR-AUC', 'ROC-AUC', 'Precision@0.1%', 'Precision@0.5%', 'Precision@1.0%']
                for metric in key_metrics:
                    if metric in ticker_eval:
                        print(f"  {metric}: {ticker_eval[metric]:.6f}")
            else:
                print(f"\nğŸ“Š {ticker}: æ— æ­£æ ·æœ¬æˆ–æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è¯„ä¼°")
        
        # è‚¡ç¥¨è¯„ä¼°ç»“æœæ±‡æ€»è¡¨
        if ticker_metrics:
            print(f"\nğŸ“‹ Per-Ticker Summary:")
            print(f"{'Ticker':<12} {'PR-AUC':<8} {'ROC-AUC':<8} {'P@0.1%':<8} {'P@0.5%':<8} {'P@1.0%':<8} {'Samples':<8} {'Positive':<8}")
            print("-" * 80)
            
            for ticker in sorted(ticker_metrics.keys()):
                tm = ticker_metrics[ticker]
                ticker_mask = df_valid['ticker'] == ticker
                samples = ticker_mask.sum()
                positive = y_va[ticker_mask].sum()
                
                print(f"{ticker:<12} "
                      f"{tm.get('PR-AUC', 0):<8.4f} "
                      f"{tm.get('ROC-AUC', 0):<8.4f} "
                      f"{tm.get('Precision@0.1%', 0):<8.4f} "
                      f"{tm.get('Precision@0.5%', 0):<8.4f} "
                      f"{tm.get('Precision@1.0%', 0):<8.4f} "
                      f"{samples:<8,} "
                      f"{positive:<8,}")
            
            # ä¿å­˜åˆ†è‚¡ç¥¨è¯„ä¼°ç»“æœ
            metrics['per_ticker_metrics'] = ticker_metrics
    else:
        print("\nâš ï¸ å•è‚¡ç¥¨æ•°æ®æˆ–æ— è‚¡ç¥¨ä¿¡æ¯ï¼Œè·³è¿‡åˆ†è‚¡ç¥¨è¯„ä¼°")
    
    # Feature importance (for single models)
    if hasattr(model, 'feature_importances_'):
        feature_imp = pd.DataFrame({
            'feature': feature_cols,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\nğŸ” Top 20 Feature Importance:")
        for i, (feat, imp) in enumerate(feature_imp.head(20).values):
            print(f"  {i+1:2d}. {feat:<30} {imp:>8.0f}")
    
    print(f"\nTotal training time: {time.time()-t0:.1f}s")
    
    # Save comprehensive evaluation results
    method_components = [f"Enhanced+{args.sampling_method}"]
    if args.use_ensemble:
        method_components.append("Ensemble")
    if args.optimize_params:
        method_components.append("Optimized")
    if args.use_focal_loss:
        method_components.append(f"FocalLoss(Î±={args.focal_alpha},Î³={args.focal_gamma})")
    if args.use_class_weight:
        method_components.append(f"ClassWeight({scale_pos_weight:.0f})")
    
    method_name = "+".join(method_components)
    
    # åˆ›å»ºè¯„ä¼°ç»“æœä¿å­˜ç›®å½•
    if args.eval_output_dir is not None:
        eval_output_dir = args.eval_output_dir
    else:
        eval_output_dir = os.path.join("results", "evaluation_results")
    
    os.makedirs(eval_output_dir, exist_ok=True)
    print(f"ğŸ“ è¯„ä¼°ç»“æœå°†ä¿å­˜åˆ°: {eval_output_dir}")
    
    # ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºæ–‡ä»¶å‘½å
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. ä¿å­˜åŸºç¡€ç»“æœ
    results = {
        'method': method_name,
        'timestamp': timestamp,
        'training_time': time.time()-t0,
        'n_features': len(feature_cols),
        'positive_ratio': y_tr.mean(),
        'train_samples': len(y_tr),
        'valid_samples': len(y_va),
        'train_positive': int(y_tr.sum()),
        'valid_positive': int(y_va.sum()),
        **metrics
    }
    
    # ä¿å­˜åŸºç¡€ç»“æœåˆ°JSON
    import json
    results_file = os.path.join(eval_output_dir, f"evaluation_results_{timestamp}.json")
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"âœ… åŸºç¡€è¯„ä¼°ç»“æœå·²ä¿å­˜: {results_file}")
    
    # 2. ä¿å­˜è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
    y_pred_binary = (y_pred_proba > 0.5).astype(int)
    
    try:
        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        from sklearn.metrics import classification_report, confusion_matrix
        
        class_report = classification_report(y_va, y_pred_binary, output_dict=True)
        
        # ä¿å­˜åˆ†ç±»æŠ¥å‘Š
        class_report_file = os.path.join(eval_output_dir, f"classification_report_{timestamp}.json")
        with open(class_report_file, 'w') as f:
            json.dump(class_report, f, indent=2)
        print(f"âœ… åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜: {class_report_file}")
        
        # ç”Ÿæˆå¹¶ä¿å­˜æ··æ·†çŸ©é˜µ
        conf_matrix = confusion_matrix(y_va, y_pred_binary)
        
        conf_matrix_data = {
            'confusion_matrix': conf_matrix.tolist(),
            'labels': ['Non-Spoofing', 'Spoofing'],
            'true_negatives': int(conf_matrix[0, 0]),
            'false_positives': int(conf_matrix[0, 1]),
            'false_negatives': int(conf_matrix[1, 0]),
            'true_positives': int(conf_matrix[1, 1])
        }
        
        # å¦‚æœæœ‰åˆ†è‚¡ç¥¨è¯„ä¼°ï¼Œä¹Ÿä¸ºæ¯ä¸ªè‚¡ç¥¨ç”Ÿæˆæ··æ·†çŸ©é˜µ
        if 'per_ticker_metrics' in metrics:
            ticker_confusion_matrices = {}
            for ticker in metrics['per_ticker_metrics'].keys():
                ticker_mask = df_valid['ticker'] == ticker
                y_ticker = y_va[ticker_mask]
                pred_ticker_binary = y_pred_binary[ticker_mask]
                
                if len(y_ticker) > 0:
                    ticker_conf_matrix = confusion_matrix(y_ticker, pred_ticker_binary)
                    ticker_confusion_matrices[ticker] = {
                        'confusion_matrix': ticker_conf_matrix.tolist(),
                        'true_negatives': int(ticker_conf_matrix[0, 0]),
                        'false_positives': int(ticker_conf_matrix[0, 1]),
                        'false_negatives': int(ticker_conf_matrix[1, 0]),
                        'true_positives': int(ticker_conf_matrix[1, 1])
                    }
            
            conf_matrix_data['per_ticker_confusion_matrices'] = ticker_confusion_matrices
        
        conf_matrix_file = os.path.join(eval_output_dir, f"confusion_matrix_{timestamp}.json")
        with open(conf_matrix_file, 'w') as f:
            json.dump(conf_matrix_data, f, indent=2)
        print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {conf_matrix_file}")
        
    except Exception as e:
        print(f"âš ï¸ åˆ†ç±»æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")
    
    # 3. ä¿å­˜é¢„æµ‹ç»“æœï¼ˆæ ·æœ¬ï¼‰
    try:
        # ä¿å­˜é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        pred_results = {
            'true_labels': y_va.tolist() if hasattr(y_va, 'tolist') else list(y_va),
            'predicted_probabilities': y_pred_proba.tolist() if hasattr(y_pred_proba, 'tolist') else list(y_pred_proba),
            'predicted_binary': y_pred_binary.tolist() if hasattr(y_pred_binary, 'tolist') else list(y_pred_binary)
        }
        
        # åªä¿å­˜å‰10000ä¸ªæ ·æœ¬ä»¥èŠ‚çœç©ºé—´
        if len(pred_results['true_labels']) > 10000:
            pred_results = {
                'true_labels': pred_results['true_labels'][:10000],
                'predicted_probabilities': pred_results['predicted_probabilities'][:10000],
                'predicted_binary': pred_results['predicted_binary'][:10000],
                'note': 'Only first 10,000 samples saved for space efficiency'
            }
        
        pred_results_file = os.path.join(eval_output_dir, f"prediction_results_{timestamp}.json")
        with open(pred_results_file, 'w') as f:
            json.dump(pred_results, f, indent=2)
        print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜: {pred_results_file}")
        
    except Exception as e:
        print(f"âš ï¸ é¢„æµ‹ç»“æœä¿å­˜å¤±è´¥: {e}")
    
    # 4. ä¿å­˜è¯„ä¼°å¯è§†åŒ–
    try:
        from sklearn.metrics import precision_recall_curve, roc_curve
        import matplotlib.pyplot as plt
        
        # åˆ›å»ºå›¾è¡¨ä¿å­˜ç›®å½•
        plots_dir = os.path.join(eval_output_dir, "plots")
        os.makedirs(plots_dir, exist_ok=True)
        
        # PRæ›²çº¿
        precision, recall, _ = precision_recall_curve(y_va, y_pred_proba)
        
        plt.figure(figsize=(10, 6))
        plt.subplot(1, 2, 1)
        plt.plot(recall, precision, marker='.')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title(f'PR Curve (AUC={metrics["PR-AUC"]:.4f})')
        plt.grid(True)
        
        # ROCæ›²çº¿
        fpr, tpr, _ = roc_curve(y_va, y_pred_proba)
        
        plt.subplot(1, 2, 2)
        plt.plot(fpr, tpr, marker='.')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve (AUC={metrics["ROC-AUC"]:.4f})')
        plt.grid(True)
        
        plt.tight_layout()
        
        curves_plot_file = os.path.join(plots_dir, f"pr_roc_curves_{timestamp}.png")
        plt.savefig(curves_plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… PR/ROCæ›²çº¿å›¾å·²ä¿å­˜: {curves_plot_file}")
        
        # é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 3, 1)
        plt.hist(y_pred_proba[y_va == 0], bins=50, alpha=0.7, label='Non-Spoofing', color='blue')
        plt.hist(y_pred_proba[y_va == 1], bins=50, alpha=0.7, label='Spoofing', color='red')
        plt.xlabel('Predicted Probability')
        plt.ylabel('Frequency')
        plt.title('Prediction Probability Distribution')
        plt.legend()
        plt.grid(True)
        
        # Precision@Kå¯è§†åŒ–
        k_values = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1]
        precisions_at_k = []
        theoretical_maxs = []
        
        total_pos = y_va.sum()
        for k in k_values:
            k_int = max(1, int(len(y_va) * k))
            top_k_idx = y_pred_proba.argsort()[::-1][:k_int]
            prec_k = y_va.iloc[top_k_idx].mean() if hasattr(y_va, 'iloc') else y_va[top_k_idx].mean()
            theoretical_max = min(total_pos / k_int, 1.0)
            precisions_at_k.append(prec_k)
            theoretical_maxs.append(theoretical_max)
        
        plt.subplot(1, 3, 2)
        x_pos = range(len(k_values))
        plt.bar([x - 0.2 for x in x_pos], precisions_at_k, 0.4, label='Actual', alpha=0.8)
        plt.bar([x + 0.2 for x in x_pos], theoretical_maxs, 0.4, label='Theoretical Max', alpha=0.8)
        plt.xlabel('Top K%')
        plt.ylabel('Precision@K')
        plt.title('Precision at Different K Values')
        plt.xticks(x_pos, [f'{k*100:.1f}%' for k in k_values])
        plt.legend()
        plt.grid(True)
        
        # æ··æ·†çŸ©é˜µå¯è§†åŒ–
        plt.subplot(1, 3, 3)
        import seaborn as sns
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['Non-Spoofing', 'Spoofing'],
                   yticklabels=['Non-Spoofing', 'Spoofing'])
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        
        plt.tight_layout()
        
        analysis_plot_file = os.path.join(plots_dir, f"detailed_analysis_{timestamp}.png")
        plt.savefig(analysis_plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… è¯¦ç»†åˆ†æå›¾å·²ä¿å­˜: {analysis_plot_file}")
        
    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–ä¿å­˜å¤±è´¥: {e}")
    
    # 5. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šæ–‡æ¡£
    try:
        report_file = os.path.join(eval_output_dir, f"evaluation_report_{timestamp}.md")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"""# Spoofing Detection Model Evaluation Report

## å®éªŒä¿¡æ¯
- **å®éªŒæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **æ¨¡å‹æ–¹æ³•**: {method_name}
- **è®­ç»ƒæ—¶é—´**: {results['training_time']:.1f}ç§’
- **ç‰¹å¾æ•°é‡**: {results['n_features']}

## æ•°æ®åˆ†å¸ƒ
- **è®­ç»ƒé›†**: {results['train_samples']:,} æ ·æœ¬ ({results['train_positive']:,} æ­£æ ·æœ¬, {results['train_positive']/results['train_samples']*100:.3f}%)
- **éªŒè¯é›†**: {results['valid_samples']:,} æ ·æœ¬ ({results['valid_positive']:,} æ­£æ ·æœ¬, {results['valid_positive']/results['valid_samples']*100:.3f}%)

## æ ¸å¿ƒæŒ‡æ ‡
- **PR-AUC**: {metrics['PR-AUC']:.6f}
- **ROC-AUC**: {metrics['ROC-AUC']:.6f}

## Precision@K åˆ†æ
""")
            
            for k in [0.001, 0.005, 0.01, 0.05]:
                if f'Precision@{k*100:.1f}%' in metrics:
                    prec = metrics[f'Precision@{k*100:.1f}%']
                    max_prec = metrics[f'Precision@{k*100:.1f}%_max']
                    achievement = metrics[f'Precision@{k*100:.1f}%_achievement']
                    f.write(f"- **K={k*100:4.1f}%**: {prec:.6f} / {max_prec:.6f} = {achievement*100:5.1f}% è¾¾æˆç‡\n")
            
            f.write(f"""
## æ··æ·†çŸ©é˜µ
- **True Negatives**: {conf_matrix_data['true_negatives']:,}
- **False Positives**: {conf_matrix_data['false_positives']:,}
- **False Negatives**: {conf_matrix_data['false_negatives']:,}
- **True Positives**: {conf_matrix_data['true_positives']:,}
""")
            
            # å¦‚æœæœ‰åˆ†è‚¡ç¥¨è¯„ä¼°ç»“æœï¼Œæ·»åŠ åˆ°æŠ¥å‘Šä¸­
            if 'per_ticker_metrics' in metrics:
                f.write(f"""
## åˆ†è‚¡ç¥¨è¯„ä¼°ç»“æœ

| è‚¡ç¥¨ä»£ç  | PR-AUC | ROC-AUC | P@0.1% | P@0.5% | P@1.0% | æ ·æœ¬æ•° | æ­£æ ·æœ¬æ•° |
|---------|--------|---------|---------|---------|---------|---------|----------|
""")
                for ticker in sorted(metrics['per_ticker_metrics'].keys()):
                    tm = metrics['per_ticker_metrics'][ticker]
                    ticker_mask = df_valid['ticker'] == ticker
                    samples = ticker_mask.sum()
                    positive = y_va[ticker_mask].sum()
                    
                    f.write(f"| {ticker} | {tm.get('PR-AUC', 0):.4f} | "
                           f"{tm.get('ROC-AUC', 0):.4f} | "
                           f"{tm.get('Precision@0.1%', 0):.4f} | "
                           f"{tm.get('Precision@0.5%', 0):.4f} | "
                           f"{tm.get('Precision@1.0%', 0):.4f} | "
                           f"{samples:,} | {positive:,} |\n")
                
                f.write(f"""
### åˆ†è‚¡ç¥¨æ··æ·†çŸ©é˜µ

""")
                if 'per_ticker_confusion_matrices' in conf_matrix_data:
                    for ticker, ticker_conf in conf_matrix_data['per_ticker_confusion_matrices'].items():
                        f.write(f"""
**{ticker}:**
- True Negatives: {ticker_conf['true_negatives']:,}
- False Positives: {ticker_conf['false_positives']:,}
- False Negatives: {ticker_conf['false_negatives']:,}
- True Positives: {ticker_conf['true_positives']:,}
""")
            
            f.write(f"""
## è®­ç»ƒå‚æ•°
```json
{json.dumps(vars(args), indent=2)}
```

## æ–‡ä»¶è¯´æ˜
- `evaluation_results_{timestamp}.json`: åŸºç¡€è¯„ä¼°æŒ‡æ ‡
- `classification_report_{timestamp}.json`: è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
- `confusion_matrix_{timestamp}.json`: æ··æ·†çŸ©é˜µæ•°æ®
- `prediction_results_{timestamp}.json`: é¢„æµ‹ç»“æœæ ·æœ¬
- `plots/pr_roc_curves_{timestamp}.png`: PRå’ŒROCæ›²çº¿å›¾
- `plots/detailed_analysis_{timestamp}.png`: è¯¦ç»†åˆ†æå›¾è¡¨

## ä½¿ç”¨å»ºè®®
åŸºäºå½“å‰ç»“æœï¼Œå»ºè®®ï¼š
""")
            
            # æ ¹æ®ç»“æœç»™å‡ºå»ºè®®
            pr_auc = metrics['PR-AUC']
            if pr_auc >= 0.3:
                f.write("- âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨\n")
            elif pr_auc >= 0.1:
                f.write("- âš ï¸ æ¨¡å‹æ€§èƒ½ä¸­ç­‰ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–\n")
            else:
                f.write("- âŒ æ¨¡å‹æ€§èƒ½è¾ƒå·®ï¼Œéœ€è¦é‡æ–°è®¾è®¡ç‰¹å¾å·¥ç¨‹å’Œç®—æ³•\n")
            
            if metrics.get('Precision@0.1%', 0) >= 0.5:
                f.write("- âœ… Top 0.1% ç²¾åº¦è‰¯å¥½ï¼Œé€‚åˆé«˜ç½®ä¿¡åº¦é¢„è­¦\n")
            else:
                f.write("- âš ï¸ Top 0.1% ç²¾åº¦ä¸è¶³ï¼Œå»ºè®®è°ƒæ•´å†³ç­–é˜ˆå€¼\n")
        
        print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
    except Exception as e:
        print(f"âš ï¸ è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    
    # åŒæ—¶ä¿ç•™åŸæœ‰çš„ç®€å•ä¿å­˜æ–¹å¼ï¼ˆå‘åå…¼å®¹ï¼‰
    simple_results_file = os.path.join(args.data_root, "enhanced_results.json")
    with open(simple_results_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"âœ… å…¼å®¹æ€§ç»“æœæ–‡ä»¶å·²ä¿å­˜: {simple_results_file}")
    
    print(f"\nğŸ“ æ‰€æœ‰è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°ç›®å½•: {eval_output_dir}")
    print(f"ğŸ“‹ æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š: {report_file}")
    
    # Save model and features for analysis
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹ä¾›åˆ†æè„šæœ¬ä½¿ç”¨...")
    model_output_dir = os.path.join("results", "trained_models")
    os.makedirs(model_output_dir, exist_ok=True)
    
    # ç¡®å®šæ¨¡å‹åç§°
    model_name = f"spoofing_model_{results['method'].replace('+', '_').replace('(', '').replace(')', '').replace(',', '').replace('=', '').replace(' ', '_')}"
    
    try:
        # ä¿å­˜æ¨¡å‹å’Œç‰¹å¾
        if args.use_ensemble:
            # é›†æˆæ¨¡å‹ï¼šä¿å­˜ä¸»è¦çš„LightGBMæ¨¡å‹
            if hasattr(model, 'fitted_models') and 'lgb' in model.fitted_models:
                main_model = model.fitted_models['lgb']
                model_path = os.path.join(model_output_dir, f"{model_name}.pkl")
                
                import pickle
                with open(model_path, 'wb') as f:
                    pickle.dump({
                        'model': main_model,
                        'ensemble_model': model,  # å®Œæ•´é›†æˆæ¨¡å‹
                        'features': feature_cols,
                        'model_type': 'ensemble_lgb',
                        'results': results,
                        'training_params': vars(args)
                    }, f)
                print(f"âœ… é›†æˆæ¨¡å‹å·²ä¿å­˜: {model_path}")
            else:
                print("âš ï¸ é›†æˆæ¨¡å‹ä¿å­˜å¤±è´¥ï¼šæœªæ‰¾åˆ°LightGBMå­æ¨¡å‹")
        else:
            # å•æ¨¡å‹
            model_path = os.path.join(model_output_dir, f"{model_name}.pkl")
            
            import pickle
            with open(model_path, 'wb') as f:
                pickle.dump({
                    'model': model,
                    'features': feature_cols,
                    'model_type': 'lightgbm_single',
                    'results': results,
                    'training_params': vars(args)
                }, f)
            print(f"âœ… å•æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜ç‰¹å¾åˆ—è¡¨
        features_path = os.path.join(model_output_dir, f"{model_name}_features.json")
        with open(features_path, 'w') as f:
            json.dump(feature_cols, f, indent=2)
        print(f"âœ… ç‰¹å¾åˆ—è¡¨å·²ä¿å­˜: {features_path}")
        
        # ä¿å­˜ä½¿ç”¨è¯´æ˜
        readme_path = os.path.join(model_output_dir, "ä½¿ç”¨è¯´æ˜.md")
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(f"""# è®­ç»ƒå¥½çš„Spoofingæ£€æµ‹æ¨¡å‹

## æ¨¡å‹ä¿¡æ¯
- **æ¨¡å‹æ–‡ä»¶**: `{model_name}.pkl`
- **ç‰¹å¾æ–‡ä»¶**: `{model_name}_features.json`
- **æ¨¡å‹ç±»å‹**: {results['method']}
- **ç‰¹å¾æ•°é‡**: {len(feature_cols)}
- **è®­ç»ƒæ—¶é—´**: {results['training_time']:.1f}s
- **PR-AUC**: {results.get('PR-AUC', 'N/A')}
- **ROC-AUC**: {results.get('ROC-AUC', 'N/A')}

## åœ¨æ“çºµæ—¶æ®µåˆ†æä¸­ä½¿ç”¨

### ä½¿ç”¨pklæ ¼å¼ï¼ˆæ¨èï¼‰ï¼š
```bash
python scripts/analysis/manipulation_detection_heatmap.py \\
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \\
  --model_path "{os.path.relpath(model_path)}" \\
  --output_dir "results/manipulation_analysis"

```

### å¦‚æœéœ€è¦åˆ†ç¦»çš„ç‰¹å¾æ–‡ä»¶ï¼š
```bash
python scripts/analysis/manipulation_detection_heatmap.py \\
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \\
  --model_path "{os.path.relpath(model_path)}" \\
  --model_features_path "{os.path.relpath(features_path)}" \\
  --output_dir "results/manipulation_analysis"
```

## è®­ç»ƒå‚æ•°
```json
{json.dumps(vars(args), indent=2)}
```
""")
        print(f"âœ… ä½¿ç”¨è¯´æ˜å·²ä¿å­˜: {readme_path}")
        
        # è¾“å‡ºå®Œæ•´çš„åˆ†æå‘½ä»¤
        print(f"\nğŸ¯ ç°åœ¨å¯ä»¥è¿è¡Œæ“çºµæ—¶æ®µåˆ†æ:")
        print(f"python scripts/analysis/manipulation_detection_heatmap.py \\")
        print(f"  --data_root \"{args.data_root}\" \\")
        print(f"  --model_path \"{os.path.relpath(model_path)}\" \\")
        print(f"  --output_dir \"results/manipulation_analysis\"")
        
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        print("æ¨¡å‹æœªä¿å­˜ï¼Œä½†è®­ç»ƒç»“æœå·²è®°å½•")

if __name__ == "__main__":
    main() 
    
"""
# ä½¿ç”¨æ˜ç¡®å®šä¹‰çš„ç‰¹å¾å’Œæ ‡ç­¾åˆ—è¿›è¡Œè®­ç»ƒ
python scripts/train/train_baseline_enhanced_fixed.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --train_regex "202503|202504" \
  --valid_regex "202505" \
  --sampling_method "none" \
  --use_ensemble \
  --eval_output_dir "results/my_evaluation_results"

# æŒ‡å®šè‚¡ç¥¨ä»£ç è¿›è¡Œè®­ç»ƒçš„ç¤ºä¾‹ï¼š

# 1. åªè®­ç»ƒç‰¹å®šè‚¡ç¥¨
python scripts/train/train_baseline_enhanced_fixed.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --train_regex "202503|202504" \
  --valid_regex "202505" \
  --include_tickers 000001.SZ 000002.SZ 600000.SH \
  --sampling_method "undersample"

# 2. æ’é™¤æŸäº›è‚¡ç¥¨
python scripts/train/train_baseline_enhanced_fixed.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --train_regex "202503|202504" \
  --valid_regex "202505" \
  --exclude_tickers 000001.SZ 000002.SZ \
  --sampling_method "undersample"

# 3. ä»æ–‡ä»¶è¯»å–è‚¡ç¥¨åˆ—è¡¨
python scripts/train/train_baseline_enhanced_fixed.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --train_regex "202503|202504" \
  --valid_regex "202505" \
  --ticker_file "tickers.txt" \
  --sampling_method "undersample"

# 4. é™åˆ¶è‚¡ç¥¨æ•°é‡å¹¶æŒ‰äº¤æ˜“é‡é€‰æ‹©
python scripts/train/train_baseline_enhanced_fixed.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --train_regex "202503|202504" \
  --valid_regex "202505" \
  --max_tickers 10 \
  --ticker_selection_method "by_volume" \
  --sampling_method "undersample"

# 5. æŒ‰æ­£æ ·æœ¬æ•°é‡é€‰æ‹©è‚¡ç¥¨
python scripts/train/train_baseline_enhanced_fixed.py \
  --data_root "/home/ma-user/code/fenglang/Spoofing Detect/data" \
  --train_regex "202503|202504" \
  --valid_regex "202505" \
  --max_tickers 5 \
  --ticker_selection_method "by_positive_samples" \
  --use_focal_loss

# åˆ—å®šä¹‰æ¨¡å—ä½¿ç”¨è¯´æ˜:
# ç‰¹å¾åˆ—å®šä¹‰åœ¨: scripts/data_process/features/feature_generator.py
# æ ‡ç­¾åˆ—å®šä¹‰åœ¨: scripts/data_process/labels/label_generator.py
# 
# ä¸»è¦å‡½æ•°ï¼š
# - get_training_feature_columns(): è·å–å¯ç”¨äºè®­ç»ƒçš„ç‰¹å¾åˆ—
# - get_training_target_columns(): è·å–å¯ç”¨ä½œç›®æ ‡å˜é‡çš„æ ‡ç­¾åˆ—
# - get_leakage_risk_columns(): è·å–æœ‰æ•°æ®æ³„éœ²é£é™©çš„åˆ—

# è‚¡ç¥¨ç­›é€‰å‚æ•°è¯´æ˜ï¼š
# --include_tickers: æŒ‡å®šåŒ…å«çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
# --exclude_tickers: æŒ‡å®šæ’é™¤çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨  
# --ticker_file: ä»æ–‡ä»¶è¯»å–è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰
# --max_tickers: æœ€å¤§è‚¡ç¥¨æ•°é‡é™åˆ¶
# --ticker_selection_method: è‚¡ç¥¨é€‰æ‹©æ–¹æ³•ï¼ˆrandom/by_volume/by_positive_samplesï¼‰
"""