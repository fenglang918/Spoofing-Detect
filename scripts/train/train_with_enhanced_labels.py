#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Training with Enhanced Labels (No Data Leakage)
----------------------------------------------
ä½¿ç”¨æ— æ³„éœ²å¢å¼ºæ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œå¯¹æ¯”ä¸åŒæ ‡ç­¾ç­–ç•¥çš„æ•ˆæœ
"""

import argparse, glob, os, time, warnings
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import average_precision_score, classification_report, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

warnings.filterwarnings('ignore')

def get_safe_features():
    """è·å–å®‰å…¨çš„ï¼ˆæ— æ•°æ®æ³„éœ²ï¼‰ç‰¹å¾åˆ—è¡¨"""
    return [
        # è¡Œæƒ…ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "bid1", "ask1", "prev_close", "mid_price", "spread",
        
        # ä»·æ ¼ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "delta_mid", "pct_spread", "price_dev_prevclose",
        
        # è®¢å•ç‰¹å¾ï¼ˆå§”æ‰˜æ—¶åˆ»å¯è§‚æµ‹ï¼‰
        "is_buy", "log_qty",
        
        # å†å²ç»Ÿè®¡ç‰¹å¾ï¼ˆåªä½¿ç”¨è¿‡å»çš„ä¿¡æ¯ï¼‰
        "orders_100ms", "cancels_5s", 
        
        # æ—¶é—´ç‰¹å¾
        "time_sin", "time_cos", "in_auction",
        
        # å¢å¼ºç‰¹å¾ï¼ˆåŸºäºå§”æ‰˜æ—¶åˆ»çš„ä¿¡æ¯ï¼‰
        "book_imbalance", "price_aggressiveness"
    ]

def comprehensive_evaluation(y_true, y_pred_proba, label_name=""):
    """ç»¼åˆè¯„ä¼°å‡½æ•°"""
    metrics = {}
    
    # åŸºç¡€æŒ‡æ ‡
    pr_auc = average_precision_score(y_true, y_pred_proba)
    roc_auc = roc_auc_score(y_true, y_pred_proba)
    
    metrics['PR-AUC'] = pr_auc
    metrics['ROC-AUC'] = roc_auc
    
    # Precision at K
    for k in [0.001, 0.005, 0.01, 0.05]:
        k_int = max(1, int(len(y_true) * k))
        top_k_idx = y_pred_proba.argsort()[::-1][:k_int]
        prec_k = y_true.iloc[top_k_idx].mean() if hasattr(y_true, 'iloc') else y_true[top_k_idx].mean()
        metrics[f'Precision@{k*100:.1f}%'] = prec_k
    
    # æ‰“å°ç»“æœ
    print(f"\nğŸ“Š {label_name} Results:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.6f}")
    
    return metrics

def train_with_label_strategy(X_train, y_train, X_valid, y_valid, strategy_name, balance_ratio=20):
    """ä½¿ç”¨ç‰¹å®šæ ‡ç­¾ç­–ç•¥è®­ç»ƒæ¨¡å‹"""
    print(f"\nğŸš€ Training with {strategy_name}...")
    
    # æ•°æ®å¹³è¡¡
    if y_train.sum() > 0:
        pos_indices = y_train[y_train == 1].index
        neg_indices = y_train[y_train == 0].index
        
        if len(neg_indices) > 0:
            target_neg_size = min(len(pos_indices) * balance_ratio, len(neg_indices))
            selected_neg_indices = np.random.choice(neg_indices, target_neg_size, replace=False)
            
            selected_indices = np.concatenate([pos_indices, selected_neg_indices])
            X_train_balanced = X_train.loc[selected_indices]
            y_train_balanced = y_train.loc[selected_indices]
            
            print(f"  Balanced data: {y_train_balanced.value_counts().to_dict()}")
        else:
            X_train_balanced, y_train_balanced = X_train, y_train
    else:
        print(f"  âš ï¸ No positive samples found for {strategy_name}")
        return None, None
    
    # è®­ç»ƒæ¨¡å‹
    model = lgb.LGBMClassifier(
        objective='binary',
        metric='average_precision',
        learning_rate=0.05,
        num_leaves=31,
        max_depth=6,
        n_estimators=1000,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=10,
        reg_lambda=10,
        random_state=42,
        verbose=-1
    )
    
    # è®­ç»ƒ
    try:
        model.fit(
            X_train_balanced, y_train_balanced,
            eval_set=[(X_valid, y_valid)],
            early_stopping_rounds=100,
            verbose=False
        )
    except TypeError:
        from lightgbm import early_stopping
        model.fit(
            X_train_balanced, y_train_balanced,
            eval_set=[(X_valid, y_valid)],
            callbacks=[early_stopping(100)]
        )
    
    # é¢„æµ‹
    y_pred_proba = model.predict_proba(X_valid)[:, 1]
    
    # è¯„ä¼°
    metrics = comprehensive_evaluation(y_valid, y_pred_proba, strategy_name)
    
    return model, metrics

def compare_label_strategies(df_train, df_valid, feature_cols):
    """å¯¹æ¯”ä¸åŒæ ‡ç­¾ç­–ç•¥çš„æ•ˆæœ"""
    print("\nğŸ”¬ Comparing Label Strategies")
    print("=" * 60)
    
    # å‡†å¤‡ç‰¹å¾
    X_train = df_train[feature_cols].fillna(0)
    X_valid = df_valid[feature_cols].fillna(0)
    
    results = {}
    
    # ç­–ç•¥1: åŸå§‹æ ‡ç­¾
    if 'y_label' in df_train.columns:
        y_train_orig = df_train['y_label']
        y_valid_orig = df_valid['y_label']
        
        model_orig, metrics_orig = train_with_label_strategy(
            X_train, y_train_orig, X_valid, y_valid_orig, 
            "Original Labels (y_label)"
        )
        if metrics_orig:
            results['original'] = metrics_orig
    
    # ç­–ç•¥2: å¢å¼ºæ ‡ç­¾ - å®½æ¾ç‰ˆ
    if 'enhanced_spoofing_liberal' in df_train.columns:
        y_train_lib = df_train['enhanced_spoofing_liberal']
        y_valid_lib = df_valid['enhanced_spoofing_liberal']
        
        model_lib, metrics_lib = train_with_label_strategy(
            X_train, y_train_lib, X_valid, y_valid_lib,
            "Enhanced Liberal Labels"
        )
        if metrics_lib:
            results['enhanced_liberal'] = metrics_lib
    
    # ç­–ç•¥3: å¢å¼ºæ ‡ç­¾ - ä¸­ç­‰ç‰ˆ
    if 'enhanced_spoofing_moderate' in df_train.columns:
        y_train_mod = df_train['enhanced_spoofing_moderate'] 
        y_valid_mod = df_valid['enhanced_spoofing_moderate']
        
        model_mod, metrics_mod = train_with_label_strategy(
            X_train, y_train_mod, X_valid, y_valid_mod,
            "Enhanced Moderate Labels"
        )
        if metrics_mod:
            results['enhanced_moderate'] = metrics_mod
    
    # ç­–ç•¥4: å¢å¼ºæ ‡ç­¾ - ä¸¥æ ¼ç‰ˆ
    if 'enhanced_spoofing_strict' in df_train.columns:
        y_train_strict = df_train['enhanced_spoofing_strict']
        y_valid_strict = df_valid['enhanced_spoofing_strict']
        
        model_strict, metrics_strict = train_with_label_strategy(
            X_train, y_train_strict, X_valid, y_valid_strict,
            "Enhanced Strict Labels"
        )
        if metrics_strict:
            results['enhanced_strict'] = metrics_strict
    
    # ç­–ç•¥5: ç»¼åˆå¢å¼ºæ ‡ç­¾
    if 'enhanced_combined' in df_train.columns:
        y_train_comb = df_train['enhanced_combined']
        y_valid_comb = df_valid['enhanced_combined']
        
        model_comb, metrics_comb = train_with_label_strategy(
            X_train, y_train_comb, X_valid, y_valid_comb,
            "Enhanced Combined Labels"
        )
        if metrics_comb:
            results['enhanced_combined'] = metrics_comb
    
    return results

def create_comparison_report(results, output_dir):
    """åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š"""
    print("\nğŸ“Š Label Strategy Comparison Report")
    print("=" * 80)
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    comparison_df = pd.DataFrame(results).T
    
    # æ‰“å°ä¸»è¦æŒ‡æ ‡å¯¹æ¯”
    key_metrics = ['PR-AUC', 'ROC-AUC', 'Precision@0.1%', 'Precision@1.0%']
    print(f"\nğŸ¯ Key Metrics Comparison:")
    print(comparison_df[key_metrics].round(6))
    
    # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
    best_pr_auc = comparison_df['PR-AUC'].idxmax()
    best_prec_01 = comparison_df['Precision@0.1%'].idxmax()
    
    print(f"\nğŸ† Best Strategies:")
    print(f"  Best PR-AUC: {best_pr_auc} ({comparison_df.loc[best_pr_auc, 'PR-AUC']:.6f})")
    print(f"  Best Precision@0.1%: {best_prec_01} ({comparison_df.loc[best_prec_01, 'Precision@0.1%']:.6f})")
    
    # è®¡ç®—æ”¹è¿›ç¨‹åº¦
    if 'original' in results and 'enhanced_combined' in results:
        orig_pr_auc = results['original']['PR-AUC']
        enh_pr_auc = results['enhanced_combined']['PR-AUC']
        improvement = (enh_pr_auc - orig_pr_auc) / orig_pr_auc * 100
        
        print(f"\nğŸ“ˆ Enhancement Impact:")
        print(f"  Original PR-AUC: {orig_pr_auc:.6f}")
        print(f"  Enhanced PR-AUC: {enh_pr_auc:.6f}")
        print(f"  Improvement: {improvement:+.1f}%")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    output_file = output_dir / "label_strategy_comparison.csv"
    comparison_df.to_csv(output_file, float_format='%.8f')
    print(f"\nğŸ’¾ Detailed comparison saved to: {output_file}")
    
    return comparison_df

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_root", required=True, help="æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--enhanced_dir", help="å¢å¼ºæ ‡ç­¾ç›®å½•ï¼ˆé»˜è®¤ä¸ºdata_root/labels_enhanced_enhancedï¼‰")
    parser.add_argument("--train_regex", default="202503|202504", help="è®­ç»ƒæ•°æ®æ—¥æœŸæ­£åˆ™")
    parser.add_argument("--valid_regex", default="202505", help="éªŒè¯æ•°æ®æ—¥æœŸæ­£åˆ™")
    parser.add_argument("--results_dir", help="ç»“æœä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ä¸ºdata_root/enhanced_label_resultsï¼‰")
    
    args = parser.parse_args()
    
    print("ğŸš€ Training with Enhanced Labels (No Data Leakage)")
    print("=" * 70)
    
    data_root = Path(args.data_root)
    
    # è®¾ç½®ç›®å½•
    if args.enhanced_dir:
        enhanced_dir = Path(args.enhanced_dir)
    else:
        enhanced_dir = data_root / "labels_enhanced_enhanced"
    
    if args.results_dir:
        results_dir = Path(args.results_dir)
    else:
        results_dir = data_root / "enhanced_label_results"
    
    results_dir.mkdir(exist_ok=True)
    
    print(f"ğŸ“ Data root: {data_root}")
    print(f"ğŸ“ Enhanced labels: {enhanced_dir}")
    print(f"ğŸ“ Results dir: {results_dir}")
    
    # æ£€æŸ¥å¢å¼ºæ ‡ç­¾æ˜¯å¦å­˜åœ¨
    if not enhanced_dir.exists():
        print(f"âŒ Enhanced labels directory not found: {enhanced_dir}")
        print("Please run enhanced labeling first:")
        print(f"python scripts/data_process/enhanced_labeling_no_leakage.py --data_root {data_root}")
        return
    
    # åŠ è½½ç‰¹å¾æ•°æ®
    feat_files = list((data_root / "features_select").glob("X_*.parquet"))
    if not feat_files:
        print("âŒ No feature files found")
        return
    
    df_features = pd.concat([pd.read_parquet(f) for f in feat_files], ignore_index=True)
    print(f"ğŸ“Š Features shape: {df_features.shape}")
    
    # åŠ è½½åŸå§‹æ ‡ç­¾
    label_files = list((data_root / "labels_select").glob("labels_*.parquet"))
    df_labels = pd.concat([pd.read_parquet(f) for f in label_files], ignore_index=True)
    print(f"ğŸ“Š Original labels shape: {df_labels.shape}")
    
    # åŠ è½½å¢å¼ºæ ‡ç­¾
    enhanced_files = list(enhanced_dir.glob("enhanced_labels_*.parquet"))
    if not enhanced_files:
        print(f"âŒ No enhanced label files found in {enhanced_dir}")
        return
    
    df_enhanced = pd.concat([pd.read_parquet(f) for f in enhanced_files], ignore_index=True)
    print(f"ğŸ“Š Enhanced labels shape: {df_enhanced.shape}")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    df = df_features.merge(df_labels, on=['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·'], how='inner')
    df = df.merge(df_enhanced.drop('y_label', axis=1, errors='ignore'), on=['è‡ªç„¶æ—¥', 'ticker', 'äº¤æ˜“æ‰€å§”æ‰˜å·'], how='inner')
    print(f"ğŸ“Š Merged data shape: {df.shape}")
    
    # æ•°æ®åˆ‡åˆ†
    train_mask = df["è‡ªç„¶æ—¥"].astype(str).str.contains(args.train_regex)
    valid_mask = df["è‡ªç„¶æ—¥"].astype(str).str.contains(args.valid_regex)
    
    df_train = df[train_mask].copy()
    df_valid = df[valid_mask].copy()
    
    print(f"\nğŸ“… Data split:")
    print(f"Training: {len(df_train):,} samples")
    print(f"Validation: {len(df_valid):,} samples")
    
    # å‡†å¤‡ç‰¹å¾
    feature_cols = get_safe_features()
    available_features = [f for f in feature_cols if f in df.columns]
    print(f"\nğŸ“Š Using {len(available_features)} safe features:")
    for i, feat in enumerate(available_features, 1):
        print(f"  {i:2d}. {feat}")
    
    # å¯¹æ¯”ä¸åŒæ ‡ç­¾ç­–ç•¥
    results = compare_label_strategies(df_train, df_valid, available_features)
    
    # åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š
    if results:
        comparison_df = create_comparison_report(results, results_dir)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        import json
        results_file = results_dir / "enhanced_label_results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"ğŸ’¾ Full results saved to: {results_file}")
    else:
        print("âŒ No valid results obtained")

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹ï¼š

# 1. é¦–å…ˆç”Ÿæˆå¢å¼ºæ ‡ç­¾
python scripts/data_process/enhanced_labeling_no_leakage.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data"

# 2. ä½¿ç”¨å¢å¼ºæ ‡ç­¾è¿›è¡Œè®­ç»ƒå¯¹æ¯”
python scripts/train/train_with_enhanced_labels.py \
  --data_root "/obs/users/fenglang/general/Spoofing Detect/data" \
  --train_regex "202503|202504" \
  --valid_regex "202505"
""" 